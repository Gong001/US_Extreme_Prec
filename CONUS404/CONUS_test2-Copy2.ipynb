{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ee76b6-51df-494c-90e2-a92b2941e6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "### automatically refresh the buffer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "### solve the auto-complete issue\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "### lvl 2 setups (systerm)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib as mpl\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pylab import *\n",
    "from matplotlib.colors import ListedColormap,LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.patches import Wedge, Circle\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from datetime import datetime\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0073be-a0e7-4ebd-9142-f6b25ecf60d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('../../tl_2019_us_state/tl_2019_us_state.shp')\n",
    "input_folder = '/N/project/Zli_lab/Data/Observations/NCAR/prec_acc_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d881bf-18cd-4ab2-8dec-b6f5799a3857",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1983\n",
    "end_year = start_year+3\n",
    "for year in range(start_year, end_year):  # 1989不包含\n",
    "\n",
    "    months = range(10, 13) if year == start_year else range(1, 10) if year == (end_year - 1) else range(1, 13)\n",
    "    # 遍历月份\n",
    "    for month in months:\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "        # 获取当前月份的天数\n",
    "        if month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "            num_days = 31\n",
    "        elif month in [4, 6, 9, 11]:\n",
    "            num_days = 30\n",
    "        elif month == 2:\n",
    "            # 考虑闰年\n",
    "            if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
    "                num_days = 29  # 闰年\n",
    "            else:\n",
    "                num_days = 28  # 平年\n",
    "\n",
    "        # 遍历每个月的天数\n",
    "        for day in range(1, num_days + 1):\n",
    "            \n",
    "            month_str = f\"{month:02}\"\n",
    "            day_str = f\"{day:02}\"\n",
    "            input_file = f'PREC_ACC_NC.wrf2d_d01_{year}-{month_str}-{day_str}_*.nc'\n",
    "            ds = xr.open_mfdataset(input_folder + input_file)\n",
    "        # 提取CONUS数据\n",
    "            lon = ds['XLONG'].values\n",
    "            lat = ds['XLAT'].values\n",
    "            grid = gpd.GeoDataFrame(\n",
    "                geometry=gpd.points_from_xy(lon.flatten(), lat.flatten()),\n",
    "                index=np.arange(lon.size)\n",
    "            )\n",
    "            grid.set_crs(gdf.crs, inplace=True)\n",
    "            grid_s = gpd.sjoin(grid, gdf, how='inner', predicate='within')\n",
    "            \n",
    "            mask = np.full(ds['PREC_ACC_NC'].shape[1:], False) \n",
    "            for index in grid_s.index:\n",
    "                row, col = np.unravel_index(index, mask.shape)  # 获取行列索引\n",
    "                mask[row, col] = True\n",
    "            mask_da = xr.DataArray(mask, dims=ds['PREC_ACC_NC'].dims[1:], coords={'south_north': ds['PREC_ACC_NC'].coords['south_north'], 'west_east': ds['PREC_ACC_NC'].coords['west_east']})\n",
    "            ds_s = ds.where(mask_da, drop=True)\n",
    "            lonn = np.linspace(-124.848, -66.885, 1137)\n",
    "            latt = np.linspace(24.396, 49.384, 708)\n",
    "            prec = ds_s.PREC_ACC_NC.values\n",
    "            lat_min = latt.min()\n",
    "            lat_max = latt.max()\n",
    "            ds_sss = xr.Dataset({'p': (['time', 'lat', 'lon'], prec)},\n",
    "                                coords={'lon': (['lon'], lonn),\n",
    "                                        'lat': (['lat'], latt),\n",
    "                                        'time': ('time', ds_s.Time.values)})\n",
    "\n",
    "            original_times = ds_sss.time.values \n",
    "            \n",
    "            lon_ranges = [(-np.inf, -112.5), (-112.5, -97.5), (-97.5, -82.5), (-82.5, np.inf)]\n",
    "            utc_offsets = [-8, -7, -6, -5]\n",
    "            \n",
    "            for (lon_min, lon_max), offset in zip(lon_ranges, utc_offsets):\n",
    "                mask = (ds_sss.lon >= lon_min) & (ds_sss.lon < lon_max)\n",
    "                ds_lon_subset = ds_sss.where(mask, drop=True)\n",
    "                if ds_lon_subset.lat.size > 0 and ds_lon_subset.lon.size > 0:\n",
    "            \n",
    "                    adjusted_times = original_times + np.timedelta64(offset, 'h')  # 保持24个时间点\n",
    "            \n",
    "                    ds_lon_subset = ds_lon_subset.assign_coords(time=adjusted_times)\n",
    "            \n",
    "                    lat_min = ds_lon_subset.lat.min().values\n",
    "                    lat_max = ds_lon_subset.lat.max().values\n",
    "            \n",
    "                    lat_splits = np.linspace(lat_min, lat_max, 10)  # 10个值分9段\n",
    "            \n",
    "                    for i in range(len(lat_splits) - 1):\n",
    "                        lat_min_split = lat_splits[i]\n",
    "                        lat_max_split = lat_splits[i + 1]\n",
    "                        lat_mask = (ds_lon_subset.lat >= lat_min_split) & (ds_lon_subset.lat < lat_max_split)\n",
    "                        ds_lat_subset = ds_lon_subset.where(lat_mask, drop=True)\n",
    "                        \n",
    "                        output_folder = '../CONUS404_data/LST/UTC/U' + str(offset)+str(i) + '/'\n",
    "                        output_file = f'PREC_ACC_NC.wrf2d_d01_{year}-{month_str}-{day_str}.nc'\n",
    "                        output_path = os.path.join(output_folder, output_file)\n",
    "                        os.makedirs(output_folder, exist_ok=True)\n",
    "                        ds_lat_subset.to_netcdf(output_folder + output_file)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df60ff8-0058-4436-83bf-dd1bd6795dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-02 20:38:43\n",
      "2024-11-02 20:48:39\n",
      "2024-11-02 21:03:29\n",
      "2024-11-02 21:17:12\n",
      "2024-11-02 21:31:51\n",
      "2024-11-02 21:45:21\n",
      "2024-11-02 21:58:37\n",
      "2024-11-02 22:12:28\n",
      "2024-11-02 22:26:11\n",
      "2024-11-02 22:40:34\n",
      "2024-11-02 22:54:34\n",
      "2024-11-02 23:08:02\n",
      "2024-11-02 23:20:58\n",
      "2024-11-02 23:34:36\n",
      "2024-11-02 23:48:13\n",
      "2024-11-03 00:01:43\n",
      "2024-11-03 00:15:21\n",
      "2024-11-03 00:26:58\n",
      "2024-11-03 00:40:36\n",
      "2024-11-03 00:54:14\n",
      "2024-11-03 01:07:15\n",
      "2024-11-03 01:20:45\n",
      "2024-11-03 01:34:12\n",
      "2024-11-03 01:47:58\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file('../../tl_2019_us_state/tl_2019_us_state.shp')\n",
    "input_folder = '/N/project/Zli_lab/Data/Observations/NCAR/CONUS404_T_dT/TarFiles/'\n",
    "\n",
    "start_year = 1983\n",
    "end_year = start_year+3\n",
    "for year in range(start_year, end_year):  # 1989不包含\n",
    "\n",
    "    months = range(10, 13) if year == start_year else range(1, 10) if year == (end_year - 1) else range(1, 13)\n",
    "    # 遍历月份\n",
    "    for month in months:\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "        # 获取当前月份的天数\n",
    "        if month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "            num_days = 31\n",
    "        elif month in [4, 6, 9, 11]:\n",
    "            num_days = 30\n",
    "        elif month == 2:\n",
    "            # 考虑闰年\n",
    "            if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
    "                num_days = 29  # 闰年\n",
    "            else:\n",
    "                num_days = 28  # 平年\n",
    "\n",
    "        # 遍历每个月的天数\n",
    "        for day in range(1, num_days + 1):\n",
    "            \n",
    "            month_str = f\"{month:02}\"\n",
    "            day_str = f\"{day:02}\"\n",
    "            input_file = f'765041.T2.wrf2d_d01_{year}-{month_str}-{day_str}_*.nc'\n",
    "            ds = xr.open_mfdataset(input_folder + input_file)\n",
    "        # 提取CONUS数据\n",
    "            lon = ds['XLONG'].values\n",
    "            lat = ds['XLAT'].values\n",
    "            grid = gpd.GeoDataFrame(\n",
    "                geometry=gpd.points_from_xy(lon.flatten(), lat.flatten()),\n",
    "                index=np.arange(lon.size)\n",
    "            )\n",
    "            grid.set_crs(gdf.crs, inplace=True)\n",
    "            grid_s = gpd.sjoin(grid, gdf, how='inner', predicate='within')\n",
    "            \n",
    "            mask = np.full(ds['T2'].shape[1:], False) \n",
    "            for index in grid_s.index:\n",
    "                row, col = np.unravel_index(index, mask.shape)  # 获取行列索引\n",
    "                mask[row, col] = True\n",
    "            mask_da = xr.DataArray(mask, dims=ds['T2'].dims[1:], coords={'south_north': ds['T2'].coords['south_north'], 'west_east': ds['T2'].coords['west_east']})\n",
    "            ds_s = ds.where(mask_da, drop=True)\n",
    "            lonn = np.linspace(-124.848, -66.885, 1137)\n",
    "            latt = np.linspace(24.396, 49.384, 708)\n",
    "            prec = ds_s['T2'].values\n",
    "            lat_min = latt.min()\n",
    "            lat_max = latt.max()\n",
    "            ds_sss = xr.Dataset({'t2': (['time', 'lat', 'lon'], prec)},\n",
    "                                coords={'lon': (['lon'], lonn),\n",
    "                                        'lat': (['lat'], latt),\n",
    "                                        'time': ('time', ds_s.Time.values)})\n",
    "\n",
    "            original_times = ds_sss.time.values \n",
    "            \n",
    "            lon_ranges = [(-np.inf, -112.5), (-112.5, -97.5), (-97.5, -82.5), (-82.5, np.inf)]\n",
    "            utc_offsets = [-8, -7, -6, -5]\n",
    "            \n",
    "            for (lon_min, lon_max), offset in zip(lon_ranges, utc_offsets):\n",
    "                mask = (ds_sss.lon >= lon_min) & (ds_sss.lon < lon_max)\n",
    "                ds_lon_subset = ds_sss.where(mask, drop=True)\n",
    "                if ds_lon_subset.lat.size > 0 and ds_lon_subset.lon.size > 0:\n",
    "            \n",
    "                    adjusted_times = original_times + np.timedelta64(offset, 'h')  # 保持24个时间点\n",
    "            \n",
    "                    ds_lon_subset = ds_lon_subset.assign_coords(time=adjusted_times)\n",
    "            \n",
    "                    lat_min = ds_lon_subset.lat.min().values\n",
    "                    lat_max = ds_lon_subset.lat.max().values\n",
    "            \n",
    "                    lat_splits = np.linspace(lat_min, lat_max, 10)  # 10个值分9段\n",
    "            \n",
    "                    for i in range(len(lat_splits) - 1):\n",
    "                        lat_min_split = lat_splits[i]\n",
    "                        lat_max_split = lat_splits[i + 1]\n",
    "                        lat_mask = (ds_lon_subset.lat >= lat_min_split) & (ds_lon_subset.lat < lat_max_split)\n",
    "                        ds_lat_subset = ds_lon_subset.where(lat_mask, drop=True)\n",
    "                        \n",
    "                        output_folder = '../CONUS404_data/LST/UTC/U' + str(offset)+str(i) + '/'\n",
    "                        output_file = f'T2.wrf2d_d01_{year}-{month_str}-{day_str}.nc'\n",
    "                        output_path = os.path.join(output_folder, output_file)\n",
    "                        os.makedirs(output_folder, exist_ok=True)\n",
    "                        ds_lat_subset.to_netcdf(output_folder + output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60c415ed-b92c-4432-a1f3-df9f1873c568",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-03 01:01:41\n",
      "2024-11-03 01:14:45\n",
      "2024-11-03 01:28:51\n",
      "2024-11-03 01:43:09\n",
      "2024-11-03 01:57:30\n",
      "2024-11-03 02:11:29\n",
      "2024-11-03 02:25:16\n",
      "2024-11-03 02:37:36\n",
      "2024-11-03 02:50:37\n",
      "2024-11-03 03:03:48\n",
      "2024-11-03 03:17:51\n",
      "2024-11-03 03:30:59\n",
      "2024-11-03 03:43:59\n",
      "2024-11-03 03:57:18\n",
      "2024-11-03 04:10:21\n",
      "2024-11-03 04:23:36\n",
      "2024-11-03 04:37:04\n",
      "2024-11-03 04:49:14\n",
      "2024-11-03 05:02:35\n",
      "2024-11-03 05:16:05\n",
      "2024-11-03 05:29:05\n",
      "2024-11-03 05:41:09\n",
      "2024-11-03 05:54:19\n",
      "2024-11-03 06:06:58\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file('../../tl_2019_us_state/tl_2019_us_state.shp')\n",
    "input_folder = '/N/project/Zli_lab/Data/Observations/NCAR/CONUS404_T_dT/TarFiles/'\n",
    "\n",
    "start_year = 1983\n",
    "end_year = start_year+3\n",
    "for year in range(start_year, end_year):  # 1989不包含\n",
    "\n",
    "    months = range(10, 13) if year == start_year else range(1, 10) if year == (end_year - 1) else range(1, 13)\n",
    "\n",
    "    for month in months:\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "\n",
    "        if month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "            num_days = 31\n",
    "        elif month in [4, 6, 9, 11]:\n",
    "            num_days = 30\n",
    "        elif month == 2:\n",
    "\n",
    "            if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
    "                num_days = 29  \n",
    "            else:\n",
    "                num_days = 28  \n",
    "\n",
    "        for day in range(1, num_days + 1):\n",
    "            \n",
    "            month_str = f\"{month:02}\"\n",
    "            day_str = f\"{day:02}\"\n",
    "            input_file = f'765041.TD2.wrf2d_d01_{year}-{month_str}-{day_str}_*.nc'\n",
    "            ds = xr.open_mfdataset(input_folder + input_file)\n",
    "\n",
    "            lon = ds['XLONG'].values\n",
    "            lat = ds['XLAT'].values\n",
    "            grid = gpd.GeoDataFrame(\n",
    "                geometry=gpd.points_from_xy(lon.flatten(), lat.flatten()),\n",
    "                index=np.arange(lon.size)\n",
    "            )\n",
    "            grid.set_crs(gdf.crs, inplace=True)\n",
    "            grid_s = gpd.sjoin(grid, gdf, how='inner', predicate='within')\n",
    "            \n",
    "            mask = np.full(ds['TD2'].shape[1:], False) \n",
    "            for index in grid_s.index:\n",
    "                row, col = np.unravel_index(index, mask.shape)  # 获取行列索引\n",
    "                mask[row, col] = True\n",
    "            mask_da = xr.DataArray(mask, dims=ds['TD2'].dims[1:], coords={'south_north': ds['TD2'].coords['south_north'], 'west_east': ds['TD2'].coords['west_east']})\n",
    "            ds_s = ds.where(mask_da, drop=True)\n",
    "            lonn = np.linspace(-124.848, -66.885, 1137)\n",
    "            latt = np.linspace(24.396, 49.384, 708)\n",
    "            prec = ds_s['TD2'].values\n",
    "            lat_min = latt.min()\n",
    "            lat_max = latt.max()\n",
    "            ds_sss = xr.Dataset({'td2': (['time', 'lat', 'lon'], prec)},\n",
    "                                coords={'lon': (['lon'], lonn),\n",
    "                                        'lat': (['lat'], latt),\n",
    "                                        'time': ('time', ds_s.Time.values)})\n",
    "\n",
    "            original_times = ds_sss.time.values \n",
    "            \n",
    "            lon_ranges = [(-np.inf, -112.5), (-112.5, -97.5), (-97.5, -82.5), (-82.5, np.inf)]\n",
    "            utc_offsets = [-8, -7, -6, -5]\n",
    "            \n",
    "            for (lon_min, lon_max), offset in zip(lon_ranges, utc_offsets):\n",
    "                mask = (ds_sss.lon >= lon_min) & (ds_sss.lon < lon_max)\n",
    "                ds_lon_subset = ds_sss.where(mask, drop=True)\n",
    "                if ds_lon_subset.lat.size > 0 and ds_lon_subset.lon.size > 0:\n",
    "            \n",
    "                    adjusted_times = original_times + np.timedelta64(offset, 'h')  # 保持24个时间点\n",
    "            \n",
    "                    ds_lon_subset = ds_lon_subset.assign_coords(time=adjusted_times)\n",
    "            \n",
    "                    lat_min = ds_lon_subset.lat.min().values\n",
    "                    lat_max = ds_lon_subset.lat.max().values\n",
    "            \n",
    "                    lat_splits = np.linspace(lat_min, lat_max, 10)  # 10个值分9段\n",
    "            \n",
    "                    for i in range(len(lat_splits) - 1):\n",
    "                        lat_min_split = lat_splits[i]\n",
    "                        lat_max_split = lat_splits[i + 1]\n",
    "                        lat_mask = (ds_lon_subset.lat >= lat_min_split) & (ds_lon_subset.lat < lat_max_split)\n",
    "                        ds_lat_subset = ds_lon_subset.where(lat_mask, drop=True)\n",
    "                        \n",
    "                        output_folder = '../CONUS404_data/LST/UTC/U' + str(offset)+str(i) + '/'\n",
    "                        output_file = f'TD2.wrf2d_d01_{year}-{month_str}-{day_str}.nc'\n",
    "                        output_path = os.path.join(output_folder, output_file)\n",
    "                        os.makedirs(output_folder, exist_ok=True)\n",
    "                        ds_lat_subset.to_netcdf(output_folder + output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d65245c-6b6e-4159-9eba-684c7bcfae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def create_temp_dataset(arr_t, latt, lonn):\n",
    "\n",
    "    arr_ntt = arr_t.reshape(43, 92, 24, arr_t.shape[1], arr_t.shape[2])\n",
    "    selected_data = np.concatenate((arr_ntt[:, :, 6:12, :, :], arr_ntt[:, :, 12:18, :, :]), axis=2)\n",
    "\n",
    "    arr_tmin = np.nanmean(np.nanmin(selected_data, axis=2),axis=1)\n",
    "    arr_tmean = np.nanmean(np.nanmean(selected_data, axis=2),axis=1)\n",
    "    \n",
    "    ds_tmin = xr.Dataset(\n",
    "        {'t': (['year',  'lat', 'lon'], arr_tmin)},\n",
    "        coords={\n",
    "            'year': (['year'], np.arange(1980, 2023)),\n",
    "            'lat': (['lat'], latt),\n",
    "            'lon': (['lon'], lonn)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    ds_tmean = xr.Dataset(\n",
    "        {'t': (['year',  'lat', 'lon'], arr_tmean)},\n",
    "        coords={\n",
    "            'year': (['year'], np.arange(1980, 2023)),\n",
    "            'lat': (['lat'], latt),\n",
    "            'lon': (['lon'], lonn)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return ds_tmin,ds_tmean\n",
    "\n",
    "\n",
    "\n",
    "def create_dtemp_dataset(arr_t, latt, lonn):\n",
    "\n",
    "    arr_ntt = arr_t.reshape(43, 92, 24, arr_t.shape[1], arr_t.shape[2])\n",
    "    selected_data = np.concatenate((arr_ntt[:, :, 6:12, :, :], arr_ntt[:, :, 12:18, :, :]), axis=2)\n",
    "\n",
    "    arr_tmin = np.nanmean(np.nanmin(selected_data, axis=2),axis=1)\n",
    "    arr_tmean = np.nanmean(np.nanmean(selected_data, axis=2),axis=1)\n",
    "    \n",
    "    ds_dtmin = xr.Dataset(\n",
    "        {'dt': (['year',  'lat', 'lon'], arr_tmin)},\n",
    "        coords={\n",
    "            'year': (['year'], np.arange(1980, 2023)),\n",
    "            'lat': (['lat'], latt),\n",
    "            'lon': (['lon'], lonn)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    ds_dtmean = xr.Dataset(\n",
    "        {'dt': (['year',  'lat', 'lon'], arr_tmean)},\n",
    "        coords={\n",
    "            'year': (['year'], np.arange(1980, 2023)),\n",
    "            'lat': (['lat'], latt),\n",
    "            'lon': (['lon'], lonn)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return ds_dtmin,ds_dtmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccf841fc-9dae-48df-ad32-57deb94d4e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-04 22:03:15\n",
      "2024-11-04 22:10:22\n",
      "2024-11-04 22:15:52\n",
      "2024-11-04 22:21:29\n",
      "2024-11-04 22:27:16\n",
      "2024-11-04 22:32:50\n",
      "2024-11-04 22:38:36\n",
      "2024-11-04 22:44:16\n",
      "2024-11-04 22:49:44\n",
      "2024-11-04 22:55:23\n",
      "2024-11-04 23:00:52\n",
      "2024-11-04 23:06:14\n",
      "2024-11-04 23:11:31\n",
      "2024-11-04 23:16:58\n",
      "2024-11-04 23:22:27\n",
      "2024-11-04 23:27:54\n",
      "2024-11-04 23:33:32\n",
      "2024-11-04 23:38:47\n",
      "2024-11-04 23:44:00\n",
      "2024-11-04 23:49:13\n",
      "2024-11-04 23:54:31\n",
      "2024-11-04 23:59:42\n",
      "2024-11-05 00:05:06\n",
      "2024-11-05 00:10:09\n",
      "2024-11-05 00:15:27\n",
      "2024-11-05 00:20:45\n",
      "2024-11-05 00:25:55\n",
      "2024-11-05 00:30:59\n",
      "2024-11-05 00:35:36\n",
      "2024-11-05 00:40:23\n",
      "2024-11-05 00:45:04\n",
      "2024-11-05 00:49:47\n",
      "2024-11-05 00:54:23\n",
      "2024-11-05 00:59:10\n",
      "2024-11-05 01:03:54\n",
      "2024-11-05 01:08:40\n"
     ]
    }
   ],
   "source": [
    "base_path = '/N/project/Zli_lab/gongg/CONUS404_data/LST/UTC/'\n",
    "file_pattern = 'TD2.wrf2d_d01_????-??-??.nc'\n",
    "\n",
    "folder_names = [\n",
    "    'U-50', 'U-51', 'U-52', 'U-53', 'U-54', 'U-55', 'U-56', 'U-57', 'U-58',\n",
    "    'U-60', 'U-61', 'U-62', 'U-63', 'U-64', 'U-65', 'U-66', 'U-67', 'U-68',\n",
    "    'U-70', 'U-71', 'U-72', 'U-73', 'U-74', 'U-75', 'U-76', 'U-77', 'U-78',\n",
    "    'U-80', 'U-81', 'U-82', 'U-83', 'U-84', 'U-85', 'U-86', 'U-87', 'U-88',\n",
    "]\n",
    "\n",
    "\n",
    "for folder in folder_names:\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    full_path = os.path.join(base_path, folder, file_pattern)\n",
    "    all_files = glob.glob(full_path)\n",
    "    #####\n",
    "    summer_files = [f for f in all_files if '-06-' in f or '-07-' in f or '-08-' in f or '-09-' in f]\n",
    "    ds_summer = xr.open_mfdataset(summer_files)\n",
    "    ds_jja = ds_summer.sel(time=ds_summer['time'].dt.month.isin([6, 7, 8]))\n",
    "    lonn = ds_jja.lon.values\n",
    "    latt = ds_jja.lat.values\n",
    "    arr_t = ds_jja.td2.values\n",
    "\n",
    "    ds_dtmin,ds_dtmean = create_dtemp_dataset(arr_t, latt, lonn)\n",
    "\n",
    "    output_folder = '/N/project/Zli_lab/gongg/CONUS404_data/LST/JJA/'\n",
    "    ds_dtmin.to_netcdf(output_folder+'daytime_dtemp_min_'+folder+'.nc')\n",
    "    ds_dtmean.to_netcdf(output_folder+'daytime_dtemp_mean_'+folder+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77985608-69cb-4f6d-bf0b-5494a5c1a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def calculate_dn_averages(data, block_size=12):\n",
    "    \"\"\"\n",
    "    计算每个时间块的平均温度值。\n",
    "\n",
    "    参数:\n",
    "    data -- 输入的温度数据数组。\n",
    "    block_size -- 每个时间块的大小（默认为12个月）。\n",
    "\n",
    "    返回:\n",
    "    包含平均温度值的新数组，形状与输入数组相同。\n",
    "    \"\"\"\n",
    "    # 创建一个形状相同的数组来存放结果，所有元素初始化为 NaN\n",
    "    arr_t_dn = np.full_like(data, np.nan)\n",
    "    \n",
    "    # 循环处理每个位置的数据\n",
    "    for i in range(data.shape[1]):\n",
    "        for j in range(data.shape[2]):\n",
    "            for k in range(data.shape[0] // block_size):\n",
    "                # 选择第k个时间块的数据块进行平均\n",
    "                arr_t_avg = np.mean(data[k*block_size:(k+1)*block_size, i, j])\n",
    "                # 将平均结果填充到新数组的相应位置\n",
    "                arr_t_dn[k*block_size:(k+1)*block_size, i, j] = arr_t_avg\n",
    "\n",
    "    return arr_t_dn\n",
    "\n",
    "# 使用示例：\n",
    "# 假设 ds_selected 是你的数据集，并且已经加载了需要的数据。\n",
    "# arr_t = ds_selected.t2.values\n",
    "# 计算平均值\n",
    "# result = calculate_monthly_averages(arr_t)\n",
    "\n",
    "def cal_dn_t(array, dataset):\n",
    "    lonn = dataset.lon.values\n",
    "    latt = dataset.lat.values\n",
    "    timee = dataset.time.values\n",
    "    \n",
    "    ds_dn_t = xr.Dataset(\n",
    "        {'dnt': (['time', 'lat', 'lon'], array)},\n",
    "        coords={\n",
    "            'lon': (['lon'], lonn),\n",
    "            'lat': (['lat'], latt),\n",
    "            'time': ('time', timee)\n",
    "        }\n",
    "    )\n",
    "    return ds_dn_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1de13b9-03f1-4e1d-a98f-61128ac0ea1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_path = '/N/project/Zli_lab/gongg/CONUS404_data/LST/UTC/'\n",
    "file_pattern_p = 'PREC_ACC_NC.wrf2d_d01_????-??-??.nc'\n",
    "file_pattern_t = 'T2.wrf2d_d01_????-??-??.nc'\n",
    "output_folder = '/N/project/Zli_lab/gongg/CONUS404_data/LST/JJA/'\n",
    "\n",
    "folder_names = [\n",
    "    # 'U-50', 'U-51', 'U-52', 'U-53', 'U-54', 'U-55', 'U-56', 'U-57', 'U-58',\n",
    "     #'U-60', \n",
    "    'U-61', 'U-62', 'U-63', 'U-64', 'U-65', 'U-66', 'U-67', 'U-68',\n",
    "    # 'U-70', 'U-71', 'U-72', 'U-73', 'U-74', 'U-75', 'U-76', 'U-77', 'U-78',\n",
    "    # 'U-80', 'U-81', 'U-82', 'U-83', 'U-84', 'U-85', 'U-86', 'U-87', 'U-88',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6170203-be1a-4e14-b198-58390ac39a32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-27 16:14:35\n",
      "2024-12-27 16:52:22\n",
      "2024-12-27 17:29:35\n",
      "2024-12-27 18:06:01\n",
      "2024-12-27 18:42:45\n",
      "2024-12-27 19:19:42\n",
      "2024-12-27 19:56:16\n",
      "2024-12-27 20:32:50\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "for folder in folder_names:\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    full_path_t = os.path.join(base_path, folder, file_pattern_t)\n",
    "    all_files_t = glob.glob(full_path_t)\n",
    "    #####\n",
    "    summer_files_t = [f for f in all_files_t if '-06-' in f or '-07-' in f or '-08-' in f or '-09-' in f or '-05-' in f]\n",
    "    ds = xr.open_mfdataset(summer_files_t)\n",
    "    \n",
    "\n",
    "    # 构建时间选择器，每年的5月31日18:00到9月1日05:00\n",
    "    time_selection = (ds.time.dt.month == 5) & (ds.time.dt.day == 31) & (ds.time.dt.hour >= 18) | \\\n",
    "                     (ds.time.dt.month == 6) | \\\n",
    "                     (ds.time.dt.month == 7) | \\\n",
    "                     (ds.time.dt.month == 8) | \\\n",
    "                     ((ds.time.dt.month == 9) & (ds.time.dt.day == 1) & (ds.time.dt.hour <= 5))\n",
    "\n",
    "    # 应用时间选择器\n",
    "    ds_selected = ds.where(time_selection, drop=True)\n",
    "    arr_t = ds_selected.t2.values\n",
    "    arr_dnt = calculate_dn_averages(arr_t)\n",
    "    ds_dnt = cal_dn_t(arr_dnt, ds_selected)\n",
    "    ds_dnt_jja = ds_dnt.sel(time=ds_dnt['time'].dt.month.isin([6, 7, 8]))\n",
    "    ds_dnt_jja.to_netcdf(output_folder+'dn_temp_'+folder+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ce1967-67dd-4699-9d4d-6f06de00f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/N/project/Zli_lab/gongg/CONUS404_data/LST/UTC/'\n",
    "file_pattern_p = 'PREC_ACC_NC.wrf2d_d01_????-??-??.nc'\n",
    "file_pattern_t = 'TD2.wrf2d_d01_????-??-??.nc'\n",
    "output_folder = '/N/project/Zli_lab/gongg/CONUS404_data/LST/JJA/'\n",
    "\n",
    "folder_names = [\n",
    "    # 'U-50', 'U-51', 'U-52', 'U-53', 'U-54', 'U-55', 'U-56', 'U-57', 'U-58',\n",
    "     'U-60', 'U-61', 'U-62', 'U-63', 'U-64', 'U-65', 'U-66', 'U-67', 'U-68',\n",
    "    # 'U-70', 'U-71', 'U-72', 'U-73', 'U-74', 'U-75', 'U-76', 'U-77', 'U-78',\n",
    "    # 'U-80', 'U-81', 'U-82', 'U-83', 'U-84', 'U-85', 'U-86', 'U-87', 'U-88',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7872abc2-e92d-4b0b-b9c6-680acd6f07e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-29 01:41:54\n",
      "2024-12-29 02:16:49\n",
      "2024-12-29 02:52:04\n",
      "2024-12-29 03:26:23\n",
      "2024-12-29 04:01:29\n",
      "2024-12-29 04:38:54\n",
      "2024-12-29 05:15:23\n",
      "2024-12-29 05:49:11\n",
      "2024-12-29 06:24:41\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "for folder in folder_names:\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    full_path_t = os.path.join(base_path, folder, file_pattern_t)\n",
    "    all_files_t = glob.glob(full_path_t)\n",
    "    #####\n",
    "    summer_files_t = [f for f in all_files_t if '-06-' in f or '-07-' in f or '-08-' in f or '-09-' in f or '-05-' in f]\n",
    "    ds = xr.open_mfdataset(summer_files_t)\n",
    "    \n",
    "\n",
    "    # 构建时间选择器，每年的5月31日18:00到9月1日05:00\n",
    "    time_selection = (ds.time.dt.month == 5) & (ds.time.dt.day == 31) & (ds.time.dt.hour >= 18) | \\\n",
    "                     (ds.time.dt.month == 6) | \\\n",
    "                     (ds.time.dt.month == 7) | \\\n",
    "                     (ds.time.dt.month == 8) | \\\n",
    "                     ((ds.time.dt.month == 9) & (ds.time.dt.day == 1) & (ds.time.dt.hour <= 5))\n",
    "\n",
    "    # 应用时间选择器\n",
    "    ds_selected = ds.where(time_selection, drop=True)\n",
    "    arr_t = ds_selected.td2.values\n",
    "    arr_dnt = calculate_dn_averages(arr_t)\n",
    "    ds_dnt = cal_dn_t(arr_dnt, ds_selected)\n",
    "    ds_dnt_jja = ds_dnt.sel(time=ds_dnt['time'].dt.month.isin([6, 7, 8]))\n",
    "    ds_dnt_jja.to_netcdf(output_folder+'dn_dewtemp_'+folder+'.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
