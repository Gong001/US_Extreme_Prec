{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ee76b6-51df-494c-90e2-a92b2941e6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### automatically refresh the buffer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "### solve the auto-complete issue\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "### lvl 2 setups (systerm)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib as mpl\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pylab import *\n",
    "from matplotlib.colors import ListedColormap,LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.patches import Wedge, Circle\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from datetime import datetime\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0073be-a0e7-4ebd-9142-f6b25ecf60d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('../../tl_2019_us_state/tl_2019_us_state.shp')\n",
    "input_folder = '/N/project/Zli_lab/Data/Observations/NCAR/prec_acc_files/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d881bf-18cd-4ab2-8dec-b6f5799a3857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-06 17:11:47\n",
      "2024-10-06 17:24:42\n",
      "2024-10-06 17:36:50\n",
      "2024-10-06 17:49:51\n",
      "2024-10-06 18:02:57\n",
      "2024-10-06 18:14:56\n",
      "2024-10-06 18:27:43\n",
      "2024-10-06 18:39:55\n",
      "2024-10-06 18:53:24\n",
      "2024-10-06 19:05:45\n",
      "2024-10-06 19:19:26\n",
      "2024-10-06 19:32:29\n",
      "2024-10-06 19:45:02\n",
      "2024-10-06 19:58:01\n",
      "2024-10-06 20:12:24\n",
      "2024-10-06 20:25:35\n",
      "2024-10-06 20:39:03\n",
      "2024-10-06 20:51:28\n",
      "2024-10-06 21:05:25\n",
      "2024-10-06 21:18:53\n",
      "2024-10-06 21:32:07\n",
      "2024-10-06 21:44:19\n",
      "2024-10-06 21:57:03\n",
      "2024-10-06 22:10:07\n"
     ]
    }
   ],
   "source": [
    "start_year = 2015\n",
    "end_year = start_year+3\n",
    "for year in range(start_year, end_year):  # 1989不包含\n",
    "\n",
    "    months = range(10, 13) if year == start_year else range(1, 10) if year == (end_year - 1) else range(1, 13)\n",
    "    # 遍历月份\n",
    "    for month in months:\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "        # 获取当前月份的天数\n",
    "        if month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "            num_days = 31\n",
    "        elif month in [4, 6, 9, 11]:\n",
    "            num_days = 30\n",
    "        elif month == 2:\n",
    "            # 考虑闰年\n",
    "            if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
    "                num_days = 29  # 闰年\n",
    "            else:\n",
    "                num_days = 28  # 平年\n",
    "\n",
    "        # 遍历每个月的天数\n",
    "        for day in range(1, num_days + 1):\n",
    "            \n",
    "            month_str = f\"{month:02}\"\n",
    "            day_str = f\"{day:02}\"\n",
    "            input_file = f'PREC_ACC_NC.wrf2d_d01_{year}-{month_str}-{day_str}_*.nc'\n",
    "            ds = xr.open_mfdataset(input_folder + input_file)\n",
    "        # 提取CONUS数据\n",
    "            lon = ds['XLONG'].values\n",
    "            lat = ds['XLAT'].values\n",
    "            grid = gpd.GeoDataFrame(\n",
    "                geometry=gpd.points_from_xy(lon.flatten(), lat.flatten()),\n",
    "                index=np.arange(lon.size)\n",
    "            )\n",
    "            grid.set_crs(gdf.crs, inplace=True)\n",
    "            grid_s = gpd.sjoin(grid, gdf, how='inner', predicate='within')\n",
    "            \n",
    "            mask = np.full(ds['PREC_ACC_NC'].shape[1:], False) \n",
    "            for index in grid_s.index:\n",
    "                row, col = np.unravel_index(index, mask.shape)  # 获取行列索引\n",
    "                mask[row, col] = True\n",
    "            mask_da = xr.DataArray(mask, dims=ds['PREC_ACC_NC'].dims[1:], coords={'south_north': ds['PREC_ACC_NC'].coords['south_north'], 'west_east': ds['PREC_ACC_NC'].coords['west_east']})\n",
    "            ds_s = ds.where(mask_da, drop=True)\n",
    "            lonn = np.linspace(-124.848, -66.885, 1137)\n",
    "            latt = np.linspace(24.396, 49.384, 708)\n",
    "            prec = ds_s.PREC_ACC_NC.values\n",
    "            lat_min = latt.min()\n",
    "            lat_max = latt.max()\n",
    "            ds_sss = xr.Dataset({'p': (['time', 'lat', 'lon'], prec)},\n",
    "                                coords={'lon': (['lon'], lonn),\n",
    "                                        'lat': (['lat'], latt),\n",
    "                                        'time': ('time', ds_s.Time.values)})\n",
    "\n",
    "            original_times = ds_sss.time.values \n",
    "            \n",
    "            lon_ranges = [(-np.inf, -112.5), (-112.5, -97.5), (-97.5, -82.5), (-82.5, np.inf)]\n",
    "            utc_offsets = [-8, -7, -6, -5]\n",
    "            \n",
    "            for (lon_min, lon_max), offset in zip(lon_ranges, utc_offsets):\n",
    "                mask = (ds_sss.lon >= lon_min) & (ds_sss.lon < lon_max)\n",
    "                ds_lon_subset = ds_sss.where(mask, drop=True)\n",
    "                if ds_lon_subset.lat.size > 0 and ds_lon_subset.lon.size > 0:\n",
    "            \n",
    "                    adjusted_times = original_times + np.timedelta64(offset, 'h')  # 保持24个时间点\n",
    "            \n",
    "                    ds_lon_subset = ds_lon_subset.assign_coords(time=adjusted_times)\n",
    "            \n",
    "                    lat_min = ds_lon_subset.lat.min().values\n",
    "                    lat_max = ds_lon_subset.lat.max().values\n",
    "            \n",
    "                    lat_splits = np.linspace(lat_min, lat_max, 10)  # 10个值分9段\n",
    "            \n",
    "                    for i in range(len(lat_splits) - 1):\n",
    "                        lat_min_split = lat_splits[i]\n",
    "                        lat_max_split = lat_splits[i + 1]\n",
    "                        lat_mask = (ds_lon_subset.lat >= lat_min_split) & (ds_lon_subset.lat < lat_max_split)\n",
    "                        ds_lat_subset = ds_lon_subset.where(lat_mask, drop=True)\n",
    "                        \n",
    "                        output_folder = '../CONUS404_data/LST/UTC/U' + str(offset)+str(i) + '/'\n",
    "                        output_file = f'PREC_ACC_NC.wrf2d_d01_{year}-{month_str}-{day_str}.nc'\n",
    "                        output_path = os.path.join(output_folder, output_file)\n",
    "                        os.makedirs(output_folder, exist_ok=True)\n",
    "                        ds_lat_subset.to_netcdf(output_folder + output_file)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df60ff8-0058-4436-83bf-dd1bd6795dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-02 20:42:24\n",
      "2024-11-02 20:56:17\n",
      "2024-11-02 21:10:04\n",
      "2024-11-02 21:24:24\n",
      "2024-11-02 21:39:04\n",
      "2024-11-02 21:52:53\n",
      "2024-11-02 22:07:19\n",
      "2024-11-02 22:20:30\n",
      "2024-11-02 22:35:18\n",
      "2024-11-02 22:48:55\n",
      "2024-11-02 23:03:47\n",
      "2024-11-02 23:18:10\n",
      "2024-11-02 23:31:58\n",
      "2024-11-02 23:45:55\n",
      "2024-11-02 23:59:05\n",
      "2024-11-03 00:12:43\n",
      "2024-11-03 00:26:27\n",
      "2024-11-03 00:38:52\n",
      "2024-11-03 00:52:25\n",
      "2024-11-03 01:06:17\n",
      "2024-11-03 01:20:29\n",
      "2024-11-03 01:35:09\n",
      "2024-11-03 01:49:26\n",
      "2024-11-03 01:03:57\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file('../../tl_2019_us_state/tl_2019_us_state.shp')\n",
    "input_folder = '/N/project/Zli_lab/Data/Observations/NCAR/CONUS404_T_dT/TarFiles/'\n",
    "\n",
    "start_year = 2015\n",
    "end_year = start_year+3\n",
    "for year in range(start_year, end_year):  # 1989不包含\n",
    "\n",
    "    months = range(10, 13) if year == start_year else range(1, 10) if year == (end_year - 1) else range(1, 13)\n",
    "    # 遍历月份\n",
    "    for month in months:\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "        # 获取当前月份的天数\n",
    "        if month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "            num_days = 31\n",
    "        elif month in [4, 6, 9, 11]:\n",
    "            num_days = 30\n",
    "        elif month == 2:\n",
    "            # 考虑闰年\n",
    "            if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
    "                num_days = 29  # 闰年\n",
    "            else:\n",
    "                num_days = 28  # 平年\n",
    "\n",
    "        # 遍历每个月的天数\n",
    "        for day in range(1, num_days + 1):\n",
    "            \n",
    "            month_str = f\"{month:02}\"\n",
    "            day_str = f\"{day:02}\"\n",
    "            input_file = f'765041.T2.wrf2d_d01_{year}-{month_str}-{day_str}_*.nc'\n",
    "            ds = xr.open_mfdataset(input_folder + input_file)\n",
    "        # 提取CONUS数据\n",
    "            lon = ds['XLONG'].values\n",
    "            lat = ds['XLAT'].values\n",
    "            grid = gpd.GeoDataFrame(\n",
    "                geometry=gpd.points_from_xy(lon.flatten(), lat.flatten()),\n",
    "                index=np.arange(lon.size)\n",
    "            )\n",
    "            grid.set_crs(gdf.crs, inplace=True)\n",
    "            grid_s = gpd.sjoin(grid, gdf, how='inner', predicate='within')\n",
    "            \n",
    "            mask = np.full(ds['T2'].shape[1:], False) \n",
    "            for index in grid_s.index:\n",
    "                row, col = np.unravel_index(index, mask.shape)  # 获取行列索引\n",
    "                mask[row, col] = True\n",
    "            mask_da = xr.DataArray(mask, dims=ds['T2'].dims[1:], coords={'south_north': ds['T2'].coords['south_north'], 'west_east': ds['T2'].coords['west_east']})\n",
    "            ds_s = ds.where(mask_da, drop=True)\n",
    "            lonn = np.linspace(-124.848, -66.885, 1137)\n",
    "            latt = np.linspace(24.396, 49.384, 708)\n",
    "            prec = ds_s['T2'].values\n",
    "            lat_min = latt.min()\n",
    "            lat_max = latt.max()\n",
    "            ds_sss = xr.Dataset({'t2': (['time', 'lat', 'lon'], prec)},\n",
    "                                coords={'lon': (['lon'], lonn),\n",
    "                                        'lat': (['lat'], latt),\n",
    "                                        'time': ('time', ds_s.Time.values)})\n",
    "\n",
    "            original_times = ds_sss.time.values \n",
    "            \n",
    "            lon_ranges = [(-np.inf, -112.5), (-112.5, -97.5), (-97.5, -82.5), (-82.5, np.inf)]\n",
    "            utc_offsets = [-8, -7, -6, -5]\n",
    "            \n",
    "            for (lon_min, lon_max), offset in zip(lon_ranges, utc_offsets):\n",
    "                mask = (ds_sss.lon >= lon_min) & (ds_sss.lon < lon_max)\n",
    "                ds_lon_subset = ds_sss.where(mask, drop=True)\n",
    "                if ds_lon_subset.lat.size > 0 and ds_lon_subset.lon.size > 0:\n",
    "            \n",
    "                    adjusted_times = original_times + np.timedelta64(offset, 'h')  # 保持24个时间点\n",
    "            \n",
    "                    ds_lon_subset = ds_lon_subset.assign_coords(time=adjusted_times)\n",
    "            \n",
    "                    lat_min = ds_lon_subset.lat.min().values\n",
    "                    lat_max = ds_lon_subset.lat.max().values\n",
    "            \n",
    "                    lat_splits = np.linspace(lat_min, lat_max, 10)  # 10个值分9段\n",
    "            \n",
    "                    for i in range(len(lat_splits) - 1):\n",
    "                        lat_min_split = lat_splits[i]\n",
    "                        lat_max_split = lat_splits[i + 1]\n",
    "                        lat_mask = (ds_lon_subset.lat >= lat_min_split) & (ds_lon_subset.lat < lat_max_split)\n",
    "                        ds_lat_subset = ds_lon_subset.where(lat_mask, drop=True)\n",
    "                        \n",
    "                        output_folder = '../CONUS404_data/LST/UTC/U' + str(offset)+str(i) + '/'\n",
    "                        output_file = f'T2.wrf2d_d01_{year}-{month_str}-{day_str}.nc'\n",
    "                        output_path = os.path.join(output_folder, output_file)\n",
    "                        os.makedirs(output_folder, exist_ok=True)\n",
    "                        ds_lat_subset.to_netcdf(output_folder + output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec5f988-6290-4aad-bd70-1fc5dce54c31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-03 01:17:17\n",
      "2024-11-03 01:32:56\n",
      "2024-11-03 01:46:46\n",
      "2024-11-03 02:02:05\n",
      "2024-11-03 02:16:54\n",
      "2024-11-03 02:30:34\n",
      "2024-11-03 02:43:49\n",
      "2024-11-03 02:57:29\n",
      "2024-11-03 03:11:18\n",
      "2024-11-03 03:24:55\n",
      "2024-11-03 03:38:55\n",
      "2024-11-03 03:53:22\n",
      "2024-11-03 04:06:39\n",
      "2024-11-03 04:21:14\n",
      "2024-11-03 04:34:59\n",
      "2024-11-03 04:49:14\n",
      "2024-11-03 05:03:02\n",
      "2024-11-03 05:15:39\n",
      "2024-11-03 05:29:04\n",
      "2024-11-03 05:41:32\n",
      "2024-11-03 05:54:20\n",
      "2024-11-03 06:06:34\n",
      "2024-11-03 06:19:28\n",
      "2024-11-03 06:28:01\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file('../../tl_2019_us_state/tl_2019_us_state.shp')\n",
    "input_folder = '/N/project/Zli_lab/Data/Observations/NCAR/CONUS404_T_dT/TarFiles/'\n",
    "\n",
    "start_year = 2015\n",
    "end_year = start_year+3\n",
    "for year in range(start_year, end_year):  # 1989不包含\n",
    "\n",
    "    months = range(10, 13) if year == start_year else range(1, 10) if year == (end_year - 1) else range(1, 13)\n",
    "\n",
    "    for month in months:\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "\n",
    "        if month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "            num_days = 31\n",
    "        elif month in [4, 6, 9, 11]:\n",
    "            num_days = 30\n",
    "        elif month == 2:\n",
    "\n",
    "            if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
    "                num_days = 29  \n",
    "            else:\n",
    "                num_days = 28  \n",
    "\n",
    "        for day in range(1, num_days + 1):\n",
    "            \n",
    "            month_str = f\"{month:02}\"\n",
    "            day_str = f\"{day:02}\"\n",
    "            input_file = f'765041.TD2.wrf2d_d01_{year}-{month_str}-{day_str}_*.nc'\n",
    "            ds = xr.open_mfdataset(input_folder + input_file)\n",
    "\n",
    "            lon = ds['XLONG'].values\n",
    "            lat = ds['XLAT'].values\n",
    "            grid = gpd.GeoDataFrame(\n",
    "                geometry=gpd.points_from_xy(lon.flatten(), lat.flatten()),\n",
    "                index=np.arange(lon.size)\n",
    "            )\n",
    "            grid.set_crs(gdf.crs, inplace=True)\n",
    "            grid_s = gpd.sjoin(grid, gdf, how='inner', predicate='within')\n",
    "            \n",
    "            mask = np.full(ds['TD2'].shape[1:], False) \n",
    "            for index in grid_s.index:\n",
    "                row, col = np.unravel_index(index, mask.shape)  # 获取行列索引\n",
    "                mask[row, col] = True\n",
    "            mask_da = xr.DataArray(mask, dims=ds['TD2'].dims[1:], coords={'south_north': ds['TD2'].coords['south_north'], 'west_east': ds['TD2'].coords['west_east']})\n",
    "            ds_s = ds.where(mask_da, drop=True)\n",
    "            lonn = np.linspace(-124.848, -66.885, 1137)\n",
    "            latt = np.linspace(24.396, 49.384, 708)\n",
    "            prec = ds_s['TD2'].values\n",
    "            lat_min = latt.min()\n",
    "            lat_max = latt.max()\n",
    "            ds_sss = xr.Dataset({'td2': (['time', 'lat', 'lon'], prec)},\n",
    "                                coords={'lon': (['lon'], lonn),\n",
    "                                        'lat': (['lat'], latt),\n",
    "                                        'time': ('time', ds_s.Time.values)})\n",
    "\n",
    "            original_times = ds_sss.time.values \n",
    "            \n",
    "            lon_ranges = [(-np.inf, -112.5), (-112.5, -97.5), (-97.5, -82.5), (-82.5, np.inf)]\n",
    "            utc_offsets = [-8, -7, -6, -5]\n",
    "            \n",
    "            for (lon_min, lon_max), offset in zip(lon_ranges, utc_offsets):\n",
    "                mask = (ds_sss.lon >= lon_min) & (ds_sss.lon < lon_max)\n",
    "                ds_lon_subset = ds_sss.where(mask, drop=True)\n",
    "                if ds_lon_subset.lat.size > 0 and ds_lon_subset.lon.size > 0:\n",
    "            \n",
    "                    adjusted_times = original_times + np.timedelta64(offset, 'h')  # 保持24个时间点\n",
    "            \n",
    "                    ds_lon_subset = ds_lon_subset.assign_coords(time=adjusted_times)\n",
    "            \n",
    "                    lat_min = ds_lon_subset.lat.min().values\n",
    "                    lat_max = ds_lon_subset.lat.max().values\n",
    "            \n",
    "                    lat_splits = np.linspace(lat_min, lat_max, 10)  # 10个值分9段\n",
    "            \n",
    "                    for i in range(len(lat_splits) - 1):\n",
    "                        lat_min_split = lat_splits[i]\n",
    "                        lat_max_split = lat_splits[i + 1]\n",
    "                        lat_mask = (ds_lon_subset.lat >= lat_min_split) & (ds_lon_subset.lat < lat_max_split)\n",
    "                        ds_lat_subset = ds_lon_subset.where(lat_mask, drop=True)\n",
    "                        \n",
    "                        output_folder = '../CONUS404_data/LST/UTC/U' + str(offset)+str(i) + '/'\n",
    "                        output_file = f'TD2.wrf2d_d01_{year}-{month_str}-{day_str}.nc'\n",
    "                        output_path = os.path.join(output_folder, output_file)\n",
    "                        os.makedirs(output_folder, exist_ok=True)\n",
    "                        ds_lat_subset.to_netcdf(output_folder + output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e127bfeb-f2c0-475e-93ac-6a682488dcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 01:45:01\n",
      "2025-02-17 01:45:34\n",
      "2025-02-17 01:46:04\n",
      "2025-02-17 01:46:31\n",
      "2025-02-17 01:47:03\n",
      "2025-02-17 01:47:29\n",
      "2025-02-17 01:47:57\n",
      "2025-02-17 01:48:26\n",
      "2025-02-17 01:48:51\n",
      "2025-02-17 01:49:28\n",
      "2025-02-17 01:49:59\n",
      "2025-02-17 01:50:27\n",
      "2025-02-17 01:50:53\n",
      "2025-02-17 01:51:22\n",
      "2025-02-17 01:51:48\n",
      "2025-02-17 01:52:16\n",
      "2025-02-17 01:52:41\n",
      "2025-02-17 01:53:06\n",
      "2025-02-17 01:53:34\n",
      "2025-02-17 01:54:00\n",
      "2025-02-17 01:54:28\n",
      "2025-02-17 01:54:55\n",
      "2025-02-17 01:55:25\n",
      "2025-02-17 01:55:56\n",
      "2025-02-17 01:56:24\n",
      "2025-02-17 01:56:48\n",
      "2025-02-17 01:57:13\n",
      "2025-02-17 01:57:36\n",
      "2025-02-17 01:57:54\n",
      "2025-02-17 01:58:17\n",
      "2025-02-17 01:58:43\n",
      "2025-02-17 01:59:11\n",
      "2025-02-17 01:59:15\n",
      "2025-02-17 01:59:41\n",
      "2025-02-17 02:00:10\n",
      "2025-02-17 02:00:36\n",
      "2025-02-17 02:01:03\n",
      "2025-02-17 02:01:32\n",
      "2025-02-17 02:02:01\n",
      "2025-02-17 02:02:29\n",
      "2025-02-17 02:02:54\n",
      "2025-02-17 02:03:27\n",
      "2025-02-17 02:03:56\n",
      "2025-02-17 02:04:19\n",
      "2025-02-17 02:04:40\n",
      "2025-02-17 02:05:02\n",
      "2025-02-17 02:05:28\n",
      "2025-02-17 02:05:52\n",
      "2025-02-17 02:06:19\n",
      "2025-02-17 02:06:44\n",
      "2025-02-17 02:07:14\n",
      "2025-02-17 02:07:37\n",
      "2025-02-17 02:07:58\n",
      "2025-02-17 02:08:26\n",
      "2025-02-17 02:08:56\n",
      "2025-02-17 02:09:27\n",
      "2025-02-17 02:10:03\n",
      "2025-02-17 02:10:32\n",
      "2025-02-17 02:11:03\n",
      "2025-02-17 02:11:37\n",
      "2025-02-17 02:12:09\n",
      "2025-02-17 02:12:40\n",
      "2025-02-17 02:13:14\n",
      "2025-02-17 02:13:18\n",
      "2025-02-17 02:13:46\n",
      "2025-02-17 02:14:15\n",
      "2025-02-17 02:14:45\n",
      "2025-02-17 02:15:20\n",
      "2025-02-17 02:15:57\n",
      "2025-02-17 02:16:33\n",
      "2025-02-17 02:17:07\n",
      "2025-02-17 02:17:41\n",
      "2025-02-17 02:18:16\n",
      "2025-02-17 02:18:51\n",
      "2025-02-17 02:19:22\n",
      "2025-02-17 02:20:03\n",
      "2025-02-17 02:20:38\n",
      "2025-02-17 02:21:13\n",
      "2025-02-17 02:21:44\n",
      "2025-02-17 02:22:20\n",
      "2025-02-17 02:22:55\n",
      "2025-02-17 02:23:25\n",
      "2025-02-17 02:23:47\n",
      "2025-02-17 02:24:16\n",
      "2025-02-17 02:24:45\n",
      "2025-02-17 02:25:18\n",
      "2025-02-17 02:25:49\n",
      "2025-02-17 02:26:20\n",
      "2025-02-17 02:26:58\n",
      "2025-02-17 02:27:34\n",
      "2025-02-17 02:28:11\n",
      "2025-02-17 02:28:45\n",
      "2025-02-17 02:29:18\n",
      "2025-02-17 02:29:44\n",
      "2025-02-17 02:30:18\n",
      "2025-02-17 02:30:23\n",
      "2025-02-17 02:30:53\n",
      "2025-02-17 02:31:24\n",
      "2025-02-17 02:31:56\n",
      "2025-02-17 02:32:28\n",
      "2025-02-17 02:32:51\n",
      "2025-02-17 02:33:20\n",
      "2025-02-17 02:33:47\n",
      "2025-02-17 02:34:16\n",
      "2025-02-17 02:34:48\n",
      "2025-02-17 02:35:14\n",
      "2025-02-17 02:35:44\n",
      "2025-02-17 02:36:12\n",
      "2025-02-17 02:36:45\n",
      "2025-02-17 02:37:12\n",
      "2025-02-17 02:37:34\n",
      "2025-02-17 02:38:05\n",
      "2025-02-17 02:38:33\n",
      "2025-02-17 02:39:05\n",
      "2025-02-17 02:39:35\n",
      "2025-02-17 02:40:07\n",
      "2025-02-17 02:40:38\n",
      "2025-02-17 02:41:09\n",
      "2025-02-17 02:41:39\n",
      "2025-02-17 02:42:11\n",
      "2025-02-17 02:42:40\n",
      "2025-02-17 02:43:08\n",
      "2025-02-17 02:43:40\n",
      "2025-02-17 02:44:12\n",
      "2025-02-17 02:44:44\n",
      "2025-02-17 02:45:16\n",
      "2025-02-17 02:45:44\n",
      "2025-02-17 02:45:48\n",
      "2025-02-17 02:46:12\n",
      "2025-02-17 02:46:43\n",
      "2025-02-17 02:47:10\n",
      "2025-02-17 02:47:41\n",
      "2025-02-17 02:48:12\n",
      "2025-02-17 02:48:44\n",
      "2025-02-17 02:49:15\n",
      "2025-02-17 02:49:43\n",
      "2025-02-17 02:50:16\n",
      "2025-02-17 02:50:51\n",
      "2025-02-17 02:51:20\n",
      "2025-02-17 02:51:52\n",
      "2025-02-17 02:52:23\n",
      "2025-02-17 02:52:54\n",
      "2025-02-17 02:53:25\n",
      "2025-02-17 02:54:01\n",
      "2025-02-17 02:54:36\n",
      "2025-02-17 02:55:09\n",
      "2025-02-17 02:55:43\n",
      "2025-02-17 02:56:14\n",
      "2025-02-17 02:56:41\n",
      "2025-02-17 02:57:07\n",
      "2025-02-17 02:57:39\n",
      "2025-02-17 02:58:14\n",
      "2025-02-17 02:58:50\n",
      "2025-02-17 02:59:18\n",
      "2025-02-17 02:59:46\n",
      "2025-02-17 03:00:24\n",
      "2025-02-17 03:00:29\n",
      "2025-02-17 03:00:59\n",
      "2025-02-17 03:01:32\n",
      "2025-02-17 03:02:05\n",
      "2025-02-17 03:02:41\n",
      "2025-02-17 03:03:11\n",
      "2025-02-17 03:03:45\n",
      "2025-02-17 03:04:17\n",
      "2025-02-17 03:04:54\n",
      "2025-02-17 03:05:23\n",
      "2025-02-17 03:05:54\n",
      "2025-02-17 03:06:27\n",
      "2025-02-17 03:06:56\n",
      "2025-02-17 03:07:28\n",
      "2025-02-17 03:08:00\n",
      "2025-02-17 03:08:30\n",
      "2025-02-17 03:09:00\n",
      "2025-02-17 03:09:34\n",
      "2025-02-17 03:10:05\n",
      "2025-02-17 03:10:36\n",
      "2025-02-17 03:11:09\n",
      "2025-02-17 03:11:45\n",
      "2025-02-17 03:12:16\n",
      "2025-02-17 03:12:51\n",
      "2025-02-17 03:13:22\n",
      "2025-02-17 03:13:51\n",
      "2025-02-17 03:14:27\n",
      "2025-02-17 03:15:00\n",
      "2025-02-17 03:15:32\n",
      "2025-02-17 03:16:03\n",
      "2025-02-17 03:16:41\n",
      "2025-02-17 03:17:16\n",
      "2025-02-17 03:17:20\n",
      "2025-02-17 03:17:47\n",
      "2025-02-17 03:18:24\n",
      "2025-02-17 03:18:59\n",
      "2025-02-17 03:19:34\n",
      "2025-02-17 03:20:06\n",
      "2025-02-17 03:20:43\n",
      "2025-02-17 03:21:12\n",
      "2025-02-17 03:21:45\n",
      "2025-02-17 03:22:19\n",
      "2025-02-17 03:22:55\n",
      "2025-02-17 03:23:30\n",
      "2025-02-17 03:24:05\n",
      "2025-02-17 03:24:36\n",
      "2025-02-17 03:25:02\n",
      "2025-02-17 03:25:30\n",
      "2025-02-17 03:26:00\n",
      "2025-02-17 03:26:30\n",
      "2025-02-17 03:27:04\n",
      "2025-02-17 03:27:39\n",
      "2025-02-17 03:28:07\n",
      "2025-02-17 03:28:41\n",
      "2025-02-17 03:29:13\n",
      "2025-02-17 03:29:44\n",
      "2025-02-17 03:30:18\n",
      "2025-02-17 03:30:45\n",
      "2025-02-17 03:31:12\n",
      "2025-02-17 03:31:43\n",
      "2025-02-17 03:32:09\n",
      "2025-02-17 03:32:37\n",
      "2025-02-17 03:33:07\n",
      "2025-02-17 03:33:10\n",
      "2025-02-17 03:33:33\n",
      "2025-02-17 03:34:01\n",
      "2025-02-17 03:34:34\n",
      "2025-02-17 03:35:05\n",
      "2025-02-17 03:35:30\n",
      "2025-02-17 03:36:03\n",
      "2025-02-17 03:36:29\n",
      "2025-02-17 03:36:52\n",
      "2025-02-17 03:37:23\n",
      "2025-02-17 03:37:50\n",
      "2025-02-17 03:38:16\n",
      "2025-02-17 03:38:52\n",
      "2025-02-17 03:39:24\n",
      "2025-02-17 03:39:52\n",
      "2025-02-17 03:40:18\n",
      "2025-02-17 03:40:45\n",
      "2025-02-17 03:41:09\n",
      "2025-02-17 03:41:38\n",
      "2025-02-17 03:42:14\n",
      "2025-02-17 03:42:40\n",
      "2025-02-17 03:43:11\n",
      "2025-02-17 03:43:48\n",
      "2025-02-17 03:44:26\n",
      "2025-02-17 03:44:50\n",
      "2025-02-17 03:45:19\n",
      "2025-02-17 03:45:43\n",
      "2025-02-17 03:46:05\n",
      "2025-02-17 03:46:36\n",
      "2025-02-17 03:47:00\n",
      "2025-02-17 03:47:22\n",
      "2025-02-17 03:47:48\n",
      "2025-02-17 03:47:51\n",
      "2025-02-17 03:48:14\n",
      "2025-02-17 03:48:43\n",
      "2025-02-17 03:49:12\n",
      "2025-02-17 03:49:40\n",
      "2025-02-17 03:50:10\n",
      "2025-02-17 03:50:39\n",
      "2025-02-17 03:51:09\n",
      "2025-02-17 03:51:42\n",
      "2025-02-17 03:52:09\n",
      "2025-02-17 03:52:38\n",
      "2025-02-17 03:53:07\n",
      "2025-02-17 03:53:35\n",
      "2025-02-17 03:54:03\n",
      "2025-02-17 03:54:32\n",
      "2025-02-17 03:55:06\n",
      "2025-02-17 03:55:33\n",
      "2025-02-17 03:56:02\n",
      "2025-02-17 03:56:30\n",
      "2025-02-17 03:56:59\n",
      "2025-02-17 03:57:24\n",
      "2025-02-17 03:57:51\n",
      "2025-02-17 03:58:22\n",
      "2025-02-17 03:58:52\n",
      "2025-02-17 03:59:13\n",
      "2025-02-17 03:59:43\n",
      "2025-02-17 04:00:01\n",
      "2025-02-17 04:00:25\n",
      "2025-02-17 04:00:52\n",
      "2025-02-17 04:01:18\n",
      "2025-02-17 04:01:40\n",
      "2025-02-17 04:01:44\n",
      "2025-02-17 04:02:07\n",
      "2025-02-17 04:02:40\n",
      "2025-02-17 04:03:06\n",
      "2025-02-17 04:03:47\n",
      "2025-02-17 04:04:15\n",
      "2025-02-17 04:04:44\n",
      "2025-02-17 04:05:14\n",
      "2025-02-17 04:06:09\n",
      "2025-02-17 04:06:39\n",
      "2025-02-17 04:07:08\n",
      "2025-02-17 04:07:28\n",
      "2025-02-17 04:07:59\n",
      "2025-02-17 04:08:37\n",
      "2025-02-17 04:09:02\n",
      "2025-02-17 04:09:27\n",
      "2025-02-17 04:09:56\n",
      "2025-02-17 04:10:29\n",
      "2025-02-17 04:10:53\n",
      "2025-02-17 04:11:27\n",
      "2025-02-17 04:11:52\n",
      "2025-02-17 04:12:18\n",
      "2025-02-17 04:12:45\n",
      "2025-02-17 04:13:12\n",
      "2025-02-17 04:13:42\n",
      "2025-02-17 04:14:11\n",
      "2025-02-17 04:14:41\n",
      "2025-02-17 04:15:09\n",
      "2025-02-17 04:15:37\n",
      "2025-02-17 04:16:04\n",
      "2025-02-17 04:16:32\n",
      "2025-02-17 04:16:58\n",
      "2025-02-17 04:17:01\n",
      "2025-02-17 04:17:26\n",
      "2025-02-17 04:17:54\n",
      "2025-02-17 04:18:20\n",
      "2025-02-17 04:18:43\n",
      "2025-02-17 04:19:11\n",
      "2025-02-17 04:19:37\n",
      "2025-02-17 04:20:12\n",
      "2025-02-17 04:20:41\n",
      "2025-02-17 04:21:09\n",
      "2025-02-17 04:21:36\n",
      "2025-02-17 04:22:04\n",
      "2025-02-17 04:22:33\n",
      "2025-02-17 04:22:57\n",
      "2025-02-17 04:23:28\n",
      "2025-02-17 04:24:00\n",
      "2025-02-17 04:24:31\n",
      "2025-02-17 04:25:05\n",
      "2025-02-17 04:25:32\n",
      "2025-02-17 04:26:04\n",
      "2025-02-17 04:26:36\n",
      "2025-02-17 04:27:03\n",
      "2025-02-17 04:27:38\n",
      "2025-02-17 04:28:09\n",
      "2025-02-17 04:28:35\n",
      "2025-02-17 04:29:06\n",
      "2025-02-17 04:29:37\n",
      "2025-02-17 04:30:06\n",
      "2025-02-17 04:30:37\n",
      "2025-02-17 04:31:11\n",
      "2025-02-17 04:31:38\n",
      "2025-02-17 04:32:07\n",
      "2025-02-17 04:32:10\n",
      "2025-02-17 04:32:38\n",
      "2025-02-17 04:33:07\n",
      "2025-02-17 04:33:34\n",
      "2025-02-17 04:34:01\n",
      "2025-02-17 04:34:31\n",
      "2025-02-17 04:35:01\n",
      "2025-02-17 04:35:30\n",
      "2025-02-17 04:35:58\n",
      "2025-02-17 04:36:26\n",
      "2025-02-17 04:36:47\n",
      "2025-02-17 04:37:09\n",
      "2025-02-17 04:37:38\n",
      "2025-02-17 04:38:06\n",
      "2025-02-17 04:38:35\n",
      "2025-02-17 04:39:04\n",
      "2025-02-17 04:39:38\n",
      "2025-02-17 04:40:11\n",
      "2025-02-17 04:40:41\n",
      "2025-02-17 04:41:05\n",
      "2025-02-17 04:41:41\n",
      "2025-02-17 04:42:17\n",
      "2025-02-17 04:42:50\n",
      "2025-02-17 04:43:20\n",
      "2025-02-17 04:43:52\n",
      "2025-02-17 04:44:25\n",
      "2025-02-17 04:44:54\n",
      "2025-02-17 04:45:29\n",
      "2025-02-17 04:46:06\n",
      "2025-02-17 04:46:39\n",
      "2025-02-17 04:47:09\n",
      "2025-02-17 04:47:11\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "no files to open",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m day_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mday\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m input_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPREC_ACC_NC.wrf2d_d01_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mday_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_*.nc\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 30\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m ds_era \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mDataset({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m: ([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m], ds\u001b[38;5;241m.\u001b[39mPREC_ACC_NC\u001b[38;5;241m.\u001b[39mvalues)},\n\u001b[1;32m     32\u001b[0m                     coords\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m: ([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m], ds\u001b[38;5;241m.\u001b[39mXLONG\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m500\u001b[39m]),\n\u001b[1;32m     33\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m: ([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m], ds\u001b[38;5;241m.\u001b[39mXLAT\u001b[38;5;241m.\u001b[39mvalues[:,\u001b[38;5;241m500\u001b[39m]),\n\u001b[1;32m     34\u001b[0m                             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, ds\u001b[38;5;241m.\u001b[39mTime\u001b[38;5;241m.\u001b[39mvalues)})\n\u001b[1;32m     35\u001b[0m ds_era_lon, ds_era_lat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(ds_era\u001b[38;5;241m.\u001b[39mlongitude\u001b[38;5;241m.\u001b[39mvalues, ds_era\u001b[38;5;241m.\u001b[39mlatitude\u001b[38;5;241m.\u001b[39mvalues, indexing\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/xarray/backends/api.py:1042\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   1039\u001b[0m paths \u001b[38;5;241m=\u001b[39m _find_absolute_paths(paths, engine\u001b[38;5;241m=\u001b[39mengine, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m paths:\n\u001b[0;32m-> 1042\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno files to open\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m combine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnested\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(concat_dim, (\u001b[38;5;28mstr\u001b[39m, DataArray)) \u001b[38;5;129;01mor\u001b[39;00m concat_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: no files to open"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file('/N/project/Zli_lab/Data/Other/tl_2019_us_state/tl_2019_us_state.shp')\n",
    "input_folder = '/N/project/Zli_lab/Data/Observations/NCAR/prec_acc_files/'\n",
    "\n",
    "start_year = 2021\n",
    "end_year = start_year+3\n",
    "for year in range(start_year, end_year):  # 1989不包含\n",
    "\n",
    "    months = range(10, 13) if year == start_year else range(1, 10) if year == (end_year - 1) else range(1, 13)\n",
    "    # 遍历月份\n",
    "    for month in months:\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "        # 获取当前月份的天数\n",
    "        if month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "            num_days = 31\n",
    "        elif month in [4, 6, 9, 11]:\n",
    "            num_days = 30\n",
    "        elif month == 2:\n",
    "            # 考虑闰年\n",
    "            if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
    "                num_days = 29  # 闰年\n",
    "            else:\n",
    "                num_days = 28  # 平年\n",
    "\n",
    "        # 遍历每个月的天数\n",
    "        for day in range(1, num_days + 1):\n",
    "            \n",
    "            month_str = f\"{month:02}\"\n",
    "            day_str = f\"{day:02}\"\n",
    "            input_file = f'PREC_ACC_NC.wrf2d_d01_{year}-{month_str}-{day_str}_*.nc'\n",
    "            ds = xr.open_mfdataset(input_folder + input_file)\n",
    "            ds_era = xr.Dataset({'p': (['time', 'latitude', 'longitude'], ds.PREC_ACC_NC.values)},\n",
    "                                coords={'longitude': (['longitude'], ds.XLONG.values[500]),\n",
    "                                        'latitude': (['latitude'], ds.XLAT.values[:,500]),\n",
    "                                        'time': ('time', ds.Time.values)})\n",
    "            ds_era_lon, ds_era_lat = np.meshgrid(ds_era.longitude.values, ds_era.latitude.values, indexing='xy')\n",
    "            # 转换为 xarray DataArray，确保其维度与 ds_era_clipped 对齐\n",
    "            ds_era_lon_da = xr.DataArray(ds_era_lon, dims=(\"latitude\", \"longitude\"), coords={\"latitude\": ds_era.latitude, \"longitude\": ds_era.longitude})\n",
    "            ds_era_lat_da = xr.DataArray(ds_era_lat, dims=(\"latitude\", \"longitude\"), coords={\"latitude\": ds_era.latitude, \"longitude\": ds_era.longitude})\n",
    "\n",
    "            # 使用 assign_coords 将二维坐标添加到 ds_era_clipped\n",
    "            ds_era_clipped = ds_era.assign_coords(lon_2d=ds_era_lon_da, lat_2d=ds_era_lat_da)\n",
    "\n",
    "            lon = ds_era_clipped['lon_2d'].values\n",
    "            lat = ds_era_clipped['lat_2d'].values\n",
    "            grid = gpd.GeoDataFrame(\n",
    "                geometry=gpd.points_from_xy(lon.flatten(), lat.flatten()),\n",
    "                index=np.arange(lon.size)\n",
    "            )\n",
    "            grid.set_crs(gdf.crs, inplace=True)\n",
    "            grid_s = gpd.sjoin(grid, gdf, how='inner', predicate='within')\n",
    "\n",
    "            mask = np.full(ds_era_clipped['p'].shape[1:], False) \n",
    "            for index in grid_s.index:\n",
    "                row, col = np.unravel_index(index, mask.shape)  # 获取行列索引\n",
    "                mask[row, col] = True\n",
    "            mask_da = xr.DataArray(mask, dims=ds_era_clipped['p'].dims[1:], coords={'latitude': ds_era_clipped['p'].coords['latitude'], 'longitude': ds_era_clipped['p'].coords['longitude']})\n",
    "            ds_sss = ds_era_clipped.where(mask_da, drop=True)\n",
    "            ds_sss = ds_sss.drop_vars(['lon_2d', 'lat_2d'])\n",
    "\n",
    "\n",
    "            original_times = ds_sss.time.values \n",
    "            \n",
    "            lon_ranges = [(-np.inf, -112.5), (-112.5, -97.5), (-97.5, -82.5), (-82.5, np.inf)]\n",
    "            utc_offsets = [-8, -7, -6, -5]\n",
    "            print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "            for (lon_min, lon_max), offset in zip(lon_ranges, utc_offsets):\n",
    "                mask = (ds_sss.longitude >= lon_min) & (ds_sss.longitude < lon_max)\n",
    "                ds_lon_subset = ds_sss.where(mask, drop=True)\n",
    "                if ds_lon_subset.latitude.size > 0 and ds_lon_subset.longitude.size > 0:\n",
    "            \n",
    "                    adjusted_times = original_times + np.timedelta64(offset, 'h')  # 保持24个时间点\n",
    "            \n",
    "                    ds_lon_subset = ds_lon_subset.assign_coords(time=adjusted_times)\n",
    "            \n",
    "                    lat_min = ds_lon_subset.latitude.min().values\n",
    "                    lat_max = ds_lon_subset.latitude.max().values\n",
    "            \n",
    "                    lat_splits = np.linspace(lat_min, lat_max, 10)  # 10个值分9段\n",
    "                    \n",
    "                    for i in range(len(lat_splits) - 1):\n",
    "                        lat_min_split = lat_splits[i]\n",
    "                        lat_max_split = lat_splits[i + 1]\n",
    "                        lat_mask = (ds_lon_subset.latitude >= lat_min_split) & (ds_lon_subset.latitude < lat_max_split)\n",
    "                        ds_lat_subset = ds_lon_subset.where(lat_mask, drop=True)\n",
    "                        \n",
    "                        output_folder = '/N/project/Zli_lab/gongg/CONUS404_data/LST/re_UTC/U' + str(offset)+str(i) + '/'\n",
    "                        output_file = f'PREC_ACC_NC.wrf2d_d01_{year}-{month_str}-{day_str}.nc'\n",
    "                        output_path = os.path.join(output_folder, output_file)\n",
    "                        os.makedirs(output_folder, exist_ok=True)\n",
    "                        ds_lat_subset.to_netcdf(output_folder + output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87de58a-3c22-4164-b0cc-83d6108b4d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
