{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103346f4-b594-470a-a7dc-d38252d216ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "### lvl 2 setups (systerm)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib as mpl\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pylab import *\n",
    "from matplotlib.colors import ListedColormap,LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.patches import Wedge, Circle\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import datetime\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f5fc57-df17-4efb-9f6f-c68503e00f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RxHhr(prec, latt, lonn, n):\n",
    "\n",
    "    arr = prec.reshape(43, 2208, prec.shape[1], prec.shape[2])\n",
    "\n",
    "    max_prec = np.full((43, prec.shape[1], prec.shape[2]), np.nan)\n",
    "\n",
    "    for year in range(43):\n",
    "        for i in range(prec.shape[1]):\n",
    "            for j in range(prec.shape[2]):\n",
    "                sliding_windows = np.lib.stride_tricks.sliding_window_view(arr[year, :, i, j], n)\n",
    "                window_sums = np.sum(sliding_windows, axis=1)\n",
    "                local_max = np.max(window_sums)\n",
    "\n",
    "                if local_max > 1:\n",
    "                    max_prec[year, i, j] = local_max\n",
    "\n",
    "    ds_RxHhr = xr.Dataset(\n",
    "        {'p': (['time', 'lat', 'lon'], max_prec)},\n",
    "        coords={\n",
    "            'time': (['time'], np.arange(1980, 2023)),\n",
    "            'lat': (['lat'], latt),\n",
    "            'lon': (['lon'], lonn)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return ds_RxHhr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94a2c669-6dac-46cc-a9b8-2125e70a7280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Rx1hrP(prec, latt, lonn):\n",
    "    # Reshape the precipitation data to a 5-dimensional array (years, days, hours, lat, lon)\n",
    "    arr = prec.reshape(43, 92, 24, prec.shape[1], prec.shape[2])\n",
    "    \n",
    "    # Initialize the output array for storing percentages\n",
    "    arr_percent = np.full((43, 92, prec.shape[1], prec.shape[2]), np.nan)\n",
    "    \n",
    "    # Loop over years, days, and spatial dimensions\n",
    "    for year in range(43):\n",
    "        for day in range(92):\n",
    "            for i in range(prec.shape[1]):  # Latitude\n",
    "                for j in range(prec.shape[2]):  # Longitude\n",
    "                    # Calculate the daily total precipitation using nansum to ignore NaNs\n",
    "                    daily_total = np.sum(arr[year, day, :, i, j])\n",
    "                    \n",
    "                    # Calculate the maximum hourly precipitation using nanmax to ignore NaNs\n",
    "                    daily_max = np.max(arr[year, day, :, i, j])\n",
    "                    \n",
    "                    # Calculate the percentage if daily total is not zero\n",
    "                    if daily_total > 0:\n",
    "                        arr_percent[year, day, i, j] = (daily_max / daily_total) \n",
    "                    if daily_total == 0:\n",
    "                        arr_percent[year, day, i, j] = 0\n",
    "    # Create an xarray dataset with the arr_percent data and appropriate coordinates\n",
    "    ds_Rx1hrP = xr.Dataset(\n",
    "        {'percent': (['year', 'day', 'lat', 'lon'], arr_percent)},\n",
    "        coords={\n",
    "            'year': (['year'], np.arange(1980, 2023)),\n",
    "            'day': (['day'], np.arange(0, 92)),  # Assuming days are indexed from 1 to 92\n",
    "            'lat': (['lat'], latt),\n",
    "            'lon': (['lon'], lonn)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return ds_Rx1hrP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dbe3281-93fc-41bb-8448-f42f6ca22661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RQpwHhrP(prec, latt, lonn):\n",
    "    # Reshape the precipitation data\n",
    "    arr = prec.reshape(43, 2208, prec.shape[1], prec.shape[2])\n",
    "\n",
    "    # Calculate the mask based on NaN presence in the original data\n",
    "    mask = np.isnan(np.nanmean(prec, axis=0))\n",
    "\n",
    "    # Initialize output arrays\n",
    "    percent_95 = np.full((43, prec.shape[1], prec.shape[2]), np.nan)\n",
    "    percent_99 = np.full((43, prec.shape[1], prec.shape[2]), np.nan)\n",
    "    quantile_95 = np.full((prec.shape[1], prec.shape[2]), np.nan)\n",
    "    quantile_99 = np.full((prec.shape[1], prec.shape[2]), np.nan)\n",
    "\n",
    "    # Processing data\n",
    "    for year in range(43):\n",
    "        for i in range(prec.shape[1]):\n",
    "            for j in range(prec.shape[2]):\n",
    "                # Extract yearly precipitation data\n",
    "                yearly_precip = arr[year, :, i, j]\n",
    "\n",
    "                # Filter wet hours\n",
    "                wet_hours = yearly_precip[yearly_precip >= 0.1]\n",
    "\n",
    "                # Calculate 95% and 99% quantiles\n",
    "                q95 = np.percentile(wet_hours, 95) if len(wet_hours) > 0 else np.nan\n",
    "                q99 = np.percentile(wet_hours, 99) if len(wet_hours) > 0 else np.nan\n",
    "                \n",
    "                quantile_95[i, j] = q95\n",
    "                quantile_99[i, j] = q99\n",
    "                \n",
    "                # Total precipitation for wet hours\n",
    "                total_wet_precip = np.sum(wet_hours)\n",
    "\n",
    "                # Total exceeding 95% and 99% quantiles\n",
    "                sum_over_q95 = np.sum(yearly_precip[yearly_precip >= q95])\n",
    "                sum_over_q99 = np.sum(yearly_precip[yearly_precip >= q99])\n",
    "\n",
    "                # Calculate percentages\n",
    "                if total_wet_precip >= 0.1:\n",
    "                    percent_95[year, i, j] = (sum_over_q95 / total_wet_precip)\n",
    "                    percent_99[year, i, j] = (sum_over_q99 / total_wet_precip)\n",
    "                else:\n",
    "                    percent_95[year, i, j] = 0\n",
    "                    percent_99[year, i, j] = 0\n",
    "\n",
    "    # Apply mask\n",
    "    percent_95[:, mask] = np.nan\n",
    "    percent_99[:, mask] = np.nan\n",
    "\n",
    "    # Create xarray datasets\n",
    "    ds_percent = xr.Dataset(\n",
    "        {\n",
    "            'percent_95': (['year', 'lat', 'lon'], percent_95),\n",
    "            'percent_99': (['year', 'lat', 'lon'], percent_99)\n",
    "        },\n",
    "        coords={\n",
    "            'year': (['year'], np.arange(1980, 2023)),\n",
    "            'lat': (['lat'], latt),\n",
    "            'lon': (['lon'], lonn)\n",
    "        }\n",
    "    )\n",
    "    ds_quantile = xr.Dataset(\n",
    "        {\n",
    "            'q_95': (['lat', 'lon'], quantile_95),\n",
    "            'q_99': (['lat', 'lon'], quantile_99)\n",
    "        },\n",
    "        coords={\n",
    "            'lon': (['lon'], lonn),\n",
    "            'lat': (['lat'], latt),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return ds_percent, ds_quantile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08eb97a6-421e-4dbd-8336-a41d1c38e683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RHhrTmm(prec, latt, lonn, H):\n",
    "\n",
    "    years = 43  # Assuming each year has 2208 hours\n",
    "    # Reshape precipitation data to (years, hours, lat, lon)\n",
    "    arr = prec.reshape(years, 2208, prec.shape[1], prec.shape[2])\n",
    "\n",
    "    # Calculate the mask for NaN values in the original data\n",
    "    mask = np.isnan(np.nanmean(prec, axis=0))\n",
    "\n",
    "    # Initialize the result array\n",
    "    arr_RHhrTmm_10 = np.zeros((years, prec.shape[1], prec.shape[2]), dtype=float)\n",
    "    arr_RHhrTmm_20 = np.zeros((years, prec.shape[1], prec.shape[2]), dtype=float)\n",
    "    arr_RHhrTmm_30 = np.zeros((years, prec.shape[1], prec.shape[2]), dtype=float)\n",
    "    arr_RHhrTmm_50 = np.zeros((years, prec.shape[1], prec.shape[2]), dtype=float)\n",
    "    # Process each year\n",
    "    for year in range(years):\n",
    "        for hour in range(0, 2208, H):\n",
    "            # Calculate summed precipitation over the interval\n",
    "            if hour + H <= 2208:\n",
    "                summed_precip = np.nansum(arr[year, hour:hour + H, :, :], axis=0)\n",
    "            else:\n",
    "                summed_precip = np.nansum(arr[year, hour:2208, :, :], axis=0)\n",
    "\n",
    "            # Check for exceedances over the threshold\n",
    "            exceedances_10 = summed_precip >= 10\n",
    "            exceedances_20 = summed_precip >= 20\n",
    "            exceedances_30 = summed_precip >= 30\n",
    "            exceedances_50 = summed_precip >= 50\n",
    "            arr_RHhrTmm_10[year, :, :] += exceedances_10.astype(int)\n",
    "            arr_RHhrTmm_20[year, :, :] += exceedances_20.astype(int)\n",
    "            arr_RHhrTmm_30[year, :, :] += exceedances_30.astype(int)\n",
    "            arr_RHhrTmm_50[year, :, :] += exceedances_50.astype(int)\n",
    "            \n",
    "            \n",
    "    # Apply the mask to the result array\n",
    "    arr_RHhrTmm_10[:, mask] = np.nan\n",
    "    arr_RHhrTmm_20[:, mask] = np.nan\n",
    "    arr_RHhrTmm_30[:, mask] = np.nan\n",
    "    arr_RHhrTmm_50[:, mask] = np.nan\n",
    "    # Create an xarray dataset to store the results\n",
    "    ds_RHhrTmm = xr.Dataset(\n",
    "        {\n",
    "            'c_10': (['year', 'lat', 'lon'], arr_RHhrTmm_10),\n",
    "            'c_20': (['year', 'lat', 'lon'], arr_RHhrTmm_20),\n",
    "            'c_30': (['year', 'lat', 'lon'], arr_RHhrTmm_30),\n",
    "            'c_50': (['year', 'lat', 'lon'], arr_RHhrTmm_50),\n",
    "        },\n",
    "        coords={\n",
    "            'year': (['year'], np.arange(1980, 1980 + years)),\n",
    "            'lat': (['lat'], latt),\n",
    "            'lon': (['lon'], lonn)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return ds_RHhrTmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f33c0820-f03a-49bb-8ce9-0d4b4ae3751f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def NWH(prec, latt, lonn):\n",
    "    arr = prec.reshape(43, 2208, prec.shape[1], prec.shape[2])\n",
    "    mask = np.isnan(np.nanmean(prec, axis=0))\n",
    "\n",
    "    nwh = np.zeros((43, prec.shape[1], prec.shape[2]))\n",
    "\n",
    "    for year in range(43):\n",
    "        for i in range(prec.shape[1]):\n",
    "            for j in range(prec.shape[2]):\n",
    "                hourly_data = arr[year, :, i, j]\n",
    "                wet_hours = hourly_data >= 0.1\n",
    "                nwh[year, i, j] = np.sum(wet_hours)\n",
    "\n",
    "    nwh[:, mask] = np.nan\n",
    "    ds_NWH = xr.Dataset(\n",
    "        {'c': (['year', 'lat', 'lon'], nwh)},\n",
    "        coords={\n",
    "            'year': (['year'], np.arange(1980, 2023)),\n",
    "            'lat': (['lat'], latt),\n",
    "            'lon': (['lon'], lonn)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return ds_NWH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71882352-1777-44f3-a5ed-d94703635fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MeLWS_MxLWS(prec,latt,lonn):\n",
    "\n",
    "    arr = prec.reshape(43, 2208, prec.shape[1], prec.shape[2])\n",
    "    mask = np.isnan(np.nanmean(prec, axis=0))\n",
    "    MeLWS = np.full((43, prec.shape[1], prec.shape[2]), np.nan)\n",
    "    MxLWS = np.full((43, prec.shape[1], prec.shape[2]), np.nan)\n",
    "\n",
    "    for year in range(43):\n",
    "        for i in range(prec.shape[1]):\n",
    "            for j in range(prec.shape[2]):\n",
    "                yearly_precip = arr[year, :, i, j]\n",
    "                is_wet = yearly_precip >= 0.1\n",
    "                wet_starts = np.where(np.diff(is_wet.astype(int)) == 1)[0] + 1\n",
    "                wet_ends = np.where(np.diff(is_wet.astype(int)) == -1)[0] + 1\n",
    "\n",
    "                if is_wet[0]:\n",
    "                    wet_starts = np.insert(wet_starts, 0, 0)\n",
    "                if is_wet[-1]:\n",
    "                    wet_ends = np.append(wet_ends, is_wet.size)\n",
    "\n",
    "                if wet_starts.size > 0:  \n",
    "                    wet_lengths = wet_ends - wet_starts\n",
    "                    MeLWS[year, i, j] = np.mean(wet_lengths)\n",
    "                    MxLWS[year, i, j] = np.max(wet_lengths)\n",
    "                else:\n",
    "                    MeLWS[year, i, j] = 0\n",
    "                    MxLWS[year, i, j] = 0\n",
    "\n",
    "\n",
    "    MeLWS[:, mask] = np.nan\n",
    "    MxLWS[:, mask] = np.nan\n",
    "\n",
    "    ds_MeLWS_MxLWS = xr.Dataset(\n",
    "        {\n",
    "            'MeLWS': (['year', 'lat', 'lon'], MeLWS),\n",
    "            'MxLWS': (['year', 'lat', 'lon'], MxLWS)\n",
    "        },\n",
    "        coords={\n",
    "            'year': (['year'], np.arange(1980, 2023)),\n",
    "            'lat': (['lat'], latt),\n",
    "            'lon': (['lon'], lonn)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return ds_MeLWS_MxLWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c1a6941-6d53-4d94-b84e-b3214b7ee5bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def SPIIHhr(prec, latt, lonn, H):\n",
    "    arr = prec.reshape(43,2208, prec.shape[1], prec.shape[2])\n",
    "    spi_ihhr = np.full((43, prec.shape[1], prec.shape[2]), np.nan)\n",
    "\n",
    "    for year in range(43):\n",
    "        for i in range(prec.shape[1]):\n",
    "            for j in range(prec.shape[2]):\n",
    "                total_precip = []\n",
    "\n",
    "                for hour in range(0, 2208, H):\n",
    "                    if hour + H <= 2208:\n",
    "                        precip_sum = np.sum(arr[year, hour:hour + H, i, j])\n",
    "                    else:\n",
    "                        precip_sum = np.sum(arr[year, hour:, i, j])\n",
    "\n",
    "                    if precip_sum >= 0.1:\n",
    "                        total_precip.append(precip_sum)\n",
    "\n",
    "                if total_precip:\n",
    "                    spi_ihhr[year, i, j] = np.mean(total_precip)\n",
    "\n",
    "    ds_SPIIHhr = xr.Dataset(\n",
    "        {'p': (['year', 'lat', 'lon'], spi_ihhr)},\n",
    "        coords={\n",
    "            'year': (['year'], np.arange(1980, 2023)),\n",
    "            'lat': (['lat'], latt),\n",
    "            'lon': (['lon'], lonn)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return ds_SPIIHhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4938469d-0fd1-46dd-8f04-e6c851ed9cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RTot(prec,latt,lonn):\n",
    "    arr = prec.reshape(43, 2208, prec.shape[1], prec.shape[2])\n",
    "    mask = np.isnan(np.nanmean(prec, axis=0))\n",
    "    new_arr = np.where(arr > 0.1, arr, np.nan)\n",
    "    RTot = np.nansum(new_arr,axis=1)\n",
    "    RTot[:, mask] = np.nan\n",
    "    ds_RTot = xr.Dataset(\n",
    "        {'p': (['year', 'lat', 'lon'], RTot)},\n",
    "        coords={\n",
    "            'year': (['year'], np.arange(1980, 2023)),\n",
    "            'lat': (['lat'], latt),\n",
    "            'lon': (['lon'], lonn)\n",
    "        }\n",
    "    )\n",
    "    return ds_RTot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565ae00-ba7c-43d3-b55f-dffd23fcabd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bac290e5-2997-4071-971f-819d5a68afaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-26 11:32:51\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#####\u001b[39;00m\n\u001b[1;32m     16\u001b[0m summer_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m all_files \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-06-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-07-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-08-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-09-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[0;32m---> 17\u001b[0m ds_summer \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummer_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m ds_jja \u001b[38;5;241m=\u001b[39m ds_summer\u001b[38;5;241m.\u001b[39msel(time\u001b[38;5;241m=\u001b[39mds_summer[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mmonth\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m]))\n\u001b[1;32m     19\u001b[0m lonn \u001b[38;5;241m=\u001b[39m ds_jja\u001b[38;5;241m.\u001b[39mlon\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/xarray/backends/api.py:1077\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     open_ \u001b[38;5;241m=\u001b[39m open_dataset\n\u001b[1;32m   1075\u001b[0m     getattr_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m\n\u001b[0;32m-> 1077\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mopen_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1078\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/xarray/backends/api.py:1077\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     open_ \u001b[38;5;241m=\u001b[39m open_dataset\n\u001b[1;32m   1075\u001b[0m     getattr_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m\n\u001b[0;32m-> 1077\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [\u001b[43mopen_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[1;32m   1078\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/xarray/backends/api.py:569\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(backend_kwargs)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 569\u001b[0m     engine \u001b[38;5;241m=\u001b[39m \u001b[43mplugins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguess_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_array_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     from_array_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/xarray/backends/plugins.py:154\u001b[0m, in \u001b[0;36mguess_engine\u001b[0;34m(store_spec)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m engine, backend \u001b[38;5;129;01min\u001b[39;00m engines\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguess_can_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_spec\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    155\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m engine\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:613\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.guess_can_open\u001b[0;34m(self, filename_or_obj)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename_or_obj, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_remote_uri(filename_or_obj):\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 613\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mtry_read_magic_number_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;66;03m# netcdf 3 or HDF5\u001b[39;00m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m magic_number\u001b[38;5;241m.\u001b[39mstartswith((\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCDF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\211\u001b[39;00m\u001b[38;5;124mHDF\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\032\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/xarray/core/utils.py:661\u001b[0m, in \u001b[0;36mtry_read_magic_number_from_path\u001b[0;34m(pathlike, count)\u001b[0m\n\u001b[1;32m    659\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(pathlike)\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 661\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    662\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m read_magic_number_from_file(f, count)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mFileNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_path = '/N/project/Zli_lab/gongg/CONUS404_data/LST/UTC/'\n",
    "file_pattern = 'PREC_ACC_NC.wrf2d_d01_????-??-??.nc'\n",
    "output_folder = '/N/project/Zli_lab/gongg/CONUS404_data/LST/test/'\n",
    "folder_names = [\n",
    "  \n",
    "    'U-50',\n",
    "   #  'U-81', 'U-82', 'U-83', 'U-84', 'U-85', 'U-86', 'U-87', 'U-88'\n",
    "]\n",
    "\n",
    "\n",
    "for folder in folder_names:\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    full_path = os.path.join(base_path, folder, file_pattern)\n",
    "    all_files = glob.glob(full_path)\n",
    "    #####\n",
    "    summer_files = [f for f in all_files if '-06-' in f or '-07-' in f or '-08-' in f or '-09-' in f]\n",
    "    ds_summer = xr.open_mfdataset(summer_files)\n",
    "    ds_jja = ds_summer.sel(time=ds_summer['time'].dt.month.isin([6, 7, 8]))\n",
    "    lonn = ds_jja.lon.values\n",
    "    latt = ds_jja.lat.values\n",
    "    prec = ds_jja.p.values\n",
    "    \n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    ds_RxHhr_1 = RxHhr(prec, latt, lonn, 1)\n",
    "    ds_RxHhr_3 = RxHhr(prec, latt, lonn, 3)\n",
    "    ds_RxHhr_6 = RxHhr(prec, latt, lonn, 6)\n",
    "    ds_RxHhr_12 = RxHhr(prec, latt, lonn, 12)\n",
    "    ds_RxHhr_24 = RxHhr(prec, latt, lonn, 24)\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    ds_Rx1hrP = Rx1hrP(prec, latt, lonn)\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    ds_percent, ds_quantile = RQpwHhrP(prec, latt, lonn)\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    ds_RHhrTmm_1 = RHhrTmm(prec, latt, lonn, 1)\n",
    "    ds_RHhrTmm_3 = RHhrTmm(prec, latt, lonn, 3)\n",
    "    ds_RHhrTmm_6 = RHhrTmm(prec, latt, lonn, 6)\n",
    "    ds_RHhrTmm_12 = RHhrTmm(prec, latt, lonn, 12)\n",
    "    ds_RHhrTmm_24 = RHhrTmm(prec, latt, lonn, 24)\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    ds_NWH = NWH(prec, latt, lonn)\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    ds_MeLWS_MxLWS = MeLWS_MxLWS(prec,latt,lonn)\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    ds_SPIIHhr_1 = SPIIHhr(prec, latt, lonn, 1)\n",
    "    ds_SPIIHhr_3 = SPIIHhr(prec, latt, lonn, 3)\n",
    "    ds_SPIIHhr_6 = SPIIHhr(prec, latt, lonn, 6)\n",
    "    ds_SPIIHhr_12 = SPIIHhr(prec, latt, lonn, 12)\n",
    "    ds_SPIIHhr_24 = SPIIHhr(prec, latt, lonn, 24)\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    ds_RTot = RTot(prec,latt,lonn)\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    \n",
    "    ################################################################################################\n",
    "    \n",
    "    \n",
    "    ds_RxHhr_1.to_netcdf(output_folder+'ds_RxHhr_1_'+folder+'.nc')\n",
    "    ds_RxHhr_3.to_netcdf(output_folder+'ds_RxHhr_3_'+folder+'.nc')\n",
    "    ds_RxHhr_6.to_netcdf(output_folder+'ds_RxHhr_6_'+folder+'.nc')\n",
    "    ds_RxHhr_12.to_netcdf(output_folder+'ds_RxHhr_12_'+folder+'.nc')\n",
    "    ds_RxHhr_24.to_netcdf(output_folder+'ds_RxHhr_24_'+folder+'.nc')\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    ds_Rx1hrP.to_netcdf(output_folder+'ds_Rx1hrP_'+folder+'.nc')\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    ds_percent.to_netcdf(output_folder+'ds_percent_'+folder+'.nc')\n",
    "    ds_quantile.to_netcdf(output_folder+'ds_quantile_'+folder+'.nc')\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    ds_RHhrTmm_1.to_netcdf(output_folder+'ds_RHhrTmm_1_'+folder+'.nc')\n",
    "    ds_RHhrTmm_3.to_netcdf(output_folder+'ds_RHhrTmm_3_'+folder+'.nc')\n",
    "    ds_RHhrTmm_6.to_netcdf(output_folder+'ds_RHhrTmm_6_'+folder+'.nc')\n",
    "    ds_RHhrTmm_12.to_netcdf(output_folder+'ds_RHhrTmm_12_'+folder+'.nc')\n",
    "    ds_RHhrTmm_24.to_netcdf(output_folder+'ds_RHhrTmm_24_'+folder+'.nc')\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    ds_NWH.to_netcdf(output_folder+'ds_NWH_'+folder+'.nc')\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    ds_MeLWS_MxLWS.to_netcdf(output_folder+'ds_MeLWS_MxLWS_'+folder+'.nc')\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    ds_SPIIHhr_1.to_netcdf(output_folder+'ds_SPIIHhr_1_'+folder+'.nc')\n",
    "    ds_SPIIHhr_3.to_netcdf(output_folder+'ds_SPIIHhr_3_'+folder+'.nc')\n",
    "    ds_SPIIHhr_6.to_netcdf(output_folder+'ds_SPIIHhr_6_'+folder+'.nc')\n",
    "    ds_SPIIHhr_12.to_netcdf(output_folder+'ds_SPIIHhr_12_'+folder+'.nc')\n",
    "    ds_SPIIHhr_24.to_netcdf(output_folder+'ds_SPIIHhr_24_'+folder+'.nc')\n",
    "    print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "    ds_RTot.to_netcdf(output_folder+'ds_RTot_'+folder+'.nc')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a63c32e-ed61-404b-b626-87bf498113f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
