{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb1ae3c-8068-4ebf-9cba-834a2e7fe8a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "### automatically refresh the buffer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "### solve the auto-complete issue\n",
    "\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "### lvl 2 setups (systerm)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import matplotlib as mpl\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pylab import *\n",
    "from matplotlib.colors import ListedColormap,LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.patches import Wedge, Circle\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import datetime\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95816f8b-f871-48e6-ae44-0d3c248bd965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_ds(ds, array):\n",
    "    lonn = ds.lon.values\n",
    "    latt = ds.lat.values\n",
    "\n",
    "    ds_ = xr.Dataset({'s': ([ 'lat', 'lon'], array)},\n",
    "                    coords={'lon': (['lon'], lonn),\n",
    "                            'lat': (['lat'], latt),})\n",
    "    return ds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba49c883-3174-4ea2-86fe-e20e5b606df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def calculate_ccscale_slope(temperature_data, precipitation_data):\n",
    "    # Ensure the data is flat (1D)\n",
    "    temperature_data = np.ravel(temperature_data)\n",
    "    precipitation_data = np.ravel(precipitation_data)\n",
    "\n",
    "    # Early exit if input arrays are empty\n",
    "    if temperature_data.size == 0 or precipitation_data.size == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    # Remove data points where either temperature or precipitation contains NaN\n",
    "    valid_mask = ~np.isnan(temperature_data) & ~np.isnan(precipitation_data)\n",
    "    temperature_data = temperature_data[valid_mask]\n",
    "    precipitation_data = precipitation_data[valid_mask]\n",
    "    \n",
    "    if temperature_data.size == 0 or precipitation_data.size == 0:\n",
    "        return np.nan, np.nan, np.nan, np.nan\n",
    "    \n",
    "    # Convert Kelvin temperatures to Celsius\n",
    "    temperature_data_celsius = temperature_data - 273.15\n",
    "\n",
    "    # Set the temperature bin size and sliding step\n",
    "    bin_size = 1.0  # Bin size\n",
    "    step = 0.5      # Sliding step\n",
    "    min_temp = temperature_data_celsius.min()\n",
    "    max_temp = temperature_data_celsius.max()\n",
    "    \n",
    "    \n",
    "    # Create sliding temperature bins\n",
    "    temperature_bins = np.arange(min_temp, max_temp, step)\n",
    "    overlapping_bins = [(start, start + bin_size) for start in temperature_bins]\n",
    "\n",
    "    # Collect precipitation data for each temperature bin using a dictionary\n",
    "    precipitation_per_bin = {i: [] for i in range(len(overlapping_bins))}\n",
    "    for temp, precip in zip(temperature_data_celsius, precipitation_data):\n",
    "        for i, (bin_start, bin_end) in enumerate(overlapping_bins):\n",
    "            if bin_start <= temp < bin_end:\n",
    "                precipitation_per_bin[i].append(precip)\n",
    "\n",
    "    # Calculate the 99th percentile of log precipitation, mean temperature, and confidence intervals\n",
    "    log_precipitation_99 = []\n",
    "    mean_temperatures = []\n",
    "    for idx, (bin_start, bin_end) in enumerate(overlapping_bins):\n",
    "        bin_data = precipitation_per_bin[idx]\n",
    "        if len(bin_data) >= 80:\n",
    "            log_precip = np.log(bin_data)\n",
    "            quantile_99 = np.percentile(log_precip, 99)\n",
    "            mean_temp = (bin_start + bin_end) / 2\n",
    "            log_precipitation_99.append(quantile_99)\n",
    "            mean_temperatures.append(mean_temp)\n",
    "\n",
    "    # Perform linear regression if we have at least 5 bins with sufficient data\n",
    "    if len(log_precipitation_99) >= 5:\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(mean_temperatures, log_precipitation_99)\n",
    "    else:\n",
    "        slope = np.nan\n",
    "        intercept = np.nan\n",
    "\n",
    "    return mean_temperatures, log_precipitation_99, slope, intercept\n",
    "\n",
    "# Example usage:\n",
    "# temperature_data = np.array([...])\n",
    "# precipitation_data = np.array([...])\n",
    "# results = calculate_ccscale_slope(temperature_data, precipitation_data)\n",
    "# print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c5eca95-d0ed-4ddb-ba0f-1889585a5123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def calculate_ccscale_quantreg(temperature_data, precipitation_data):\n",
    "    # 确保数据是平的（一维的）\n",
    "    temperature_data = np.ravel(temperature_data)\n",
    "    precipitation_data = np.ravel(precipitation_data)\n",
    "\n",
    "    # 移除温度或降水量中包含NaN的数据点\n",
    "    valid_mask = ~np.isnan(temperature_data) & ~np.isnan(precipitation_data)\n",
    "    temperature_data = temperature_data[valid_mask]\n",
    "    precipitation_data = precipitation_data[valid_mask]\n",
    "    # Early exit if input arrays are empty\n",
    "    if temperature_data.size == 0 or precipitation_data.size == 0:\n",
    "        return np.nan\n",
    "\n",
    "    # 将开尔文温度转换为摄氏度\n",
    "    temperature_data_celsius = temperature_data - 273.15\n",
    "\n",
    "    # 创建一个 DataFrame 来包含温度和降水量的数据\n",
    "    data = pd.DataFrame({\n",
    "        'Temperature': temperature_data_celsius,\n",
    "        'Precipitation': precipitation_data\n",
    "    })\n",
    "\n",
    "    # 定义模型：在 0.99 分位数处进行分位数回归\n",
    "    quantile_model = sm.QuantReg(np.log(data['Precipitation']), sm.add_constant(data['Temperature']))\n",
    "    quantile_regression_result = quantile_model.fit(q=0.99).params[1]\n",
    "\n",
    "    return quantile_regression_result\n",
    "\n",
    "# 使用实际的 temperature_data 和 precipitation_data 调用该函数\n",
    "# 例如：\n",
    "# result = calculate_ccscale_quantreg(temperature_data, precipitation_data)\n",
    "# print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f10722c-6d58-4e11-bdad-84aadb2e6deb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_folder_t = '/N/project/Zli_lab/gongg/CONUS404_data/LST/JJA/'\n",
    "base_path = '/N/project/Zli_lab/gongg/CONUS404_data/LST/UTC/'\n",
    "file_pattern_p = 'PREC_ACC_NC.wrf2d_d01_????-??-??.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e8412-f8e2-4e2a-bbc6-3fbb4f36bcfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_names = [\n",
    "    # 'U-50', 'U-51', 'U-52', 'U-53', 'U-54', 'U-55', 'U-56', 'U-57', 'U-58',\n",
    "    # 'U-60', 'U-61', 'U-62', 'U-63', 'U-64', 'U-65', 'U-66', 'U-67', 'U-68',\n",
    "    # 'U-70', 'U-71', 'U-72', 'U-73', 'U-74', 'U-75', 'U-76', 'U-77', 'U-78',\n",
    "     'U-80', 'U-81', 'U-82', 'U-83', 'U-84', 'U-85', 'U-86', 'U-87', 'U-88',\n",
    "]\n",
    "for folder in folder_names:\n",
    "    full_path_p = os.path.join(base_path, folder, file_pattern_p)\n",
    "    all_files_p = glob.glob(full_path_p)\n",
    "    #####\n",
    "    summer_files_p = [f for f in all_files_p if '-06-' in f or '-07-' in f or '-08-' in f or '-09-' in f]\n",
    "    ds_p = xr.open_mfdataset(summer_files_p)\n",
    "    ds_p = ds_p.sel(time=ds_p['time'].dt.month.isin([6, 7, 8]))\n",
    "    ds_t = xr.open_mfdataset(input_folder_t+'dn_temp_'+folder+'.nc')\n",
    "    ds_p_filtered = ds_p.where(ds_p['p'] > 0.1, np.nan)\n",
    "    \n",
    "    # 保留特定时间范围内的数据，其他时间标记为nan\n",
    "    ds_p_daytime = ds_p_filtered.where((ds_p_filtered['time.hour'] >= 6) & (ds_p_filtered['time.hour'] < 18), np.nan)\n",
    "    # 保留18点到次日早上6点的数据，其他时间标记为nan\n",
    "    ds_p_nighttime = ds_p_filtered.where((ds_p_filtered['time.hour'] >= 18) | (ds_p_filtered['time.hour'] < 6), np.nan)\n",
    "    \n",
    "    \n",
    "    arr_dtp = ds_p_daytime.p.values\n",
    "    arr_ntp = ds_p_nighttime.p.values\n",
    "    arr_t = ds_t.dnt.values\n",
    "    \n",
    "    arr_dt = np.where(np.isnan(arr_dtp), np.nan, arr_t)\n",
    "    arr_nt = np.where(np.isnan(arr_ntp), np.nan, arr_t)\n",
    "    \n",
    "    arr_slope_nt = np.full((arr_nt.shape[1], arr_nt.shape[2]), np.nan)\n",
    "    arr_slope_dt = np.full((arr_nt.shape[1], arr_nt.shape[2]), np.nan)\n",
    "    # 循环遍历每个网格点\n",
    "    \n",
    "    for i in range(arr_nt.shape[1]):\n",
    "        for j in range(arr_nt.shape[2]):\n",
    "            temperature_data_nt = arr_nt[:, i, j]\n",
    "            precipitation_data_nt = arr_ntp[:, i, j]\n",
    "            \n",
    "            temperature_data_dt = arr_dt[:, i, j]\n",
    "            precipitation_data_dt = arr_dtp[:, i, j]\n",
    "            # 调用函数并获取斜率\n",
    "            slope_nt = calculate_ccscale_slope(temperature_data_nt, precipitation_data_nt)[2]\n",
    "            slope_dt = calculate_ccscale_slope(temperature_data_dt, precipitation_data_dt)[2]\n",
    "            # 将斜率值存储到arr_slope的对应位置\n",
    "            arr_slope_nt[i, j] = slope_nt\n",
    "            arr_slope_dt[i, j] = slope_dt\n",
    "            ds_lrs_nt = create_ds(ds_t, arr_slope_nt)\n",
    "            ds_lrs_dt = create_ds(ds_t, arr_slope_dt)\n",
    "    ds_lrs_nt.to_netcdf(input_folder_t+'ds_lrs_nt'+folder+'.nc')\n",
    "    ds_lrs_dt.to_netcdf(input_folder_t+'ds_lrs_dt'+folder+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122e8bd-46da-4923-ad44-40f33d72c3f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "folder_names = [\n",
    "    # 'U-50', 'U-51', 'U-52', 'U-53', 'U-54', 'U-55', 'U-56', 'U-57', 'U-58',\n",
    "    # 'U-60', 'U-61', 'U-62', 'U-63', 'U-64', 'U-65', 'U-66', 'U-67', 'U-68',\n",
    "    # 'U-70', 'U-71', 'U-72', 'U-73', 'U-74', 'U-75', 'U-76', 'U-77', 'U-78',\n",
    "     'U-80', 'U-81', 'U-82', 'U-83', 'U-84', 'U-85', 'U-86', 'U-87', 'U-88',\n",
    "]\n",
    "for folder in folder_names:\n",
    "    full_path_p = os.path.join(base_path, folder, file_pattern_p)\n",
    "    all_files_p = glob.glob(full_path_p)\n",
    "    #####\n",
    "    summer_files_p = [f for f in all_files_p if '-06-' in f or '-07-' in f or '-08-' in f or '-09-' in f]\n",
    "    ds_p = xr.open_mfdataset(summer_files_p)\n",
    "    ds_p = ds_p.sel(time=ds_p['time'].dt.month.isin([6, 7, 8]))\n",
    "    ds_t = xr.open_mfdataset(input_folder_t+'dn_temp_'+folder+'.nc')\n",
    "    ds_p_filtered = ds_p.where(ds_p['p'] > 0.1, np.nan)\n",
    "    \n",
    "    # 保留特定时间范围内的数据，其他时间标记为nan\n",
    "    ds_p_daytime = ds_p_filtered.where((ds_p_filtered['time.hour'] >= 6) & (ds_p_filtered['time.hour'] < 18), np.nan)\n",
    "    # 保留18点到次日早上6点的数据，其他时间标记为nan\n",
    "    ds_p_nighttime = ds_p_filtered.where((ds_p_filtered['time.hour'] >= 18) | (ds_p_filtered['time.hour'] < 6), np.nan)\n",
    "    \n",
    "    \n",
    "    arr_dtp = ds_p_daytime.p.values\n",
    "    arr_ntp = ds_p_nighttime.p.values\n",
    "    arr_t = ds_t.dnt.values\n",
    "    \n",
    "    arr_dt = np.where(np.isnan(arr_dtp), np.nan, arr_t)\n",
    "    arr_nt = np.where(np.isnan(arr_ntp), np.nan, arr_t)\n",
    "    \n",
    "    arr_slope_nt = np.full((arr_nt.shape[1], arr_nt.shape[2]), np.nan)\n",
    "    arr_slope_dt = np.full((arr_nt.shape[1], arr_nt.shape[2]), np.nan)\n",
    "    # 循环遍历每个网格点\n",
    "    \n",
    "    for i in range(arr_nt.shape[1]):\n",
    "        for j in range(arr_nt.shape[2]):\n",
    "            temperature_data_nt = arr_nt[:, i, j]\n",
    "            precipitation_data_nt = arr_ntp[:, i, j]\n",
    "            \n",
    "            temperature_data_dt = arr_dt[:, i, j]\n",
    "            precipitation_data_dt = arr_dtp[:, i, j]\n",
    "            # 调用函数并获取斜率\n",
    "    \n",
    "            slope_nt = calculate_ccscale_quantreg(temperature_data_nt, precipitation_data_nt)\n",
    "            slope_dt = calculate_ccscale_quantreg(temperature_data_dt, precipitation_data_dt)\n",
    "            # 将斜率值存储到arr_slope的对应位置\n",
    "            arr_slope_nt[i, j] = slope_nt\n",
    "            arr_slope_dt[i, j] = slope_dt\n",
    "            ds_qrs_nt = create_ds(ds_t, arr_slope_nt)\n",
    "            ds_qrs_dt = create_ds(ds_t, arr_slope_dt)\n",
    "    ds_qrs_nt.to_netcdf(input_folder_t+'ds_qrs_nt'+folder+'.nc')\n",
    "    ds_qrs_dt.to_netcdf(input_folder_t+'ds_qrs_dt'+folder+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e9bbe-1689-4f43-be1a-1054080e1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder_t = '/N/project/Zli_lab/gongg/CONUS404_data/LST/JJA/'\n",
    "base_path = '/N/project/Zli_lab/gongg/CONUS404_data/LST/UTC/'\n",
    "file_pattern_p = 'PREC_ACC_NC.wrf2d_d01_????-??-??.nc'\n",
    "\n",
    "\n",
    "folder_names = [\n",
    "    # 'U-50', 'U-51', 'U-52', 'U-53', 'U-54', 'U-55', 'U-56', 'U-57', 'U-58',\n",
    "    # 'U-60', 'U-61', 'U-62', 'U-63', 'U-64', 'U-65', 'U-66', 'U-67', 'U-68',\n",
    "    # 'U-70', 'U-71', 'U-72', 'U-73', 'U-74', 'U-75', 'U-76', 'U-77', 'U-78',\n",
    "      'U-80', 'U-81', 'U-82', 'U-83', 'U-84', 'U-85', 'U-86', 'U-87','U-88',\n",
    "]\n",
    "for folder in folder_names:\n",
    "    full_path_p = os.path.join(base_path, folder, file_pattern_p)\n",
    "    all_files_p = glob.glob(full_path_p)\n",
    "    #####\n",
    "    summer_files_p = [f for f in all_files_p if '-06-' in f or '-07-' in f or '-08-' in f or '-09-' in f]\n",
    "    ds_p = xr.open_mfdataset(summer_files_p)\n",
    "    ds_p = ds_p.sel(time=ds_p['time'].dt.month.isin([6, 7, 8]))\n",
    "    ds_t = xr.open_mfdataset(input_folder_t+'dn_dewtemp_'+folder+'.nc')\n",
    "    ds_p_filtered = ds_p.where(ds_p['p'] > 0.1, np.nan)\n",
    "    \n",
    "    # 保留特定时间范围内的数据，其他时间标记为nan\n",
    "    ds_p_daytime = ds_p_filtered.where((ds_p_filtered['time.hour'] >= 6) & (ds_p_filtered['time.hour'] < 18), np.nan)\n",
    "    # 保留18点到次日早上6点的数据，其他时间标记为nan\n",
    "    ds_p_nighttime = ds_p_filtered.where((ds_p_filtered['time.hour'] >= 18) | (ds_p_filtered['time.hour'] < 6), np.nan)\n",
    "    \n",
    "    \n",
    "    arr_dtp = ds_p_daytime.p.values\n",
    "    arr_ntp = ds_p_nighttime.p.values\n",
    "    arr_t = ds_t.dnt.values\n",
    "    \n",
    "    arr_dt = np.where(np.isnan(arr_dtp), np.nan, arr_t)\n",
    "    arr_nt = np.where(np.isnan(arr_ntp), np.nan, arr_t)\n",
    "    \n",
    "    arr_slope_nt = np.full((arr_nt.shape[1], arr_nt.shape[2]), np.nan)\n",
    "    arr_slope_dt = np.full((arr_nt.shape[1], arr_nt.shape[2]), np.nan)\n",
    "    # 循环遍历每个网格点\n",
    "    \n",
    "    for i in range(arr_nt.shape[1]):\n",
    "        for j in range(arr_nt.shape[2]):\n",
    "            temperature_data_nt = arr_nt[:, i, j]\n",
    "            precipitation_data_nt = arr_ntp[:, i, j]\n",
    "            \n",
    "            temperature_data_dt = arr_dt[:, i, j]\n",
    "            precipitation_data_dt = arr_dtp[:, i, j]\n",
    "            # 调用函数并获取斜率\n",
    "            slope_nt = calculate_ccscale_slope(temperature_data_nt, precipitation_data_nt)[2]\n",
    "            slope_dt = calculate_ccscale_slope(temperature_data_dt, precipitation_data_dt)[2]\n",
    "            # 将斜率值存储到arr_slope的对应位置\n",
    "            arr_slope_nt[i, j] = slope_nt\n",
    "            arr_slope_dt[i, j] = slope_dt\n",
    "            ds_lrs_nt = create_ds(ds_t, arr_slope_nt)\n",
    "            ds_lrs_dt = create_ds(ds_t, arr_slope_dt)\n",
    "    ds_lrs_nt.to_netcdf(input_folder_t+'ds_dewlrs_nt'+folder+'.nc')\n",
    "    ds_lrs_dt.to_netcdf(input_folder_t+'ds_dewlrs_dt'+folder+'.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f7a90c7-8d53-4690-b662-744a71cc9647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:21:21.156065\n",
      "09:00:10.494204\n",
      "15:31:09.091420\n",
      "21:15:43.563238\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# 导出每日数据为 NetCDF 文件\u001b[39;00m\n\u001b[1;32m     67\u001b[0m ds_day_p\u001b[38;5;241m.\u001b[39mto_netcdf(output\u001b[38;5;241m+\u001b[39mfilename_p)\n\u001b[0;32m---> 68\u001b[0m \u001b[43mds_day_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfilename_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m ds_day_dt\u001b[38;5;241m.\u001b[39mto_netcdf(output\u001b[38;5;241m+\u001b[39mfilename_dt)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/xarray/core/dataset.py:2329\u001b[0m, in \u001b[0;36mDataset.to_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf)\u001b[0m\n\u001b[1;32m   2326\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2327\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_netcdf\n\u001b[0;32m-> 2329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2337\u001b[0m \u001b[43m    \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultifile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2340\u001b[0m \u001b[43m    \u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/xarray/backends/api.py:1369\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multifile:\n\u001b[1;32m   1367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m writer, store\n\u001b[0;32m-> 1369\u001b[0m writes \u001b[38;5;241m=\u001b[39m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target, BytesIO):\n\u001b[1;32m   1372\u001b[0m     store\u001b[38;5;241m.\u001b[39msync()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/xarray/backends/common.py:267\u001b[0m, in \u001b[0;36mArrayWriter.sync\u001b[0;34m(self, compute, chunkmanager_store_kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunkmanager_store_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     chunkmanager_store_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 267\u001b[0m delayed_store \u001b[38;5;241m=\u001b[39m \u001b[43mchunkmanager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflush\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mchunkmanager_store_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msources \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/xarray/namedarray/daskmanager.py:249\u001b[0m, in \u001b[0;36mDaskManager.store\u001b[0;34m(self, sources, targets, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstore\u001b[39m(\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    243\u001b[0m     sources: Any \u001b[38;5;241m|\u001b[39m Sequence[Any],\n\u001b[1;32m    244\u001b[0m     targets: Any,\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    246\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m store\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43msources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/N/soft/rhel8/python/gnu/3.11.4/lib/python3.11/site-packages/dask/array/core.py:1235\u001b[0m, in \u001b[0;36mstore\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute:\n\u001b[1;32m   1234\u001b[0m     store_dsk \u001b[38;5;241m=\u001b[39m HighLevelGraph(layers, dependencies)\n\u001b[0;32m-> 1235\u001b[0m     \u001b[43mcompute_as_if_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mArray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_dsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/N/soft/rhel8/python/gnu/3.11.4/lib/python3.11/site-packages/dask/base.py:408\u001b[0m, in \u001b[0;36mcompute_as_if_collection\u001b[0;34m(cls, dsk, keys, scheduler, get, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m schedule \u001b[38;5;241m=\u001b[39m get_scheduler(scheduler\u001b[38;5;241m=\u001b[39mscheduler, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, get\u001b[38;5;241m=\u001b[39mget)\n\u001b[1;32m    407\u001b[0m dsk2 \u001b[38;5;241m=\u001b[39m optimization_function(\u001b[38;5;28mcls\u001b[39m)(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 408\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/N/soft/rhel8/python/gnu/3.11.4/lib/python3.11/site-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/N/soft/rhel8/python/gnu/3.11.4/lib/python3.11/site-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaiting\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mready\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, res_info, failed \u001b[38;5;129;01min\u001b[39;00m \u001b[43mqueue_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[38;5;241m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m/N/soft/rhel8/python/gnu/3.11.4/lib/python3.11/site-packages/dask/local.py:137\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/N/soft/rhel8/python/gnu/3.11.4/lib/python3.11/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/N/soft/rhel8/python/gnu/3.11.4/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_folder_t = '/N/project/Zli_lab/gongg/CONUS404_data/LST/JJA/'\n",
    "base_path = '/N/project/Zli_lab/gongg/CONUS404_data/LST/UTC/'\n",
    "file_pattern_p = 'PREC_ACC_NC.wrf2d_d01_????-??-??.nc'\n",
    "output = '/N/project/Zli_lab/gongg/CONUS404_data/LST/JJA_dailydata/'\n",
    "\n",
    "\n",
    "folder_names = [\n",
    "     # 'U-50', 'U-51', 'U-52', 'U-53', 'U-54', 'U-55', 'U-56', 'U-57', 'U-58',\n",
    "     # 'U-60', 'U-61', 'U-62', 'U-63', 'U-64', 'U-65', 'U-66', 'U-67', 'U-68',\n",
    "     # 'U-70', 'U-71', 'U-72', 'U-73', 'U-74', 'U-75', 'U-76', 'U-77', 'U-78',\n",
    "      'U-80', 'U-81', 'U-82', 'U-83', 'U-84', 'U-85', 'U-86', 'U-87', 'U-88',\n",
    "]\n",
    "for folder in folder_names:\n",
    "    print(datetime.datetime.now().time())\n",
    "    full_path_p = os.path.join(base_path, folder, file_pattern_p)\n",
    "    all_files_p = glob.glob(full_path_p)\n",
    "    #####\n",
    "    summer_files_p = [f for f in all_files_p if '-06-' in f or '-07-' in f or '-08-' in f or '-09-' in f]\n",
    "    ds_p = xr.open_mfdataset(summer_files_p)\n",
    "    ds_p = ds_p.sel(time=ds_p['time'].dt.month.isin([6, 7, 8]))\n",
    "    ds_t = xr.open_mfdataset(input_folder_t+'dn_temp_'+folder+'.nc')\n",
    "    ds_dt = xr.open_mfdataset(input_folder_t+'dn_dewtemp_'+folder+'.nc')\n",
    "    \n",
    "    \n",
    "    grouped_p = ds_p.groupby('time.year').groups\n",
    "    grouped_t = ds_t.groupby('time.year').groups\n",
    "    grouped_dt = ds_dt.groupby('time.year').groups\n",
    "\n",
    "    for year, year_indices_p in grouped_p.items():\n",
    "        year_indices_t = grouped_t[year]\n",
    "        year_indices_dt = grouped_dt[year]\n",
    "\n",
    "        ds_year_p = ds_p.isel(time=year_indices_p)\n",
    "        ds_year_t = ds_t.isel(time=year_indices_t)\n",
    "        ds_year_dt = ds_dt.isel(time=year_indices_dt)\n",
    "\n",
    "        monthly_groups_p = ds_year_p.groupby('time.month').groups\n",
    "        monthly_groups_t = ds_year_t.groupby('time.month').groups\n",
    "        monthly_groups_dt = ds_year_dt.groupby('time.month').groups\n",
    "\n",
    "        for month, month_indices_p in monthly_groups_p.items():\n",
    "            month_indices_t = monthly_groups_t[month]\n",
    "            month_indices_dt = monthly_groups_dt[month]\n",
    "\n",
    "            ds_month_p = ds_year_p.isel(time=month_indices_p)\n",
    "            ds_month_t = ds_year_t.isel(time=month_indices_t)\n",
    "            ds_month_dt = ds_year_dt.isel(time=month_indices_dt)\n",
    "\n",
    "            daily_groups_p = ds_month_p.groupby('time.day').groups\n",
    "            daily_groups_t = ds_month_t.groupby('time.day').groups\n",
    "            daily_groups_dt = ds_month_dt.groupby('time.day').groups\n",
    "\n",
    "            # 循环遍历每日的数据\n",
    "            for day, day_indices_p in daily_groups_p.items():\n",
    "                day_indices_t = daily_groups_t[day]\n",
    "                day_indices_dt = daily_groups_dt[day]\n",
    "\n",
    "                ds_day_p = ds_month_p.isel(time=day_indices_p)\n",
    "                ds_day_t = ds_month_t.isel(time=day_indices_t)\n",
    "                ds_day_dt = ds_month_dt.isel(time=day_indices_dt)\n",
    "\n",
    "                filename_p = f'{folder}_prec_{year}_{month:02d}_{day:02d}.nc'\n",
    "                filename_t = f'{folder}_dnt_{year}_{month:02d}_{day:02d}.nc'\n",
    "                filename_dt = f'{folder}_dndt_{year}_{month:02d}_{day:02d}.nc'\n",
    "\n",
    "                # 导出每日数据为 NetCDF 文件\n",
    "                ds_day_p.to_netcdf(output+filename_p)\n",
    "                ds_day_t.to_netcdf(output+filename_t)\n",
    "                ds_day_dt.to_netcdf(output+filename_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474318e-7ca6-49ee-b7d3-465e20ee02b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:20:04.466132\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file('/N/project/Zli_lab/Data/Other/tl_2019_us_state/tl_2019_us_state.shp')\n",
    "US = gpd.read_file('/N/project/Zli_lab/Data/Other/tl_2019_us_state/tl_2019_us_state.shp')\n",
    "base_path = '/N/project/Zli_lab/gongg/CONUS404_data/LST/JJA_dailydata'\n",
    "output1 = '/N/project/Zli_lab/gongg/CONUS404_data/LST/'\n",
    "ds_raster = xr.open_dataset('/N/project/Zli_lab/Data/Observations/NCAR/prec_acc_files/PREC_ACC_NC.wrf2d_d01_2022-09-30_23:00:00.nc')\n",
    "# 定义所有的U*前缀\n",
    "prefixes = [\n",
    "    'U-50', 'U-51', 'U-52', 'U-53', 'U-54', 'U-55', 'U-56', 'U-57', 'U-58',\n",
    "    'U-60', 'U-61', 'U-62', 'U-63', 'U-64', 'U-65', 'U-66', 'U-67', 'U-68',\n",
    "    'U-70', 'U-71', 'U-72', 'U-73', 'U-74', 'U-75', 'U-76', 'U-77', 'U-78',\n",
    "    'U-80', 'U-81', 'U-82', 'U-83', 'U-84', 'U-85', 'U-86', 'U-87', 'U-88'\n",
    "]\n",
    "\n",
    "for year in range(2011, 2023):  # 从1980年到2022年\n",
    "    for month in [6, 7, 8]:  # 只读取6, 7, 8月的数据\n",
    "        print(datetime.datetime.now().time())\n",
    "        days_in_month = 30 if month == 6 else 31  # 6月30天，7月和8月31天\n",
    "        \n",
    "        \n",
    "        for day in range(1, days_in_month + 1):\n",
    "            files_to_open = []\n",
    "            # 对每一个前缀和日期组合构造文件路径\n",
    "            for prefix in prefixes:\n",
    "                file_pattern = f'{base_path}/{prefix}_mdt_{year}_{month:02d}_{day:02d}.nc'\n",
    "                files_to_open.append(file_pattern)\n",
    "                \n",
    "            ds = xr.open_mfdataset(files_to_open)\n",
    "            lon = ds_raster['XLONG'].values\n",
    "            lat = ds_raster['XLAT'].values\n",
    "            grid = gpd.GeoDataFrame(\n",
    "                geometry=gpd.points_from_xy(lon.flatten(), lat.flatten()),\n",
    "                index=np.arange(lon.size)\n",
    "            )\n",
    "            grid.set_crs(gdf.crs, inplace=True)\n",
    "            grid_s = gpd.sjoin(grid, gdf, how='inner', predicate='within')\n",
    "\n",
    "            mask = np.full(ds_raster['PREC_ACC_NC'].shape[1:], False) \n",
    "            for index in grid_s.index:\n",
    "                row, col = np.unravel_index(index, mask.shape)  # 获取行列索引\n",
    "                mask[row, col] = True\n",
    "            mask_da = xr.DataArray(mask, dims=ds_raster['PREC_ACC_NC'].dims[1:], coords={'south_north': ds_raster['PREC_ACC_NC'].coords['south_north'], 'west_east': ds_raster['PREC_ACC_NC'].coords['west_east']})\n",
    "            ds_conus = ds_raster.where(mask_da, drop=True)\n",
    "\n",
    "            XLON = ds_conus.XLONG.values[:707,:]\n",
    "            XLAT = ds_conus.XLAT.values[:707,:]\n",
    "            ds_n = ds.assign_coords({\n",
    "                'XLON': (('lat', 'lon'), XLON),\n",
    "                'XLAT': (('lat', 'lon'), XLAT)\n",
    "            })\n",
    "\n",
    "            regions_dict = {\n",
    "                'NE': ['CT', 'DE', 'ME', 'MD', 'MA', 'NH', 'NJ', 'NY', 'PA', 'RI', 'VT', 'WV'],\n",
    "                'Midwest': ['IA', 'MI', 'MN', 'WI', 'IL', 'IN', 'MO', 'OH'],\n",
    "                'SE': ['AL', 'FL', 'GA', 'NC', 'SC', 'VA', 'TN', 'KY', 'AR', 'LA', 'MS'],\n",
    "                'NGP': ['MT', 'NE', 'ND', 'SD', 'WY'],\n",
    "                'SGP': ['KS', 'OK', 'TX'],\n",
    "                'SW': ['AZ', 'CO', 'NM', 'UT', 'CA', 'NV'],\n",
    "                'NW': ['ID', 'OR', 'WA']\n",
    "            }\n",
    "            regions = {name: US[US['STUSPS'].isin(states)] for name, states in regions_dict.items()}\n",
    "            regi = ['NE','Midwest','SE','NGP','SGP','SW','NW',]\n",
    "\n",
    "            ds_results = {}\n",
    "\n",
    "            for r in regi:\n",
    "                lon = ds_n['XLON'].values\n",
    "                lat = ds_n['XLAT'].values\n",
    "                grid = gpd.GeoDataFrame(\n",
    "                    geometry=gpd.points_from_xy(lon.flatten(), lat.flatten()),\n",
    "                    index=np.arange(lon.size)\n",
    "                )\n",
    "\n",
    "                grid.set_crs(regions[r].crs, inplace=True)\n",
    "                grid_s = gpd.sjoin(grid, regions[r], how='inner', predicate='within')\n",
    "\n",
    "                mask = np.full((ds_n['td2'].shape[1], ds_n['td2'].shape[2]), False) \n",
    "                for index in grid_s.index:\n",
    "                    row, col = np.unravel_index(index, mask.shape)\n",
    "                    mask[row, col] = True \n",
    "\n",
    "                mask_da = xr.DataArray(\n",
    "                    mask, \n",
    "                    dims=['lat', 'lon'],\n",
    "                    coords={\n",
    "                        'lat': ds_n['lat'].values,\n",
    "                        'lon': ds_n['lon'].values\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                ds_ssss = ds_n.where(mask_da, drop=True)\n",
    "                ds_results[f'ds_{r}'] = ds_ssss\n",
    "\n",
    "            ds_results['ds_NE'].drop_vars(['XLON', 'XLAT']).to_netcdf(f\"{output1}NE/mdt_{year}_{month:02d}_{day:02d}.nc\")\n",
    "            ds_results['ds_Midwest'].drop_vars(['XLON', 'XLAT']).to_netcdf(f\"{output1}Midwest/mdt_{year}_{month:02d}_{day:02d}.nc\")\n",
    "            ds_results['ds_SE'].drop_vars(['XLON', 'XLAT']).to_netcdf(f\"{output1}SE/mdt_{year}_{month:02d}_{day:02d}.nc\")\n",
    "            ds_results['ds_NGP'].drop_vars(['XLON', 'XLAT']).to_netcdf(f\"{output1}NGP/mdt_{year}_{month:02d}_{day:02d}.nc\")\n",
    "            ds_results['ds_SGP'].drop_vars(['XLON', 'XLAT']).to_netcdf(f\"{output1}SGP/mdt_{year}_{month:02d}_{day:02d}.nc\")\n",
    "            ds_results['ds_SW'].drop_vars(['XLON', 'XLAT']).to_netcdf(f\"{output1}SW/mdt_{year}_{month:02d}_{day:02d}.nc\")\n",
    "            ds_results['ds_NW'].drop_vars(['XLON', 'XLAT']).to_netcdf(f\"{output1}NW/mdt_{year}_{month:02d}_{day:02d}.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee049208-3439-4a23-943e-40488245cfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-17 01:05:59\n",
      "2025-02-17 01:06:13\n",
      "2025-02-17 01:06:28\n",
      "2025-02-17 01:06:43\n",
      "2025-02-17 01:06:58\n",
      "2025-02-17 01:07:13\n",
      "2025-02-17 01:07:28\n",
      "2025-02-17 01:07:46\n",
      "2025-02-17 01:08:03\n",
      "2025-02-17 01:08:16\n",
      "2025-02-17 01:08:33\n",
      "2025-02-17 01:08:53\n",
      "2025-02-17 01:09:08\n",
      "2025-02-17 01:09:29\n",
      "2025-02-17 01:09:46\n",
      "2025-02-17 01:10:00\n",
      "2025-02-17 01:10:17\n",
      "2025-02-17 01:10:33\n",
      "2025-02-17 01:10:46\n",
      "2025-02-17 01:11:00\n",
      "2025-02-17 01:11:14\n",
      "2025-02-17 01:11:30\n",
      "2025-02-17 01:11:46\n",
      "2025-02-17 01:12:02\n",
      "2025-02-17 01:12:20\n",
      "2025-02-17 01:12:40\n",
      "2025-02-17 01:12:54\n",
      "2025-02-17 01:13:07\n",
      "2025-02-17 01:13:23\n",
      "2025-02-17 01:13:40\n",
      "2025-02-17 01:13:56\n",
      "2025-02-17 01:14:17\n",
      "2025-02-17 01:14:20\n",
      "2025-02-17 01:14:31\n",
      "2025-02-17 01:14:44\n",
      "2025-02-17 01:15:01\n",
      "2025-02-17 01:15:18\n",
      "2025-02-17 01:15:33\n",
      "2025-02-17 01:15:52\n",
      "2025-02-17 01:16:09\n",
      "2025-02-17 01:16:27\n",
      "2025-02-17 01:16:47\n",
      "2025-02-17 01:17:05\n",
      "2025-02-17 01:17:21\n",
      "2025-02-17 01:17:35\n",
      "2025-02-17 01:17:55\n",
      "2025-02-17 01:18:14\n",
      "2025-02-17 01:18:28\n",
      "2025-02-17 01:18:42\n",
      "2025-02-17 01:18:56\n",
      "2025-02-17 01:19:15\n",
      "2025-02-17 01:19:34\n",
      "2025-02-17 01:19:53\n",
      "2025-02-17 01:20:10\n",
      "2025-02-17 01:20:25\n",
      "2025-02-17 01:20:40\n",
      "2025-02-17 01:20:53\n",
      "2025-02-17 01:21:07\n",
      "2025-02-17 01:21:22\n",
      "2025-02-17 01:21:40\n",
      "2025-02-17 01:21:56\n",
      "2025-02-17 01:22:11\n",
      "2025-02-17 01:22:29\n",
      "2025-02-17 01:22:32\n",
      "2025-02-17 01:22:44\n",
      "2025-02-17 01:23:01\n",
      "2025-02-17 01:23:18\n",
      "2025-02-17 01:23:34\n",
      "2025-02-17 01:23:58\n",
      "2025-02-17 01:24:17\n",
      "2025-02-17 01:24:36\n",
      "2025-02-17 01:24:54\n",
      "2025-02-17 01:25:15\n",
      "2025-02-17 01:25:33\n",
      "2025-02-17 01:25:55\n",
      "2025-02-17 01:26:16\n",
      "2025-02-17 01:26:34\n",
      "2025-02-17 01:26:55\n",
      "2025-02-17 01:27:15\n",
      "2025-02-17 01:27:34\n",
      "2025-02-17 01:27:59\n",
      "2025-02-17 01:28:26\n",
      "2025-02-17 01:28:50\n",
      "2025-02-17 01:29:08\n",
      "2025-02-17 01:29:32\n",
      "2025-02-17 01:29:52\n",
      "2025-02-17 01:30:10\n",
      "2025-02-17 01:30:29\n",
      "2025-02-17 01:30:54\n",
      "2025-02-17 01:31:21\n",
      "2025-02-17 01:31:42\n",
      "2025-02-17 01:32:05\n",
      "2025-02-17 01:32:29\n",
      "2025-02-17 01:32:49\n",
      "2025-02-17 01:33:23\n",
      "2025-02-17 01:33:29\n",
      "2025-02-17 01:33:50\n",
      "2025-02-17 01:34:17\n",
      "2025-02-17 01:34:49\n",
      "2025-02-17 01:35:32\n",
      "2025-02-17 01:35:57\n",
      "2025-02-17 01:36:16\n",
      "2025-02-17 01:36:33\n",
      "2025-02-17 01:36:51\n",
      "2025-02-17 01:37:09\n",
      "2025-02-17 01:37:32\n",
      "2025-02-17 01:37:57\n",
      "2025-02-17 01:38:19\n",
      "2025-02-17 01:38:41\n",
      "2025-02-17 01:38:58\n",
      "2025-02-17 01:39:27\n",
      "2025-02-17 01:39:49\n",
      "2025-02-17 01:40:13\n",
      "2025-02-17 01:40:33\n",
      "2025-02-17 01:40:57\n",
      "2025-02-17 01:41:23\n",
      "2025-02-17 01:41:45\n",
      "2025-02-17 01:42:05\n",
      "2025-02-17 01:42:23\n",
      "2025-02-17 01:42:46\n",
      "2025-02-17 01:43:06\n",
      "2025-02-17 01:43:27\n",
      "2025-02-17 01:43:49\n",
      "2025-02-17 01:44:11\n",
      "2025-02-17 01:44:32\n",
      "2025-02-17 01:44:54\n",
      "2025-02-17 01:45:22\n",
      "2025-02-17 01:45:26\n",
      "2025-02-17 01:45:49\n",
      "2025-02-17 01:46:17\n",
      "2025-02-17 01:46:46\n",
      "2025-02-17 01:47:13\n",
      "2025-02-17 01:47:40\n",
      "2025-02-17 01:48:08\n",
      "2025-02-17 01:48:35\n",
      "2025-02-17 01:49:01\n",
      "2025-02-17 01:49:23\n",
      "2025-02-17 01:49:51\n",
      "2025-02-17 01:50:15\n",
      "2025-02-17 01:50:40\n",
      "2025-02-17 01:51:08\n",
      "2025-02-17 01:51:33\n",
      "2025-02-17 01:51:58\n",
      "2025-02-17 01:52:27\n",
      "2025-02-17 01:52:51\n",
      "2025-02-17 01:53:18\n",
      "2025-02-17 01:53:39\n",
      "2025-02-17 01:54:00\n",
      "2025-02-17 01:54:24\n",
      "2025-02-17 01:54:48\n",
      "2025-02-17 01:55:11\n",
      "2025-02-17 01:55:33\n",
      "2025-02-17 01:56:03\n",
      "2025-02-17 01:56:27\n",
      "2025-02-17 01:56:53\n",
      "2025-02-17 01:57:17\n",
      "2025-02-17 01:57:20\n",
      "2025-02-17 01:57:44\n",
      "2025-02-17 01:58:16\n",
      "2025-02-17 01:58:45\n",
      "2025-02-17 01:59:12\n",
      "2025-02-17 01:59:44\n",
      "2025-02-17 02:00:11\n",
      "2025-02-17 02:00:38\n",
      "2025-02-17 02:01:04\n",
      "2025-02-17 02:01:36\n",
      "2025-02-17 02:02:03\n",
      "2025-02-17 02:02:35\n",
      "2025-02-17 02:03:01\n",
      "2025-02-17 02:03:28\n",
      "2025-02-17 02:03:56\n",
      "2025-02-17 02:04:22\n",
      "2025-02-17 02:04:51\n",
      "2025-02-17 02:05:16\n",
      "2025-02-17 02:05:38\n",
      "2025-02-17 02:06:07\n",
      "2025-02-17 02:06:35\n",
      "2025-02-17 02:06:57\n",
      "2025-02-17 02:07:23\n",
      "2025-02-17 02:07:49\n",
      "2025-02-17 02:08:11\n",
      "2025-02-17 02:08:41\n",
      "2025-02-17 02:09:01\n",
      "2025-02-17 02:09:24\n",
      "2025-02-17 02:09:46\n",
      "2025-02-17 02:10:13\n",
      "2025-02-17 02:10:42\n",
      "2025-02-17 02:11:04\n",
      "2025-02-17 02:11:07\n",
      "2025-02-17 02:11:24\n",
      "2025-02-17 02:11:47\n",
      "2025-02-17 02:12:10\n",
      "2025-02-17 02:12:38\n",
      "2025-02-17 02:13:05\n",
      "2025-02-17 02:13:29\n",
      "2025-02-17 02:13:52\n",
      "2025-02-17 02:14:14\n",
      "2025-02-17 02:14:37\n",
      "2025-02-17 02:15:04\n",
      "2025-02-17 02:15:35\n",
      "2025-02-17 02:15:58\n",
      "2025-02-17 02:16:28\n",
      "2025-02-17 02:16:54\n",
      "2025-02-17 02:17:32\n",
      "2025-02-17 02:17:56\n",
      "2025-02-17 02:18:21\n",
      "2025-02-17 02:18:39\n",
      "2025-02-17 02:19:00\n",
      "2025-02-17 02:19:19\n",
      "2025-02-17 02:19:50\n",
      "2025-02-17 02:20:18\n",
      "2025-02-17 02:20:57\n",
      "2025-02-17 02:21:27\n",
      "2025-02-17 02:21:52\n",
      "2025-02-17 02:22:19\n",
      "2025-02-17 02:22:49\n",
      "2025-02-17 02:23:14\n",
      "2025-02-17 02:23:38\n",
      "2025-02-17 02:23:59\n",
      "2025-02-17 02:24:01\n",
      "2025-02-17 02:24:21\n",
      "2025-02-17 02:24:43\n",
      "2025-02-17 02:25:04\n",
      "2025-02-17 02:25:30\n",
      "2025-02-17 02:25:49\n",
      "2025-02-17 02:26:09\n",
      "2025-02-17 02:26:35\n",
      "2025-02-17 02:27:07\n",
      "2025-02-17 02:27:35\n",
      "2025-02-17 02:28:08\n",
      "2025-02-17 02:28:39\n",
      "2025-02-17 02:29:08\n",
      "2025-02-17 02:29:40\n",
      "2025-02-17 02:30:12\n",
      "2025-02-17 02:30:47\n",
      "2025-02-17 02:31:22\n",
      "2025-02-17 02:31:51\n",
      "2025-02-17 02:32:22\n",
      "2025-02-17 02:32:49\n",
      "2025-02-17 02:33:18\n",
      "2025-02-17 02:33:49\n",
      "2025-02-17 02:34:19\n",
      "2025-02-17 02:34:48\n",
      "2025-02-17 02:35:15\n",
      "2025-02-17 02:35:45\n",
      "2025-02-17 02:36:12\n",
      "2025-02-17 02:36:39\n",
      "2025-02-17 02:37:08\n",
      "2025-02-17 02:37:36\n",
      "2025-02-17 02:38:03\n",
      "2025-02-17 02:38:32\n",
      "2025-02-17 02:38:38\n",
      "2025-02-17 02:39:07\n",
      "2025-02-17 02:39:36\n",
      "2025-02-17 02:40:06\n",
      "2025-02-17 02:40:40\n",
      "2025-02-17 02:41:03\n",
      "2025-02-17 02:41:38\n",
      "2025-02-17 02:42:11\n",
      "2025-02-17 02:42:41\n",
      "2025-02-17 02:43:10\n",
      "2025-02-17 02:43:41\n",
      "2025-02-17 02:44:06\n",
      "2025-02-17 02:44:30\n",
      "2025-02-17 02:44:57\n",
      "2025-02-17 02:45:26\n",
      "2025-02-17 02:45:50\n",
      "2025-02-17 02:46:18\n",
      "2025-02-17 02:46:43\n",
      "2025-02-17 02:47:06\n",
      "2025-02-17 02:47:29\n",
      "2025-02-17 02:47:52\n",
      "2025-02-17 02:48:23\n",
      "2025-02-17 02:48:48\n",
      "2025-02-17 02:49:10\n",
      "2025-02-17 02:49:33\n",
      "2025-02-17 02:49:55\n",
      "2025-02-17 02:50:18\n",
      "2025-02-17 02:50:43\n",
      "2025-02-17 02:51:05\n",
      "2025-02-17 02:51:25\n",
      "2025-02-17 02:51:47\n",
      "2025-02-17 02:51:51\n",
      "2025-02-17 02:52:16\n",
      "2025-02-17 02:52:44\n",
      "2025-02-17 02:53:15\n",
      "2025-02-17 02:53:42\n",
      "2025-02-17 02:54:08\n",
      "2025-02-17 02:54:37\n",
      "2025-02-17 02:55:08\n",
      "2025-02-17 02:55:46\n",
      "2025-02-17 02:56:17\n",
      "2025-02-17 02:56:55\n",
      "2025-02-17 02:57:38\n",
      "2025-02-17 02:58:14\n",
      "2025-02-17 02:58:48\n",
      "2025-02-17 02:59:21\n",
      "2025-02-17 02:59:51\n",
      "2025-02-17 03:00:29\n",
      "2025-02-17 03:01:01\n",
      "2025-02-17 03:01:35\n",
      "2025-02-17 03:02:08\n",
      "2025-02-17 03:02:43\n",
      "2025-02-17 03:03:16\n",
      "2025-02-17 03:03:47\n",
      "2025-02-17 03:04:18\n",
      "2025-02-17 03:04:53\n",
      "2025-02-17 03:05:21\n",
      "2025-02-17 03:05:55\n",
      "2025-02-17 03:06:28\n",
      "2025-02-17 03:07:03\n",
      "2025-02-17 03:07:35\n",
      "2025-02-17 03:08:03\n",
      "2025-02-17 03:08:35\n",
      "2025-02-17 03:08:39\n",
      "2025-02-17 03:09:06\n",
      "2025-02-17 03:09:36\n",
      "2025-02-17 03:10:08\n",
      "2025-02-17 03:10:40\n",
      "2025-02-17 03:11:11\n",
      "2025-02-17 03:11:48\n",
      "2025-02-17 03:12:19\n",
      "2025-02-17 03:12:52\n",
      "2025-02-17 03:13:22\n",
      "2025-02-17 03:13:49\n",
      "2025-02-17 03:14:26\n",
      "2025-02-17 03:15:02\n",
      "2025-02-17 03:15:36\n",
      "2025-02-17 03:16:10\n",
      "2025-02-17 03:16:41\n",
      "2025-02-17 03:17:15\n",
      "2025-02-17 03:17:49\n",
      "2025-02-17 03:18:27\n",
      "2025-02-17 03:18:58\n",
      "2025-02-17 03:19:35\n",
      "2025-02-17 03:20:08\n",
      "2025-02-17 03:20:41\n",
      "2025-02-17 03:21:16\n",
      "2025-02-17 03:21:47\n",
      "2025-02-17 03:22:22\n",
      "2025-02-17 03:22:58\n",
      "2025-02-17 03:23:36\n",
      "2025-02-17 03:24:06\n",
      "2025-02-17 03:24:43\n",
      "2025-02-17 03:25:18\n",
      "2025-02-17 03:25:56\n",
      "2025-02-17 03:26:00\n",
      "2025-02-17 03:26:28\n",
      "2025-02-17 03:27:02\n",
      "2025-02-17 03:27:39\n",
      "2025-02-17 03:28:08\n",
      "2025-02-17 03:28:40\n",
      "2025-02-17 03:29:13\n",
      "2025-02-17 03:29:45\n",
      "2025-02-17 03:30:16\n",
      "2025-02-17 03:30:47\n",
      "2025-02-17 03:31:17\n",
      "2025-02-17 03:31:46\n",
      "2025-02-17 03:32:13\n",
      "2025-02-17 03:32:39\n",
      "2025-02-17 03:33:08\n",
      "2025-02-17 03:33:35\n",
      "2025-02-17 03:34:03\n",
      "2025-02-17 03:34:33\n",
      "2025-02-17 03:35:04\n",
      "2025-02-17 03:35:28\n",
      "2025-02-17 03:35:59\n",
      "2025-02-17 03:36:27\n",
      "2025-02-17 03:36:51\n",
      "2025-02-17 03:37:18\n",
      "2025-02-17 03:37:40\n",
      "2025-02-17 03:38:01\n",
      "2025-02-17 03:38:32\n",
      "2025-02-17 03:38:53\n",
      "2025-02-17 03:39:18\n",
      "2025-02-17 03:39:51\n",
      "2025-02-17 03:40:17\n",
      "2025-02-17 03:40:21\n",
      "2025-02-17 03:40:47\n",
      "2025-02-17 03:41:11\n",
      "2025-02-17 03:41:34\n",
      "2025-02-17 03:42:08\n",
      "2025-02-17 03:42:37\n",
      "2025-02-17 03:43:11\n",
      "2025-02-17 03:43:40\n",
      "2025-02-17 03:44:08\n",
      "2025-02-17 03:44:36\n",
      "2025-02-17 03:45:02\n",
      "2025-02-17 03:45:31\n",
      "2025-02-17 03:46:02\n",
      "2025-02-17 03:46:26\n",
      "2025-02-17 03:46:48\n",
      "2025-02-17 03:47:17\n",
      "2025-02-17 03:47:47\n",
      "2025-02-17 03:48:15\n",
      "2025-02-17 03:48:43\n",
      "2025-02-17 03:49:11\n",
      "2025-02-17 03:49:41\n",
      "2025-02-17 03:50:09\n",
      "2025-02-17 03:50:41\n",
      "2025-02-17 03:51:11\n",
      "2025-02-17 03:51:40\n",
      "2025-02-17 03:52:09\n",
      "2025-02-17 03:52:39\n",
      "2025-02-17 03:53:08\n",
      "2025-02-17 03:53:36\n",
      "2025-02-17 03:54:03\n",
      "2025-02-17 03:54:34\n",
      "2025-02-17 03:55:08\n",
      "2025-02-17 03:55:10\n",
      "2025-02-17 03:55:36\n",
      "2025-02-17 03:56:02\n",
      "2025-02-17 03:56:31\n",
      "2025-02-17 03:56:59\n",
      "2025-02-17 03:57:28\n",
      "2025-02-17 03:57:54\n",
      "2025-02-17 03:58:26\n",
      "2025-02-17 03:58:58\n",
      "2025-02-17 03:59:44\n",
      "2025-02-17 04:00:19\n",
      "2025-02-17 04:00:43\n",
      "2025-02-17 04:01:08\n",
      "2025-02-17 04:01:36\n",
      "2025-02-17 04:01:57\n",
      "2025-02-17 04:02:33\n",
      "2025-02-17 04:03:05\n",
      "2025-02-17 04:03:28\n",
      "2025-02-17 04:03:54\n",
      "2025-02-17 04:04:18\n",
      "2025-02-17 04:04:46\n",
      "2025-02-17 04:05:09\n",
      "2025-02-17 04:05:40\n",
      "2025-02-17 04:06:11\n",
      "2025-02-17 04:06:33\n",
      "2025-02-17 04:07:04\n",
      "2025-02-17 04:07:31\n",
      "2025-02-17 04:08:05\n",
      "2025-02-17 04:08:30\n",
      "2025-02-17 04:09:01\n",
      "2025-02-17 04:09:33\n",
      "2025-02-17 04:09:35\n",
      "2025-02-17 04:10:03\n",
      "2025-02-17 04:10:30\n",
      "2025-02-17 04:10:56\n",
      "2025-02-17 04:11:26\n",
      "2025-02-17 04:11:49\n",
      "2025-02-17 04:12:17\n",
      "2025-02-17 04:12:45\n",
      "2025-02-17 04:13:15\n",
      "2025-02-17 04:13:44\n",
      "2025-02-17 04:14:15\n",
      "2025-02-17 04:14:44\n",
      "2025-02-17 04:15:08\n",
      "2025-02-17 04:15:38\n",
      "2025-02-17 04:16:04\n",
      "2025-02-17 04:16:32\n",
      "2025-02-17 04:17:00\n",
      "2025-02-17 04:17:26\n",
      "2025-02-17 04:17:54\n",
      "2025-02-17 04:18:22\n",
      "2025-02-17 04:18:48\n",
      "2025-02-17 04:19:15\n",
      "2025-02-17 04:19:45\n",
      "2025-02-17 04:20:12\n",
      "2025-02-17 04:20:42\n",
      "2025-02-17 04:21:10\n",
      "2025-02-17 04:21:36\n",
      "2025-02-17 04:22:07\n",
      "2025-02-17 04:22:34\n",
      "2025-02-17 04:23:03\n",
      "2025-02-17 04:23:36\n",
      "2025-02-17 04:24:08\n",
      "2025-02-17 04:24:12\n",
      "2025-02-17 04:24:36\n",
      "2025-02-17 04:25:07\n",
      "2025-02-17 04:25:33\n",
      "2025-02-17 04:26:05\n",
      "2025-02-17 04:26:35\n",
      "2025-02-17 04:27:05\n",
      "2025-02-17 04:27:32\n",
      "2025-02-17 04:27:57\n",
      "2025-02-17 04:28:32\n",
      "2025-02-17 04:29:03\n",
      "2025-02-17 04:29:34\n",
      "2025-02-17 04:30:08\n",
      "2025-02-17 04:30:38\n",
      "2025-02-17 04:31:08\n",
      "2025-02-17 04:31:37\n",
      "2025-02-17 04:32:10\n",
      "2025-02-17 04:32:37\n",
      "2025-02-17 04:33:09\n",
      "2025-02-17 04:33:36\n",
      "2025-02-17 04:34:02\n",
      "2025-02-17 04:34:34\n",
      "2025-02-17 04:35:06\n",
      "2025-02-17 04:35:35\n",
      "2025-02-17 04:36:05\n",
      "2025-02-17 04:36:35\n",
      "2025-02-17 04:37:08\n",
      "2025-02-17 04:37:38\n",
      "2025-02-17 04:38:06\n",
      "2025-02-17 04:38:35\n",
      "2025-02-17 04:39:05\n",
      "2025-02-17 04:39:37\n",
      "2025-02-17 04:39:41\n",
      "2025-02-17 04:40:11\n",
      "2025-02-17 04:40:40\n",
      "2025-02-17 04:41:08\n",
      "2025-02-17 04:41:41\n",
      "2025-02-17 04:42:15\n",
      "2025-02-17 04:42:53\n",
      "2025-02-17 04:43:25\n",
      "2025-02-17 04:43:57\n",
      "2025-02-17 04:44:28\n",
      "2025-02-17 04:44:59\n",
      "2025-02-17 04:45:26\n",
      "2025-02-17 04:46:01\n",
      "2025-02-17 04:46:29\n",
      "2025-02-17 04:47:04\n",
      "2025-02-17 04:47:34\n",
      "2025-02-17 04:48:02\n",
      "2025-02-17 04:48:28\n",
      "2025-02-17 04:49:01\n",
      "2025-02-17 04:49:28\n",
      "2025-02-17 04:49:56\n",
      "2025-02-17 04:50:26\n",
      "2025-02-17 04:50:55\n",
      "2025-02-17 04:51:21\n",
      "2025-02-17 04:51:48\n",
      "2025-02-17 04:52:20\n",
      "2025-02-17 04:52:50\n",
      "2025-02-17 04:53:18\n",
      "2025-02-17 04:53:48\n",
      "2025-02-17 04:53:51\n",
      "2025-02-17 04:54:13\n",
      "2025-02-17 04:54:44\n",
      "2025-02-17 04:55:11\n",
      "2025-02-17 04:55:34\n",
      "2025-02-17 04:56:04\n",
      "2025-02-17 04:56:31\n",
      "2025-02-17 04:57:01\n",
      "2025-02-17 04:57:30\n",
      "2025-02-17 04:57:56\n",
      "2025-02-17 04:58:18\n",
      "2025-02-17 04:58:42\n",
      "2025-02-17 04:59:11\n",
      "2025-02-17 04:59:39\n",
      "2025-02-17 05:00:04\n",
      "2025-02-17 05:00:35\n",
      "2025-02-17 05:00:55\n",
      "2025-02-17 05:01:20\n",
      "2025-02-17 05:01:45\n",
      "2025-02-17 05:02:13\n",
      "2025-02-17 05:02:42\n",
      "2025-02-17 05:03:12\n",
      "2025-02-17 05:03:43\n",
      "2025-02-17 05:04:08\n",
      "2025-02-17 05:04:41\n",
      "2025-02-17 05:05:14\n",
      "2025-02-17 05:05:40\n",
      "2025-02-17 05:06:15\n",
      "2025-02-17 05:06:43\n",
      "2025-02-17 05:07:17\n",
      "2025-02-17 05:07:43\n",
      "2025-02-17 05:08:12\n",
      "2025-02-17 05:08:15\n",
      "2025-02-17 05:08:39\n",
      "2025-02-17 05:09:07\n",
      "2025-02-17 05:09:38\n",
      "2025-02-17 05:10:08\n",
      "2025-02-17 05:10:37\n",
      "2025-02-17 05:11:04\n",
      "2025-02-17 05:11:37\n",
      "2025-02-17 05:12:07\n",
      "2025-02-17 05:12:36\n",
      "2025-02-17 05:13:05\n",
      "2025-02-17 05:13:33\n",
      "2025-02-17 05:14:02\n",
      "2025-02-17 05:14:36\n",
      "2025-02-17 05:15:06\n",
      "2025-02-17 05:15:34\n",
      "2025-02-17 05:16:06\n",
      "2025-02-17 05:16:38\n",
      "2025-02-17 05:17:07\n",
      "2025-02-17 05:17:42\n",
      "2025-02-17 05:18:13\n",
      "2025-02-17 05:18:48\n",
      "2025-02-17 05:19:14\n",
      "2025-02-17 05:19:41\n",
      "2025-02-17 05:20:14\n",
      "2025-02-17 05:20:40\n",
      "2025-02-17 05:21:11\n",
      "2025-02-17 05:21:38\n",
      "2025-02-17 05:22:11\n",
      "2025-02-17 05:22:36\n",
      "2025-02-17 05:23:12\n",
      "2025-02-17 05:23:16\n",
      "2025-02-17 05:23:44\n",
      "2025-02-17 05:24:17\n",
      "2025-02-17 05:24:46\n",
      "2025-02-17 05:25:14\n",
      "2025-02-17 05:25:44\n",
      "2025-02-17 05:26:17\n",
      "2025-02-17 05:26:44\n",
      "2025-02-17 05:27:12\n",
      "2025-02-17 05:27:46\n",
      "2025-02-17 05:28:13\n",
      "2025-02-17 05:28:43\n",
      "2025-02-17 05:29:12\n",
      "2025-02-17 05:29:43\n",
      "2025-02-17 05:30:12\n",
      "2025-02-17 05:30:40\n",
      "2025-02-17 05:31:08\n",
      "2025-02-17 05:31:36\n",
      "2025-02-17 05:32:06\n",
      "2025-02-17 05:32:39\n",
      "2025-02-17 05:33:10\n",
      "2025-02-17 05:33:40\n",
      "2025-02-17 05:34:10\n",
      "2025-02-17 05:34:37\n",
      "2025-02-17 05:35:11\n",
      "2025-02-17 05:35:48\n",
      "2025-02-17 05:36:18\n",
      "2025-02-17 05:36:47\n",
      "2025-02-17 05:37:16\n",
      "2025-02-17 05:37:52\n",
      "2025-02-17 05:38:30\n",
      "2025-02-17 05:38:59\n",
      "2025-02-17 05:39:02\n",
      "2025-02-17 05:39:29\n",
      "2025-02-17 05:40:02\n",
      "2025-02-17 05:40:33\n",
      "2025-02-17 05:41:07\n",
      "2025-02-17 05:41:38\n",
      "2025-02-17 05:42:06\n",
      "2025-02-17 05:42:38\n",
      "2025-02-17 05:43:05\n",
      "2025-02-17 05:43:37\n",
      "2025-02-17 05:44:07\n",
      "2025-02-17 05:44:36\n",
      "2025-02-17 05:45:05\n",
      "2025-02-17 05:45:32\n",
      "2025-02-17 05:45:59\n",
      "2025-02-17 05:46:25\n",
      "2025-02-17 05:46:46\n",
      "2025-02-17 05:47:10\n",
      "2025-02-17 05:47:36\n",
      "2025-02-17 05:48:05\n",
      "2025-02-17 05:48:31\n",
      "2025-02-17 05:48:57\n",
      "2025-02-17 05:49:31\n",
      "2025-02-17 05:50:06\n",
      "2025-02-17 05:50:37\n",
      "2025-02-17 05:51:09\n",
      "2025-02-17 05:51:44\n",
      "2025-02-17 05:52:14\n",
      "2025-02-17 05:52:43\n",
      "2025-02-17 05:53:10\n",
      "2025-02-17 05:53:42\n",
      "2025-02-17 05:53:46\n",
      "2025-02-17 05:54:15\n",
      "2025-02-17 05:54:41\n",
      "2025-02-17 05:55:05\n",
      "2025-02-17 05:55:29\n",
      "2025-02-17 05:55:54\n",
      "2025-02-17 05:56:23\n",
      "2025-02-17 05:56:48\n",
      "2025-02-17 05:57:29\n",
      "2025-02-17 05:57:58\n",
      "2025-02-17 05:58:33\n",
      "2025-02-17 05:59:05\n",
      "2025-02-17 05:59:39\n",
      "2025-02-17 06:00:12\n",
      "2025-02-17 06:00:38\n",
      "2025-02-17 06:01:03\n",
      "2025-02-17 06:01:34\n",
      "2025-02-17 06:02:06\n",
      "2025-02-17 06:02:36\n",
      "2025-02-17 06:03:09\n",
      "2025-02-17 06:03:40\n",
      "2025-02-17 06:04:10\n",
      "2025-02-17 06:04:40\n",
      "2025-02-17 06:05:11\n",
      "2025-02-17 06:05:39\n",
      "2025-02-17 06:06:08\n",
      "2025-02-17 06:06:35\n",
      "2025-02-17 06:07:06\n",
      "2025-02-17 06:07:36\n",
      "2025-02-17 06:08:04\n",
      "2025-02-17 06:08:32\n",
      "2025-02-17 06:09:01\n",
      "2025-02-17 06:09:04\n",
      "2025-02-17 06:09:28\n",
      "2025-02-17 06:10:00\n",
      "2025-02-17 06:10:31\n",
      "2025-02-17 06:11:01\n",
      "2025-02-17 06:11:31\n",
      "2025-02-17 06:11:56\n",
      "2025-02-17 06:12:29\n",
      "2025-02-17 06:12:56\n",
      "2025-02-17 06:13:23\n",
      "2025-02-17 06:14:01\n",
      "2025-02-17 06:14:27\n",
      "2025-02-17 06:14:59\n",
      "2025-02-17 06:15:38\n",
      "2025-02-17 06:16:10\n",
      "2025-02-17 06:16:35\n",
      "2025-02-17 06:17:11\n",
      "2025-02-17 06:17:38\n",
      "2025-02-17 06:18:07\n",
      "2025-02-17 06:18:38\n",
      "2025-02-17 06:19:07\n",
      "2025-02-17 06:19:34\n",
      "2025-02-17 06:19:59\n",
      "2025-02-17 06:20:28\n",
      "2025-02-17 06:21:00\n",
      "2025-02-17 06:21:32\n",
      "2025-02-17 06:22:04\n",
      "2025-02-17 06:22:38\n",
      "2025-02-17 06:23:11\n",
      "2025-02-17 06:23:41\n",
      "2025-02-17 06:24:12\n",
      "2025-02-17 06:24:42\n",
      "2025-02-17 06:24:44\n",
      "2025-02-17 06:25:10\n",
      "2025-02-17 06:25:36\n",
      "2025-02-17 06:26:08\n",
      "2025-02-17 06:26:37\n",
      "2025-02-17 06:27:03\n",
      "2025-02-17 06:27:31\n",
      "2025-02-17 06:28:00\n",
      "2025-02-17 06:28:29\n",
      "2025-02-17 06:28:58\n",
      "2025-02-17 06:29:30\n",
      "2025-02-17 06:29:56\n",
      "2025-02-17 06:30:25\n",
      "2025-02-17 06:30:55\n",
      "2025-02-17 06:31:29\n",
      "2025-02-17 06:32:02\n",
      "2025-02-17 06:32:32\n",
      "2025-02-17 06:32:58\n",
      "2025-02-17 06:33:28\n",
      "2025-02-17 06:33:53\n",
      "2025-02-17 06:34:20\n",
      "2025-02-17 06:34:43\n",
      "2025-02-17 06:35:12\n",
      "2025-02-17 06:35:38\n",
      "2025-02-17 06:36:06\n",
      "2025-02-17 06:36:34\n",
      "2025-02-17 06:36:59\n",
      "2025-02-17 06:37:37\n",
      "2025-02-17 06:38:06\n",
      "2025-02-17 06:38:33\n",
      "2025-02-17 06:39:00\n"
     ]
    }
   ],
   "source": [
    "gdf = gpd.read_file('/N/project/Zli_lab/Data/Other/tl_2019_us_state/tl_2019_us_state.shp')\n",
    "input_folder = '/N/project/Zli_lab/Data/Observations/NCAR/prec_acc_files/'\n",
    "\n",
    "start_year = 1985\n",
    "end_year = start_year+3\n",
    "for year in range(start_year, end_year):  # 1989不包含\n",
    "\n",
    "    months = range(10, 13) if year == start_year else range(1, 10) if year == (end_year - 1) else range(1, 13)\n",
    "    # 遍历月份\n",
    "    for month in months:\n",
    "        print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "        # 获取当前月份的天数\n",
    "        if month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "            num_days = 31\n",
    "        elif month in [4, 6, 9, 11]:\n",
    "            num_days = 30\n",
    "        elif month == 2:\n",
    "            # 考虑闰年\n",
    "            if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0):\n",
    "                num_days = 29  # 闰年\n",
    "            else:\n",
    "                num_days = 28  # 平年\n",
    "\n",
    "        # 遍历每个月的天数\n",
    "        for day in range(1, num_days + 1):\n",
    "            \n",
    "            month_str = f\"{month:02}\"\n",
    "            day_str = f\"{day:02}\"\n",
    "            input_file = f'PREC_ACC_NC.wrf2d_d01_{year}-{month_str}-{day_str}_*.nc'\n",
    "            ds = xr.open_mfdataset(input_folder + input_file)\n",
    "            ds_era = xr.Dataset({'p': (['time', 'latitude', 'longitude'], ds.PREC_ACC_NC.values)},\n",
    "                                coords={'longitude': (['longitude'], ds.XLONG.values[500]),\n",
    "                                        'latitude': (['latitude'], ds.XLAT.values[:,500]),\n",
    "                                        'time': ('time', ds.Time.values)})\n",
    "            ds_era_lon, ds_era_lat = np.meshgrid(ds_era.longitude.values, ds_era.latitude.values, indexing='xy')\n",
    "            # 转换为 xarray DataArray，确保其维度与 ds_era_clipped 对齐\n",
    "            ds_era_lon_da = xr.DataArray(ds_era_lon, dims=(\"latitude\", \"longitude\"), coords={\"latitude\": ds_era.latitude, \"longitude\": ds_era.longitude})\n",
    "            ds_era_lat_da = xr.DataArray(ds_era_lat, dims=(\"latitude\", \"longitude\"), coords={\"latitude\": ds_era.latitude, \"longitude\": ds_era.longitude})\n",
    "\n",
    "            # 使用 assign_coords 将二维坐标添加到 ds_era_clipped\n",
    "            ds_era_clipped = ds_era.assign_coords(lon_2d=ds_era_lon_da, lat_2d=ds_era_lat_da)\n",
    "\n",
    "            lon = ds_era_clipped['lon_2d'].values\n",
    "            lat = ds_era_clipped['lat_2d'].values\n",
    "            grid = gpd.GeoDataFrame(\n",
    "                geometry=gpd.points_from_xy(lon.flatten(), lat.flatten()),\n",
    "                index=np.arange(lon.size)\n",
    "            )\n",
    "            grid.set_crs(gdf.crs, inplace=True)\n",
    "            grid_s = gpd.sjoin(grid, gdf, how='inner', predicate='within')\n",
    "\n",
    "            mask = np.full(ds_era_clipped['p'].shape[1:], False) \n",
    "            for index in grid_s.index:\n",
    "                row, col = np.unravel_index(index, mask.shape)  # 获取行列索引\n",
    "                mask[row, col] = True\n",
    "            mask_da = xr.DataArray(mask, dims=ds_era_clipped['p'].dims[1:], coords={'latitude': ds_era_clipped['p'].coords['latitude'], 'longitude': ds_era_clipped['p'].coords['longitude']})\n",
    "            ds_sss = ds_era_clipped.where(mask_da, drop=True)\n",
    "            ds_sss = ds_sss.drop_vars(['lon_2d', 'lat_2d'])\n",
    "\n",
    "\n",
    "            original_times = ds_sss.time.values \n",
    "            \n",
    "            lon_ranges = [(-np.inf, -112.5), (-112.5, -97.5), (-97.5, -82.5), (-82.5, np.inf)]\n",
    "            utc_offsets = [-8, -7, -6, -5]\n",
    "            print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()))\n",
    "            for (lon_min, lon_max), offset in zip(lon_ranges, utc_offsets):\n",
    "                mask = (ds_sss.longitude >= lon_min) & (ds_sss.longitude < lon_max)\n",
    "                ds_lon_subset = ds_sss.where(mask, drop=True)\n",
    "                if ds_lon_subset.latitude.size > 0 and ds_lon_subset.longitude.size > 0:\n",
    "            \n",
    "                    adjusted_times = original_times + np.timedelta64(offset, 'h')  # 保持24个时间点\n",
    "            \n",
    "                    ds_lon_subset = ds_lon_subset.assign_coords(time=adjusted_times)\n",
    "            \n",
    "                    lat_min = ds_lon_subset.latitude.min().values\n",
    "                    lat_max = ds_lon_subset.latitude.max().values\n",
    "            \n",
    "                    lat_splits = np.linspace(lat_min, lat_max, 10)  # 10个值分9段\n",
    "                    \n",
    "                    for i in range(len(lat_splits) - 1):\n",
    "                        lat_min_split = lat_splits[i]\n",
    "                        lat_max_split = lat_splits[i + 1]\n",
    "                        lat_mask = (ds_lon_subset.latitude >= lat_min_split) & (ds_lon_subset.latitude < lat_max_split)\n",
    "                        ds_lat_subset = ds_lon_subset.where(lat_mask, drop=True)\n",
    "                        \n",
    "                        output_folder = '/N/project/Zli_lab/gongg/CONUS404_data/LST/re_UTC/U' + str(offset)+str(i) + '/'\n",
    "                        output_file = f'PREC_ACC_NC.wrf2d_d01_{year}-{month_str}-{day_str}.nc'\n",
    "                        output_path = os.path.join(output_folder, output_file)\n",
    "                        os.makedirs(output_folder, exist_ok=True)\n",
    "                        ds_lat_subset.to_netcdf(output_folder + output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a20977-d013-4301-979e-0c3bccba1137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
