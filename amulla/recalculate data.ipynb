{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8f5b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/N/u/amulla/Quartz/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/geode2/home/u010/amulla/Quartz/Desktop/hourly_precipitation_analysis'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59760411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/N/project/Zli_lab/ERA5_land_data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/N/project/Zli_lab/ERA5_land_data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /N/project/Zli_lab/ERA5_land_data\n",
    "curr_data_path = os.getcwd()\n",
    "curr_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dddccf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(s_dir, month, year):\n",
    "    path = os.path.join(s_dir, f\"Hourly_Total_Precipitation_T_{str(month).zfill(2)}{year}.nc\")\n",
    "    ds = xr.open_dataset(path)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efa3dc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  2,  26,  50,  74,  98, 122, 146, 170, 194, 218, 242, 266, 290,\n",
       "        314, 338, 362, 386, 410, 434, 458, 482, 506, 530, 554, 578, 602,\n",
       "        626, 650, 674, 698]),)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = read_data('../ERA5_land_data_recalculated/JJA_prev_val', 6,1979)\n",
    "np.where(np.arange(len(ex.tp.data))%24==2)\n",
    "# np.where(np.arange(len(data))%24==2)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2eccb6",
   "metadata": {},
   "source": [
    "# JJA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c188b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "5-6-7-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c4eb321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalculate(data):\n",
    "    nan_mask = np.isnan(data)\n",
    "    binary_mask = np.where(nan_mask, 0, 1)\n",
    "    numeric_indices = np.where(binary_mask == 1)[0]\n",
    "    if len(numeric_indices) == 0:\n",
    "        differenced_data = data[1:]\n",
    "    elif len(numeric_indices) == len(data):\n",
    "        differenced_data = np.zeros_like(data[1:])\n",
    "        indices_01utc = np.where(np.arange(len(data))%24==2)[0]\n",
    "        indices_allutc = np.where(np.arange(len(data))%24!=2)[0][1:]\n",
    "        differenced_data[1::24] = data[indices_01utc]\n",
    "        differenced_data[indices_allutc - 1] = np.diff(data)[indices_allutc - 1]\n",
    "    else:\n",
    "        print(\"Error Encountered...\")\n",
    "    return differenced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb3b6b0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979-06-30T23:00:00.000000000\n",
      "1979\n",
      "1980-06-30T23:00:00.000000000\n",
      "1980\n",
      "1981-06-30T23:00:00.000000000\n",
      "1981\n",
      "1982-06-30T23:00:00.000000000\n",
      "1982\n",
      "1983-06-30T23:00:00.000000000\n",
      "1983\n",
      "1984-06-30T23:00:00.000000000\n",
      "1984\n",
      "1985-06-30T23:00:00.000000000\n",
      "1985\n",
      "1986-06-30T23:00:00.000000000\n",
      "1986\n",
      "1987-06-30T23:00:00.000000000\n",
      "1987\n",
      "1988-06-30T23:00:00.000000000\n",
      "1988\n",
      "1989-06-30T23:00:00.000000000\n",
      "1989\n",
      "1990-06-30T23:00:00.000000000\n",
      "1990\n",
      "1991-06-30T23:00:00.000000000\n",
      "1991\n",
      "1992-06-30T23:00:00.000000000\n",
      "1992\n",
      "1993-06-30T23:00:00.000000000\n",
      "1993\n",
      "1994-06-30T23:00:00.000000000\n",
      "1994\n",
      "1995-06-30T23:00:00.000000000\n",
      "1995\n",
      "1996-06-30T23:00:00.000000000\n",
      "1996\n",
      "1997-06-30T23:00:00.000000000\n",
      "1997\n",
      "1998-06-30T23:00:00.000000000\n",
      "1998\n",
      "1999-06-30T23:00:00.000000000\n",
      "1999\n",
      "2000-06-30T23:00:00.000000000\n",
      "2000\n",
      "2001-06-30T23:00:00.000000000\n",
      "2001\n",
      "2002-06-30T23:00:00.000000000\n",
      "2002\n",
      "2003-06-30T23:00:00.000000000\n",
      "2003\n",
      "2004-06-30T23:00:00.000000000\n",
      "2004\n",
      "2005-06-30T23:00:00.000000000\n",
      "2005\n",
      "2006-06-30T23:00:00.000000000\n",
      "2006\n",
      "2007-06-30T23:00:00.000000000\n",
      "2007\n",
      "2008-06-30T23:00:00.000000000\n",
      "2008\n",
      "2009-06-30T23:00:00.000000000\n",
      "2009\n",
      "2010-06-30T23:00:00.000000000\n",
      "2010\n",
      "2011-06-30T23:00:00.000000000\n",
      "2011\n",
      "2012-06-30T23:00:00.000000000\n",
      "2012\n",
      "2013-06-30T23:00:00.000000000\n",
      "2013\n",
      "2014-06-30T23:00:00.000000000\n",
      "2014\n",
      "2015-06-30T23:00:00.000000000\n",
      "2015\n",
      "2016-06-30T23:00:00.000000000\n",
      "2016\n",
      "2017-06-30T23:00:00.000000000\n",
      "2017\n",
      "2018-06-30T23:00:00.000000000\n",
      "2018\n",
      "2019-06-30T23:00:00.000000000\n",
      "2019\n",
      "2020-06-30T23:00:00.000000000\n",
      "2020\n",
      "2021-06-30T23:00:00.000000000\n",
      "2021\n",
      "2022-06-30T23:00:00.000000000\n",
      "2022\n",
      "2023-06-30T23:00:00.000000000\n",
      "2023\n"
     ]
    }
   ],
   "source": [
    "for year in range(1979,2024):\n",
    "    ds_6 = read_data(curr_data_path, 6, year)\n",
    "    ds_7 = read_data(curr_data_path, 7, year)\n",
    "    your_existing_dataarray = ds_6.tp[-1]\n",
    "    print(ds_6.tp[-1].time.data)\n",
    "    new_ds = xr.Dataset({'tp': your_existing_dataarray})\n",
    "    combined_ds = xr.concat([new_ds, ds_7], dim='time')\n",
    "    combined_ds = combined_ds.transpose('time' , 'latitude', 'longitude')\n",
    "    combined_ds.to_netcdf(f\"../ERA5_land_data_recalculated/JJA/Hourly_Total_Precipitation_T_{str(7).zfill(2)}{year}.nc\")\n",
    "    result = xr.apply_ufunc(\n",
    "            recalculate,\n",
    "            combined_ds,\n",
    "            input_core_dims=[['time']],\n",
    "            vectorize=True,\n",
    "            output_core_dims=[['time_recalc']],\n",
    "            output_dtypes=[np.float64],\n",
    "            dask='parallelized',\n",
    "        )\n",
    "    result = result.transpose('time_recalc' ,'latitude', 'longitude')\n",
    "    \n",
    "    time = combined_ds.time[1:]\n",
    "    latitude = result['latitude']\n",
    "    longitude = result['longitude']\n",
    "\n",
    "    final_result = xr.Dataset(coords={'latitude': latitude, 'longitude': longitude, 'time': time})\n",
    "\n",
    "    final_result['tp'] = (('time', 'latitude', 'longitude'), np.nan * np.ones((len(time), len(latitude), len(longitude))))\n",
    "\n",
    "    final_result['tp'][:,:,:] = result['tp'][:,:,:].data\n",
    "    final_result.to_netcdf(f\"../ERA5_land_data_recalculated/Hourly_Total_Precipitation_T_{str(7).zfill(2)}{year}.nc\")\n",
    "    print(year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72a5759",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n"
     ]
    }
   ],
   "source": [
    "for year in range(1979,2024):\n",
    "    ds_7 = read_data(curr_data_path, 7, year)\n",
    "    ds_8 = read_data(curr_data_path, 8, year)\n",
    "    your_existing_dataarray = ds_7.tp[-1]\n",
    "    new_ds = xr.Dataset({'tp': your_existing_dataarray})\n",
    "    combined_ds = xr.concat([new_ds, ds_8], dim='time')\n",
    "    combined_ds = combined_ds.transpose('time' , 'latitude', 'longitude')\n",
    "    combined_ds.to_netcdf(f\"../ERA5_land_data_recalculated/JJA/Hourly_Total_Precipitation_T_{str(8).zfill(2)}{year}.nc\")\n",
    "    result = xr.apply_ufunc(\n",
    "            recalculate,\n",
    "            combined_ds,\n",
    "            input_core_dims=[['time']],\n",
    "            vectorize=True,\n",
    "            output_core_dims=[['time_recalc']],\n",
    "            output_dtypes=[np.float64],\n",
    "            dask='parallelized',\n",
    "        )\n",
    "    result = result.transpose('time_recalc' ,'latitude', 'longitude')\n",
    "    \n",
    "    time = combined_ds.time[1:]\n",
    "    latitude = result['latitude']\n",
    "    longitude = result['longitude']\n",
    "\n",
    "    final_result = xr.Dataset(coords={'latitude': latitude, 'longitude': longitude, 'time': time})\n",
    "\n",
    "    final_result['tp'] = (('time', 'latitude', 'longitude'), np.nan * np.ones((len(time), len(latitude), len(longitude))))\n",
    "\n",
    "    final_result['tp'][:,:,:] = result['tp'][:,:,:].data\n",
    "    final_result.to_netcdf(f\"../ERA5_land_data_recalculated/Hourly_Total_Precipitation_T_{str(8).zfill(2)}{year}.nc\")\n",
    "    print(year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a90286d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform temporal aggregation by calculating percentile and then nansum or directly nansum for total accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52b2db41",
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_data = []\n",
    "for year in range(1979,1980):\n",
    "    for month in range(9, 10):\n",
    "        ds = read_data(curr_data_path, month, year)\n",
    "        if 'expver' in ds.dims:\n",
    "            ds = ds.sel(expver=1).combine_first(ds.sel(expver=5))\n",
    "            yearly_data.append(ds)\n",
    "        else:\n",
    "            yearly_data.append(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0b1e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6962726d",
   "metadata": {},
   "source": [
    "# MAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed486a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "2-3-4-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c97960d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalculate(data):\n",
    "    nan_mask = np.isnan(data)\n",
    "    binary_mask = np.where(nan_mask, 0, 1)\n",
    "    numeric_indices = np.where(binary_mask == 1)[0]\n",
    "    if len(numeric_indices) == 0:\n",
    "        differenced_data = data[1:]\n",
    "    elif len(numeric_indices) == len(data):\n",
    "        differenced_data = np.zeros_like(data[1:])\n",
    "        indices_01utc = np.where(np.arange(len(data))%24==2)[0]\n",
    "        indices_allutc = np.where(np.arange(len(data))%24!=2)[0][1:]\n",
    "        differenced_data[1::24] = data[indices_01utc]\n",
    "        differenced_data[indices_allutc - 1] = np.diff(data)[indices_allutc - 1]\n",
    "    else:\n",
    "        print(\"Error Encountered...\")\n",
    "    return differenced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1e76352b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979-02-28T23:00:00.000000000\n",
      "1979\n",
      "1980-02-29T23:00:00.000000000\n",
      "1980\n",
      "1981-02-28T23:00:00.000000000\n",
      "1981\n",
      "1982-02-28T23:00:00.000000000\n",
      "1982\n",
      "1983-02-28T23:00:00.000000000\n",
      "1983\n",
      "1984-02-29T23:00:00.000000000\n",
      "1984\n",
      "1985-02-28T23:00:00.000000000\n",
      "1985\n",
      "1986-02-28T23:00:00.000000000\n",
      "1986\n",
      "1987-02-28T23:00:00.000000000\n",
      "1987\n",
      "1988-02-29T23:00:00.000000000\n",
      "1988\n",
      "1989-02-28T23:00:00.000000000\n",
      "1989\n",
      "1990-02-28T23:00:00.000000000\n",
      "1990\n",
      "1991-02-28T23:00:00.000000000\n",
      "1991\n",
      "1992-02-29T23:00:00.000000000\n",
      "1992\n",
      "1993-02-28T23:00:00.000000000\n",
      "1993\n",
      "1994-02-28T23:00:00.000000000\n",
      "1994\n",
      "1995-02-28T23:00:00.000000000\n",
      "1995\n",
      "1996-02-29T23:00:00.000000000\n",
      "1996\n",
      "1997-02-28T23:00:00.000000000\n",
      "1997\n",
      "1998-02-28T23:00:00.000000000\n",
      "1998\n",
      "1999-02-28T23:00:00.000000000\n",
      "1999\n",
      "2000-02-29T23:00:00.000000000\n",
      "2000\n",
      "2001-02-28T23:00:00.000000000\n",
      "2001\n",
      "2002-02-28T23:00:00.000000000\n",
      "2002\n",
      "2003-02-28T23:00:00.000000000\n",
      "2003\n",
      "2004-02-29T23:00:00.000000000\n",
      "2004\n",
      "2005-02-28T23:00:00.000000000\n",
      "2005\n",
      "2006-02-28T23:00:00.000000000\n",
      "2006\n",
      "2007-02-28T23:00:00.000000000\n",
      "2007\n",
      "2008-02-29T23:00:00.000000000\n",
      "2008\n",
      "2009-02-28T23:00:00.000000000\n",
      "2009\n",
      "2010-02-28T23:00:00.000000000\n",
      "2010\n",
      "2011-02-28T23:00:00.000000000\n",
      "2011\n",
      "2012-02-29T23:00:00.000000000\n",
      "2012\n",
      "2013-02-28T23:00:00.000000000\n",
      "2013\n",
      "2014-02-28T23:00:00.000000000\n",
      "2014\n",
      "2015-02-28T23:00:00.000000000\n",
      "2015\n",
      "2016-02-29T23:00:00.000000000\n",
      "2016\n",
      "2017-02-28T23:00:00.000000000\n",
      "2017\n",
      "2018-02-28T23:00:00.000000000\n",
      "2018\n",
      "2019-02-28T23:00:00.000000000\n",
      "2019\n",
      "2020-02-29T23:00:00.000000000\n",
      "2020\n",
      "2021-02-28T23:00:00.000000000\n",
      "2021\n",
      "2022-02-28T23:00:00.000000000\n",
      "2022\n",
      "2023-02-28T23:00:00.000000000\n",
      "2023\n"
     ]
    }
   ],
   "source": [
    "for year in range(1979,2024):\n",
    "    ds_2 = read_data(curr_data_path, 2, year)\n",
    "    ds_3 = read_data(curr_data_path, 3, year)\n",
    "    your_existing_dataarray = ds_2.tp[-1]\n",
    "    print(ds_2.tp[-1].time.data)\n",
    "    new_ds = xr.Dataset({'tp': your_existing_dataarray})\n",
    "    combined_ds = xr.concat([new_ds, ds_3], dim='time')\n",
    "    combined_ds = combined_ds.transpose('time' , 'latitude', 'longitude')\n",
    "    combined_ds.to_netcdf(f\"../ERA5_land_data_recalculated/MAM_prev_val/Hourly_Total_Precipitation_T_{str(3).zfill(2)}{year}.nc\")\n",
    "    result = xr.apply_ufunc(\n",
    "            recalculate,\n",
    "            combined_ds,\n",
    "            input_core_dims=[['time']],\n",
    "            vectorize=True,\n",
    "            output_core_dims=[['time_recalc']],\n",
    "            output_dtypes=[np.float64],\n",
    "            dask='parallelized',\n",
    "        )\n",
    "    result = result.transpose('time_recalc' ,'latitude', 'longitude')\n",
    "    \n",
    "    time = combined_ds.time[1:]\n",
    "    latitude = result['latitude']\n",
    "    longitude = result['longitude']\n",
    "\n",
    "    final_result = xr.Dataset(coords={'latitude': latitude, 'longitude': longitude, 'time': time})\n",
    "\n",
    "    final_result['tp'] = (('time', 'latitude', 'longitude'), np.nan * np.ones((len(time), len(latitude), len(longitude))))\n",
    "\n",
    "    final_result['tp'][:,:,:] = result['tp'][:,:,:].data\n",
    "    final_result.to_netcdf(f\"../ERA5_land_data_recalculated/Hourly_Total_Precipitation_T_{str(3).zfill(2)}{year}.nc\")\n",
    "    print(year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a07fe50",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979-03-31T23:00:00.000000000\n",
      "1979\n",
      "1980-03-31T23:00:00.000000000\n",
      "1980\n",
      "1981-03-31T23:00:00.000000000\n",
      "1981\n",
      "1982-03-31T23:00:00.000000000\n",
      "1982\n",
      "1983-03-31T23:00:00.000000000\n",
      "1983\n",
      "1984-03-31T23:00:00.000000000\n",
      "1984\n",
      "1985-03-31T23:00:00.000000000\n",
      "1985\n",
      "1986-03-31T23:00:00.000000000\n",
      "1986\n",
      "1987-03-31T23:00:00.000000000\n",
      "1987\n",
      "1988-03-31T23:00:00.000000000\n",
      "1988\n",
      "1989-03-31T23:00:00.000000000\n",
      "1989\n",
      "1990-03-31T23:00:00.000000000\n",
      "1990\n",
      "1991-03-31T23:00:00.000000000\n",
      "1991\n",
      "1992-03-31T23:00:00.000000000\n",
      "1992\n",
      "1993-03-31T23:00:00.000000000\n",
      "1993\n",
      "1994-03-31T23:00:00.000000000\n",
      "1994\n",
      "1995-03-31T23:00:00.000000000\n",
      "1995\n",
      "1996-03-31T23:00:00.000000000\n",
      "1996\n",
      "1997-03-31T23:00:00.000000000\n",
      "1997\n",
      "1998-03-31T23:00:00.000000000\n",
      "1998\n",
      "1999-03-31T23:00:00.000000000\n",
      "1999\n",
      "2000-03-31T23:00:00.000000000\n",
      "2000\n",
      "2001-03-31T23:00:00.000000000\n",
      "2001\n",
      "2002-03-31T23:00:00.000000000\n",
      "2002\n",
      "2003-03-31T23:00:00.000000000\n",
      "2003\n",
      "2004-03-31T23:00:00.000000000\n",
      "2004\n",
      "2005-03-31T23:00:00.000000000\n",
      "2005\n",
      "2006-03-31T23:00:00.000000000\n",
      "2006\n",
      "2007-03-31T23:00:00.000000000\n",
      "2007\n",
      "2008-03-31T23:00:00.000000000\n",
      "2008\n",
      "2009-03-31T23:00:00.000000000\n",
      "2009\n",
      "2010-03-31T23:00:00.000000000\n",
      "2010\n",
      "2011-03-31T23:00:00.000000000\n",
      "2011\n",
      "2012-03-31T23:00:00.000000000\n",
      "2012\n",
      "2013-03-31T23:00:00.000000000\n",
      "2013\n",
      "2014-03-31T23:00:00.000000000\n",
      "2014\n",
      "2015-03-31T23:00:00.000000000\n",
      "2015\n",
      "2016-03-31T23:00:00.000000000\n",
      "2016\n",
      "2017-03-31T23:00:00.000000000\n",
      "2017\n",
      "2018-03-31T23:00:00.000000000\n",
      "2018\n",
      "2019-03-31T23:00:00.000000000\n",
      "2019\n",
      "2020-03-31T23:00:00.000000000\n",
      "2020\n",
      "2021-03-31T23:00:00.000000000\n",
      "2021\n",
      "2022-03-31T23:00:00.000000000\n",
      "2022\n",
      "2023-03-31T23:00:00.000000000\n",
      "2023\n"
     ]
    }
   ],
   "source": [
    "for year in range(1979,2024):\n",
    "    ds_3 = read_data(curr_data_path, 3, year)\n",
    "    ds_4 = read_data(curr_data_path, 4, year)\n",
    "    your_existing_dataarray = ds_3.tp[-1]\n",
    "    print(ds_3.tp[-1].time.data)\n",
    "    new_ds = xr.Dataset({'tp': your_existing_dataarray})\n",
    "    combined_ds = xr.concat([new_ds, ds_4], dim='time')\n",
    "    combined_ds = combined_ds.transpose('time' , 'latitude', 'longitude')\n",
    "    combined_ds.to_netcdf(f\"../ERA5_land_data_recalculated/MAM_prev_val/Hourly_Total_Precipitation_T_{str(4).zfill(2)}{year}.nc\")\n",
    "    result = xr.apply_ufunc(\n",
    "            recalculate,\n",
    "            combined_ds,\n",
    "            input_core_dims=[['time']],\n",
    "            vectorize=True,\n",
    "            output_core_dims=[['time_recalc']],\n",
    "            output_dtypes=[np.float64],\n",
    "            dask='parallelized',\n",
    "        )\n",
    "    result = result.transpose('time_recalc' ,'latitude', 'longitude')\n",
    "    \n",
    "    time = combined_ds.time[1:]\n",
    "    latitude = result['latitude']\n",
    "    longitude = result['longitude']\n",
    "\n",
    "    final_result = xr.Dataset(coords={'latitude': latitude, 'longitude': longitude, 'time': time})\n",
    "\n",
    "    final_result['tp'] = (('time', 'latitude', 'longitude'), np.nan * np.ones((len(time), len(latitude), len(longitude))))\n",
    "\n",
    "    final_result['tp'][:,:,:] = result['tp'][:,:,:].data\n",
    "    final_result.to_netcdf(f\"../ERA5_land_data_recalculated/Hourly_Total_Precipitation_T_{str(4).zfill(2)}{year}.nc\")\n",
    "    print(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4197c0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979-04-30T23:00:00.000000000\n",
      "1979\n",
      "1980-04-30T23:00:00.000000000\n",
      "1980\n",
      "1981-04-30T23:00:00.000000000\n",
      "1981\n",
      "1982-04-30T23:00:00.000000000\n",
      "1982\n",
      "1983-04-30T23:00:00.000000000\n",
      "1983\n",
      "1984-04-30T23:00:00.000000000\n",
      "1984\n",
      "1985-04-30T23:00:00.000000000\n",
      "1985\n",
      "1986-04-30T23:00:00.000000000\n",
      "1986\n",
      "1987-04-30T23:00:00.000000000\n",
      "1987\n",
      "1988-04-30T23:00:00.000000000\n",
      "1988\n",
      "1989-04-30T23:00:00.000000000\n",
      "1989\n",
      "1990-04-30T23:00:00.000000000\n",
      "1990\n",
      "1991-04-30T23:00:00.000000000\n",
      "1991\n",
      "1992-04-30T23:00:00.000000000\n",
      "1992\n",
      "1993-04-30T23:00:00.000000000\n",
      "1993\n",
      "1994-04-30T23:00:00.000000000\n",
      "1994\n",
      "1995-04-30T23:00:00.000000000\n",
      "1995\n",
      "1996-04-30T23:00:00.000000000\n",
      "1996\n",
      "1997-04-30T23:00:00.000000000\n",
      "1997\n",
      "1998-04-30T23:00:00.000000000\n",
      "1998\n",
      "1999-04-30T23:00:00.000000000\n",
      "1999\n",
      "2000-04-30T23:00:00.000000000\n",
      "2000\n",
      "2001-04-30T23:00:00.000000000\n",
      "2001\n",
      "2002-04-30T23:00:00.000000000\n",
      "2002\n",
      "2003-04-30T23:00:00.000000000\n",
      "2003\n",
      "2004-04-30T23:00:00.000000000\n",
      "2004\n",
      "2005-04-30T23:00:00.000000000\n",
      "2005\n",
      "2006-04-30T23:00:00.000000000\n",
      "2006\n",
      "2007-04-30T23:00:00.000000000\n",
      "2007\n",
      "2008-04-30T23:00:00.000000000\n",
      "2008\n",
      "2009-04-30T23:00:00.000000000\n",
      "2009\n",
      "2010-04-30T23:00:00.000000000\n",
      "2010\n",
      "2011-04-30T23:00:00.000000000\n",
      "2011\n",
      "2012-04-30T23:00:00.000000000\n",
      "2012\n",
      "2013-04-30T23:00:00.000000000\n",
      "2013\n",
      "2014-04-30T23:00:00.000000000\n",
      "2014\n",
      "2015-04-30T23:00:00.000000000\n",
      "2015\n",
      "2016-04-30T23:00:00.000000000\n",
      "2016\n",
      "2017-04-30T23:00:00.000000000\n",
      "2017\n",
      "2018-04-30T23:00:00.000000000\n",
      "2018\n",
      "2019-04-30T23:00:00.000000000\n",
      "2019\n",
      "2020-04-30T23:00:00.000000000\n",
      "2020\n",
      "2021-04-30T23:00:00.000000000\n",
      "2021\n",
      "2022-04-30T23:00:00.000000000\n",
      "2022\n",
      "2023-04-30T23:00:00.000000000\n",
      "2023\n"
     ]
    }
   ],
   "source": [
    "for year in range(1979,2024):\n",
    "    ds_4 = read_data(curr_data_path, 4, year)\n",
    "    ds_5 = read_data(curr_data_path, 5, year)\n",
    "    your_existing_dataarray = ds_4.tp[-1]\n",
    "    print(ds_4.tp[-1].time.data)\n",
    "    new_ds = xr.Dataset({'tp': your_existing_dataarray})\n",
    "    combined_ds = xr.concat([new_ds, ds_5], dim='time')\n",
    "    combined_ds = combined_ds.transpose('time' , 'latitude', 'longitude')\n",
    "    combined_ds.to_netcdf(f\"../ERA5_land_data_recalculated/MAM_prev_val/Hourly_Total_Precipitation_T_{str(5).zfill(2)}{year}.nc\")\n",
    "    result = xr.apply_ufunc(\n",
    "            recalculate,\n",
    "            combined_ds,\n",
    "            input_core_dims=[['time']],\n",
    "            vectorize=True,\n",
    "            output_core_dims=[['time_recalc']],\n",
    "            output_dtypes=[np.float64],\n",
    "            dask='parallelized',\n",
    "        )\n",
    "    result = result.transpose('time_recalc' ,'latitude', 'longitude')\n",
    "    \n",
    "    time = combined_ds.time[1:]\n",
    "    latitude = result['latitude']\n",
    "    longitude = result['longitude']\n",
    "\n",
    "    final_result = xr.Dataset(coords={'latitude': latitude, 'longitude': longitude, 'time': time})\n",
    "\n",
    "    final_result['tp'] = (('time', 'latitude', 'longitude'), np.nan * np.ones((len(time), len(latitude), len(longitude))))\n",
    "\n",
    "    final_result['tp'][:,:,:] = result['tp'][:,:,:].data\n",
    "    final_result.to_netcdf(f\"../ERA5_land_data_recalculated/Hourly_Total_Precipitation_T_{str(5).zfill(2)}{year}.nc\")\n",
    "    print(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315e204b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d6049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e0d6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef56045f",
   "metadata": {},
   "source": [
    "# SON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de9a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "8-9-10-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c61edb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalculate(data):\n",
    "    nan_mask = np.isnan(data)\n",
    "    binary_mask = np.where(nan_mask, 0, 1)\n",
    "    numeric_indices = np.where(binary_mask == 1)[0]\n",
    "    if len(numeric_indices) == 0:\n",
    "        differenced_data = data[1:]\n",
    "    elif len(numeric_indices) == len(data):\n",
    "        differenced_data = np.zeros_like(data[1:])\n",
    "        indices_01utc = np.where(np.arange(len(data))%24==2)[0]\n",
    "        indices_allutc = np.where(np.arange(len(data))%24!=2)[0][1:]\n",
    "        differenced_data[1::24] = data[indices_01utc]\n",
    "        differenced_data[indices_allutc - 1] = np.diff(data)[indices_allutc - 1]\n",
    "    else:\n",
    "        print(\"Error Encountered...\")\n",
    "    return differenced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11d13846",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979-08-31T23:00:00.000000000\n",
      "1979\n",
      "1980-08-31T23:00:00.000000000\n",
      "1980\n",
      "1981-08-31T23:00:00.000000000\n",
      "1981\n",
      "1982-08-31T23:00:00.000000000\n",
      "1982\n",
      "1983-08-31T23:00:00.000000000\n",
      "1983\n",
      "1984-08-31T23:00:00.000000000\n",
      "1984\n",
      "1985-08-31T23:00:00.000000000\n",
      "1985\n",
      "1986-08-31T23:00:00.000000000\n",
      "1986\n",
      "1987-08-31T23:00:00.000000000\n",
      "1987\n",
      "1988-08-31T23:00:00.000000000\n",
      "1988\n",
      "1989-08-31T23:00:00.000000000\n",
      "1989\n",
      "1990-08-31T23:00:00.000000000\n",
      "1990\n",
      "1991-08-31T23:00:00.000000000\n",
      "1991\n",
      "1992-08-31T23:00:00.000000000\n",
      "1992\n",
      "1993-08-31T23:00:00.000000000\n",
      "1993\n",
      "1994-08-31T23:00:00.000000000\n",
      "1994\n",
      "1995-08-31T23:00:00.000000000\n",
      "1995\n",
      "1996-08-31T23:00:00.000000000\n",
      "1996\n",
      "1997-08-31T23:00:00.000000000\n",
      "1997\n",
      "1998-08-31T23:00:00.000000000\n",
      "1998\n",
      "1999-08-31T23:00:00.000000000\n",
      "1999\n",
      "2000-08-31T23:00:00.000000000\n",
      "2000\n",
      "2001-08-31T23:00:00.000000000\n",
      "2001\n",
      "2002-08-31T23:00:00.000000000\n",
      "2002\n",
      "2003-08-31T23:00:00.000000000\n",
      "2003\n",
      "2004-08-31T23:00:00.000000000\n",
      "2004\n",
      "2005-08-31T23:00:00.000000000\n",
      "2005\n",
      "2006-08-31T23:00:00.000000000\n",
      "2006\n",
      "2007-08-31T23:00:00.000000000\n",
      "2007\n",
      "2008-08-31T23:00:00.000000000\n",
      "2008\n",
      "2009-08-31T23:00:00.000000000\n",
      "2009\n",
      "2010-08-31T23:00:00.000000000\n",
      "2010\n",
      "2011-08-31T23:00:00.000000000\n",
      "2011\n",
      "2012-08-31T23:00:00.000000000\n",
      "2012\n",
      "2013-08-31T23:00:00.000000000\n",
      "2013\n",
      "2014-08-31T23:00:00.000000000\n",
      "2014\n",
      "2015-08-31T23:00:00.000000000\n",
      "2015\n",
      "2016-08-31T23:00:00.000000000\n",
      "2016\n",
      "2017-08-31T23:00:00.000000000\n",
      "2017\n",
      "2018-08-31T23:00:00.000000000\n",
      "2018\n",
      "2019-08-31T23:00:00.000000000\n",
      "2019\n",
      "2020-08-31T23:00:00.000000000\n",
      "2020\n",
      "2021-08-31T23:00:00.000000000\n",
      "2021\n",
      "2022-08-31T23:00:00.000000000\n",
      "2022\n",
      "2023-08-31T23:00:00.000000000\n",
      "2023\n"
     ]
    }
   ],
   "source": [
    "for year in range(1979,2024):\n",
    "    ds_8 = read_data(curr_data_path, 8, year)\n",
    "    ds_9 = read_data(curr_data_path, 9, year)\n",
    "    your_existing_dataarray = ds_8.tp[-1]\n",
    "    print(ds_8.tp[-1].time.data)\n",
    "    new_ds = xr.Dataset({'tp': your_existing_dataarray})\n",
    "    combined_ds = xr.concat([new_ds, ds_9], dim='time')\n",
    "    combined_ds = combined_ds.transpose('time' , 'latitude', 'longitude')\n",
    "    combined_ds.to_netcdf(f\"../ERA5_land_data_recalculated/SON_prev_val/Hourly_Total_Precipitation_T_{str(9).zfill(2)}{year}.nc\")\n",
    "    result = xr.apply_ufunc(\n",
    "            recalculate,\n",
    "            combined_ds,\n",
    "            input_core_dims=[['time']],\n",
    "            vectorize=True,\n",
    "            output_core_dims=[['time_recalc']],\n",
    "            output_dtypes=[np.float64],\n",
    "            dask='parallelized',\n",
    "        )\n",
    "    result = result.transpose('time_recalc' ,'latitude', 'longitude')\n",
    "    \n",
    "    time = combined_ds.time[1:]\n",
    "    latitude = result['latitude']\n",
    "    longitude = result['longitude']\n",
    "\n",
    "    final_result = xr.Dataset(coords={'latitude': latitude, 'longitude': longitude, 'time': time})\n",
    "\n",
    "    final_result['tp'] = (('time', 'latitude', 'longitude'), np.nan * np.ones((len(time), len(latitude), len(longitude))))\n",
    "\n",
    "    final_result['tp'][:,:,:] = result['tp'][:,:,:].data\n",
    "    final_result.to_netcdf(f\"../ERA5_land_data_recalculated/Hourly_Total_Precipitation_T_{str(9).zfill(2)}{year}.nc\")\n",
    "    print(year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af33da5d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979-09-30T23:00:00.000000000\n",
      "1979\n",
      "1980-09-30T23:00:00.000000000\n",
      "1980\n",
      "1981-09-30T23:00:00.000000000\n",
      "1981\n",
      "1982-09-30T23:00:00.000000000\n",
      "1982\n",
      "1983-09-30T23:00:00.000000000\n",
      "1983\n",
      "1984-09-30T23:00:00.000000000\n",
      "1984\n",
      "1985-09-30T23:00:00.000000000\n",
      "1985\n",
      "1986-09-30T23:00:00.000000000\n",
      "1986\n",
      "1987-09-30T23:00:00.000000000\n",
      "1987\n",
      "1988-09-30T23:00:00.000000000\n",
      "1988\n",
      "1989-09-30T23:00:00.000000000\n",
      "1989\n",
      "1990-09-30T23:00:00.000000000\n",
      "1990\n",
      "1991-09-30T23:00:00.000000000\n",
      "1991\n",
      "1992-09-30T23:00:00.000000000\n",
      "1992\n",
      "1993-09-30T23:00:00.000000000\n",
      "1993\n",
      "1994-09-30T23:00:00.000000000\n",
      "1994\n",
      "1995-09-30T23:00:00.000000000\n",
      "1995\n",
      "1996-09-30T23:00:00.000000000\n",
      "1996\n",
      "1997-09-30T23:00:00.000000000\n",
      "1997\n",
      "1998-09-30T23:00:00.000000000\n",
      "1998\n",
      "1999-09-30T23:00:00.000000000\n",
      "1999\n",
      "2000-09-30T23:00:00.000000000\n",
      "2000\n",
      "2001-09-30T23:00:00.000000000\n",
      "2001\n",
      "2002-09-30T23:00:00.000000000\n",
      "2002\n",
      "2003-09-30T23:00:00.000000000\n",
      "2003\n",
      "2004-09-30T23:00:00.000000000\n",
      "2004\n",
      "2005-09-30T23:00:00.000000000\n",
      "2005\n",
      "2006-09-30T23:00:00.000000000\n",
      "2006\n",
      "2007-09-30T23:00:00.000000000\n",
      "2007\n",
      "2008-09-30T23:00:00.000000000\n",
      "2008\n",
      "2009-09-30T23:00:00.000000000\n",
      "2009\n",
      "2010-09-30T23:00:00.000000000\n",
      "2010\n",
      "2011-09-30T23:00:00.000000000\n",
      "2011\n",
      "2012-09-30T23:00:00.000000000\n",
      "2012\n",
      "2013-09-30T23:00:00.000000000\n",
      "2013\n",
      "2014-09-30T23:00:00.000000000\n",
      "2014\n",
      "2015-09-30T23:00:00.000000000\n",
      "2015\n",
      "2016-09-30T23:00:00.000000000\n",
      "2016\n",
      "2017-09-30T23:00:00.000000000\n",
      "2017\n",
      "2018-09-30T23:00:00.000000000\n",
      "2018\n",
      "2019-09-30T23:00:00.000000000\n",
      "2019\n",
      "2020-09-30T23:00:00.000000000\n",
      "2020\n",
      "2021-09-30T23:00:00.000000000\n",
      "2021\n",
      "2022-09-30T23:00:00.000000000\n",
      "2022\n",
      "2023-09-30T23:00:00.000000000\n",
      "2023\n"
     ]
    }
   ],
   "source": [
    "for year in range(1979,2024):\n",
    "    ds_9 = read_data(curr_data_path, 9, year)\n",
    "    ds_10 = read_data(curr_data_path, 10, year)\n",
    "    your_existing_dataarray = ds_9.tp[-1]\n",
    "    print(ds_9.tp[-1].time.data)\n",
    "    new_ds = xr.Dataset({'tp': your_existing_dataarray})\n",
    "    combined_ds = xr.concat([new_ds, ds_10], dim='time')\n",
    "    combined_ds = combined_ds.transpose('time' , 'latitude', 'longitude')\n",
    "    combined_ds.to_netcdf(f\"../ERA5_land_data_recalculated/SON_prev_val/Hourly_Total_Precipitation_T_{str(10).zfill(2)}{year}.nc\")\n",
    "    result = xr.apply_ufunc(\n",
    "            recalculate,\n",
    "            combined_ds,\n",
    "            input_core_dims=[['time']],\n",
    "            vectorize=True,\n",
    "            output_core_dims=[['time_recalc']],\n",
    "            output_dtypes=[np.float64],\n",
    "            dask='parallelized',\n",
    "        )\n",
    "    result = result.transpose('time_recalc' ,'latitude', 'longitude')\n",
    "    \n",
    "    time = combined_ds.time[1:]\n",
    "    latitude = result['latitude']\n",
    "    longitude = result['longitude']\n",
    "\n",
    "    final_result = xr.Dataset(coords={'latitude': latitude, 'longitude': longitude, 'time': time})\n",
    "\n",
    "    final_result['tp'] = (('time', 'latitude', 'longitude'), np.nan * np.ones((len(time), len(latitude), len(longitude))))\n",
    "\n",
    "    final_result['tp'][:,:,:] = result['tp'][:,:,:].data\n",
    "    final_result.to_netcdf(f\"../ERA5_land_data_recalculated/Hourly_Total_Precipitation_T_{str(10).zfill(2)}{year}.nc\")\n",
    "    print(year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8a2d1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-31T23:00:00.000000000\n",
      "2023\n"
     ]
    }
   ],
   "source": [
    "for year in range(1979,2024):\n",
    "    ds_10 = read_data(curr_data_path, 10, year)\n",
    "    ds_11 = read_data(curr_data_path, 11, year)\n",
    "    if 'expver' in ds_11.dims:\n",
    "            # Remove the 'expver' dimension using isel\n",
    "            ds_11 = ds_11.sel(expver=1).combine_first(ds_11.sel(expver=5))\n",
    "    your_existing_dataarray = ds_10.tp[-1]\n",
    "    print(ds_10.tp[-1].time.data)\n",
    "    new_ds = xr.Dataset({'tp': your_existing_dataarray})\n",
    "    combined_ds = xr.concat([new_ds, ds_11], dim='time')\n",
    "    combined_ds = combined_ds.transpose('time' , 'latitude', 'longitude')\n",
    "    combined_ds.to_netcdf(f\"../ERA5_land_data_recalculated/SON_prev_val/Hourly_Total_Precipitation_T_{str(11).zfill(2)}{year}.nc\")\n",
    "    result = xr.apply_ufunc(\n",
    "            recalculate,\n",
    "            combined_ds,\n",
    "            input_core_dims=[['time']],\n",
    "            vectorize=True,\n",
    "            output_core_dims=[['time_recalc']],\n",
    "            output_dtypes=[np.float64],\n",
    "            dask='parallelized',\n",
    "        )\n",
    "    result = result.transpose('time_recalc' ,'latitude', 'longitude')\n",
    "    \n",
    "    time = combined_ds.time[1:]\n",
    "    latitude = result['latitude']\n",
    "    longitude = result['longitude']\n",
    "\n",
    "    final_result = xr.Dataset(coords={'latitude': latitude, 'longitude': longitude, 'time': time})\n",
    "\n",
    "    final_result['tp'] = (('time', 'latitude', 'longitude'), np.nan * np.ones((len(time), len(latitude), len(longitude))))\n",
    "\n",
    "    final_result['tp'][:,:,:] = result['tp'][:,:,:].data\n",
    "    final_result.to_netcdf(f\"../ERA5_land_data_recalculated/Hourly_Total_Precipitation_T_{str(11).zfill(2)}{year}.nc\")\n",
    "    print(year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18200fa",
   "metadata": {},
   "source": [
    "# DJF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862bd641",
   "metadata": {},
   "outputs": [],
   "source": [
    "11-12-1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43b055b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalculate(data):\n",
    "    nan_mask = np.isnan(data)\n",
    "    binary_mask = np.where(nan_mask, 0, 1)\n",
    "    numeric_indices = np.where(binary_mask == 1)[0]\n",
    "    if len(numeric_indices) == 0:\n",
    "        differenced_data = data[1:]\n",
    "    elif len(numeric_indices) == len(data):\n",
    "        differenced_data = np.zeros_like(data[1:])\n",
    "        indices_01utc = np.where(np.arange(len(data))%24==2)[0]\n",
    "        indices_allutc = np.where(np.arange(len(data))%24!=2)[0][1:]\n",
    "        differenced_data[1::24] = data[indices_01utc]\n",
    "        differenced_data[indices_allutc - 1] = np.diff(data)[indices_allutc - 1]\n",
    "    else:\n",
    "        print(\"Error Encountered...\")\n",
    "    return differenced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f325bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1978-11-30T23:00:00.000000000\n",
      "1978\n",
      "1979-11-30T23:00:00.000000000\n",
      "1979\n",
      "1980-11-30T23:00:00.000000000\n",
      "1980\n",
      "1981-11-30T23:00:00.000000000\n",
      "1981\n",
      "1982-11-30T23:00:00.000000000\n",
      "1982\n",
      "1983-11-30T23:00:00.000000000\n",
      "1983\n",
      "1984-11-30T23:00:00.000000000\n",
      "1984\n",
      "1985-11-30T23:00:00.000000000\n",
      "1985\n",
      "1986-11-30T23:00:00.000000000\n",
      "1986\n",
      "1987-11-30T23:00:00.000000000\n",
      "1987\n",
      "1988-11-30T23:00:00.000000000\n",
      "1988\n",
      "1989-11-30T23:00:00.000000000\n",
      "1989\n",
      "1990-11-30T23:00:00.000000000\n",
      "1990\n",
      "1991-11-30T23:00:00.000000000\n",
      "1991\n",
      "1992-11-30T23:00:00.000000000\n",
      "1992\n",
      "1993-11-30T23:00:00.000000000\n",
      "1993\n",
      "1994-11-30T23:00:00.000000000\n",
      "1994\n",
      "1995-11-30T23:00:00.000000000\n",
      "1995\n",
      "1996-11-30T23:00:00.000000000\n",
      "1996\n",
      "1997-11-30T23:00:00.000000000\n",
      "1997\n",
      "1998-11-30T23:00:00.000000000\n",
      "1998\n",
      "1999-11-30T23:00:00.000000000\n",
      "1999\n",
      "2000-11-30T23:00:00.000000000\n",
      "2000\n",
      "2001-11-30T23:00:00.000000000\n",
      "2001\n",
      "2002-11-30T23:00:00.000000000\n",
      "2002\n",
      "2003-11-30T23:00:00.000000000\n",
      "2003\n",
      "2004-11-30T23:00:00.000000000\n",
      "2004\n",
      "2005-11-30T23:00:00.000000000\n",
      "2005\n",
      "2006-11-30T23:00:00.000000000\n",
      "2006\n",
      "2007-11-30T23:00:00.000000000\n",
      "2007\n",
      "2008-11-30T23:00:00.000000000\n",
      "2008\n",
      "2009-11-30T23:00:00.000000000\n",
      "2009\n",
      "2010-11-30T23:00:00.000000000\n",
      "2010\n",
      "2011-11-30T23:00:00.000000000\n",
      "2011\n",
      "2012-11-30T23:00:00.000000000\n",
      "2012\n",
      "2013-11-30T23:00:00.000000000\n",
      "2013\n",
      "2014-11-30T23:00:00.000000000\n",
      "2014\n",
      "2015-11-30T23:00:00.000000000\n",
      "2015\n",
      "2016-11-30T23:00:00.000000000\n",
      "2016\n",
      "2017-11-30T23:00:00.000000000\n",
      "2017\n",
      "2018-11-30T23:00:00.000000000\n",
      "2018\n",
      "2019-11-30T23:00:00.000000000\n",
      "2019\n",
      "2020-11-30T23:00:00.000000000\n",
      "2020\n",
      "2021-11-30T23:00:00.000000000\n",
      "2021\n",
      "2022-11-30T23:00:00.000000000\n",
      "2022\n"
     ]
    }
   ],
   "source": [
    "for year in range(1978,2023):\n",
    "    ds_11 = read_data(curr_data_path, 11, year)\n",
    "    ds_12 = read_data(curr_data_path, 12, year)\n",
    "    if 'expver' in ds_11.dims:\n",
    "            # Remove the 'expver' dimension using isel\n",
    "            ds_11 = ds_11.sel(expver=1).combine_first(ds_11.sel(expver=5))\n",
    "    your_existing_dataarray = ds_11.tp[-1]\n",
    "    print(ds_11.tp[-1].time.data)\n",
    "    new_ds = xr.Dataset({'tp': your_existing_dataarray})\n",
    "    combined_ds = xr.concat([new_ds, ds_12], dim='time')\n",
    "    combined_ds = combined_ds.transpose('time' , 'latitude', 'longitude')\n",
    "    combined_ds.to_netcdf(f\"../ERA5_land_data_recalculated/DJF_prev_val/Hourly_Total_Precipitation_T_{str(12).zfill(2)}{year}.nc\")\n",
    "    result = xr.apply_ufunc(\n",
    "            recalculate,\n",
    "            combined_ds,\n",
    "            input_core_dims=[['time']],\n",
    "            vectorize=True,\n",
    "            output_core_dims=[['time_recalc']],\n",
    "            output_dtypes=[np.float64],\n",
    "            dask='parallelized',\n",
    "        )\n",
    "    result = result.transpose('time_recalc' ,'latitude', 'longitude')\n",
    "    \n",
    "    time = combined_ds.time[1:]\n",
    "    latitude = result['latitude']\n",
    "    longitude = result['longitude']\n",
    "\n",
    "    final_result = xr.Dataset(coords={'latitude': latitude, 'longitude': longitude, 'time': time})\n",
    "\n",
    "    final_result['tp'] = (('time', 'latitude', 'longitude'), np.nan * np.ones((len(time), len(latitude), len(longitude))))\n",
    "\n",
    "    final_result['tp'][:,:,:] = result['tp'][:,:,:].data\n",
    "    final_result.to_netcdf(f\"../ERA5_land_data_recalculated/Hourly_Total_Precipitation_T_{str(12).zfill(2)}{year}.nc\")\n",
    "    print(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bac43092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1978-12-31T23:00:00.000000000\n",
      "1978\n",
      "1979-12-31T23:00:00.000000000\n",
      "1979\n",
      "1980-12-31T23:00:00.000000000\n",
      "1980\n",
      "1981-12-31T23:00:00.000000000\n",
      "1981\n",
      "1982-12-31T23:00:00.000000000\n",
      "1982\n",
      "1983-12-31T23:00:00.000000000\n",
      "1983\n",
      "1984-12-31T23:00:00.000000000\n",
      "1984\n",
      "1985-12-31T23:00:00.000000000\n",
      "1985\n",
      "1986-12-31T23:00:00.000000000\n",
      "1986\n",
      "1987-12-31T23:00:00.000000000\n",
      "1987\n",
      "1988-12-31T23:00:00.000000000\n",
      "1988\n",
      "1989-12-31T23:00:00.000000000\n",
      "1989\n",
      "1990-12-31T23:00:00.000000000\n",
      "1990\n",
      "1991-12-31T23:00:00.000000000\n",
      "1991\n",
      "1992-12-31T23:00:00.000000000\n",
      "1992\n",
      "1993-12-31T23:00:00.000000000\n",
      "1993\n",
      "1994-12-31T23:00:00.000000000\n",
      "1994\n",
      "1995-12-31T23:00:00.000000000\n",
      "1995\n",
      "1996-12-31T23:00:00.000000000\n",
      "1996\n",
      "1997-12-31T23:00:00.000000000\n",
      "1997\n",
      "1998-12-31T23:00:00.000000000\n",
      "1998\n",
      "1999-12-31T23:00:00.000000000\n",
      "1999\n",
      "2000-12-31T23:00:00.000000000\n",
      "2000\n",
      "2001-12-31T23:00:00.000000000\n",
      "2001\n",
      "2002-12-31T23:00:00.000000000\n",
      "2002\n",
      "2003-12-31T23:00:00.000000000\n",
      "2003\n",
      "2004-12-31T23:00:00.000000000\n",
      "2004\n",
      "2005-12-31T23:00:00.000000000\n",
      "2005\n",
      "2006-12-31T23:00:00.000000000\n",
      "2006\n",
      "2007-12-31T23:00:00.000000000\n",
      "2007\n",
      "2008-12-31T23:00:00.000000000\n",
      "2008\n",
      "2009-12-31T23:00:00.000000000\n",
      "2009\n",
      "2010-12-31T23:00:00.000000000\n",
      "2010\n",
      "2011-12-31T23:00:00.000000000\n",
      "2011\n",
      "2012-12-31T23:00:00.000000000\n",
      "2012\n",
      "2013-12-31T23:00:00.000000000\n",
      "2013\n",
      "2014-12-31T23:00:00.000000000\n",
      "2014\n",
      "2015-12-31T23:00:00.000000000\n",
      "2015\n",
      "2016-12-31T23:00:00.000000000\n",
      "2016\n",
      "2017-12-31T23:00:00.000000000\n",
      "2017\n",
      "2018-12-31T23:00:00.000000000\n",
      "2018\n",
      "2019-12-31T23:00:00.000000000\n",
      "2019\n",
      "2020-12-31T23:00:00.000000000\n",
      "2020\n",
      "2021-12-31T23:00:00.000000000\n",
      "2021\n",
      "2022-12-31T23:00:00.000000000\n",
      "2022\n"
     ]
    }
   ],
   "source": [
    "for year in range(1978,2023):\n",
    "    ds_12 = read_data(curr_data_path, 12, year)\n",
    "    ds_1 = read_data(curr_data_path, 1, year+1)\n",
    "    if 'expver' in ds_12.dims:\n",
    "            # Remove the 'expver' dimension using isel\n",
    "            ds_12 = ds_12.sel(expver=1).combine_first(ds_12.sel(expver=5))\n",
    "    your_existing_dataarray = ds_12.tp[-1]\n",
    "    print(ds_12.tp[-1].time.data)\n",
    "    new_ds = xr.Dataset({'tp': your_existing_dataarray})\n",
    "    combined_ds = xr.concat([new_ds, ds_1], dim='time')\n",
    "    combined_ds = combined_ds.transpose('time' , 'latitude', 'longitude')\n",
    "    combined_ds.to_netcdf(f\"../ERA5_land_data_recalculated/DJF_prev_val/Hourly_Total_Precipitation_T_{str(1).zfill(2)}{year+1}.nc\")\n",
    "    result = xr.apply_ufunc(\n",
    "            recalculate,\n",
    "            combined_ds,\n",
    "            input_core_dims=[['time']],\n",
    "            vectorize=True,\n",
    "            output_core_dims=[['time_recalc']],\n",
    "            output_dtypes=[np.float64],\n",
    "            dask='parallelized',\n",
    "        )\n",
    "    result = result.transpose('time_recalc' ,'latitude', 'longitude')\n",
    "    \n",
    "    time = combined_ds.time[1:]\n",
    "    latitude = result['latitude']\n",
    "    longitude = result['longitude']\n",
    "\n",
    "    final_result = xr.Dataset(coords={'latitude': latitude, 'longitude': longitude, 'time': time})\n",
    "\n",
    "    final_result['tp'] = (('time', 'latitude', 'longitude'), np.nan * np.ones((len(time), len(latitude), len(longitude))))\n",
    "\n",
    "    final_result['tp'][:,:,:] = result['tp'][:,:,:].data\n",
    "    final_result.to_netcdf(f\"../ERA5_land_data_recalculated/Hourly_Total_Precipitation_T_{str(1).zfill(2)}{year+1}.nc\")\n",
    "    print(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0d15d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-31T23:00:00.000000000\n",
      "2023\n"
     ]
    }
   ],
   "source": [
    "for year in range(2023,2024):\n",
    "    ds_1 = read_data(curr_data_path, 1, year)\n",
    "    ds_2 = read_data(curr_data_path, 2, year)\n",
    "    your_existing_dataarray = ds_1.tp[-1]\n",
    "    print(ds_1.tp[-1].time.data)\n",
    "    new_ds = xr.Dataset({'tp': your_existing_dataarray})\n",
    "    combined_ds = xr.concat([new_ds, ds_2], dim='time')\n",
    "    combined_ds = combined_ds.transpose('time' , 'latitude', 'longitude')\n",
    "    combined_ds.to_netcdf(f\"../ERA5_land_data_recalculated/DJF_prev_val/Hourly_Total_Precipitation_T_{str(2).zfill(2)}{year}.nc\")\n",
    "    result = xr.apply_ufunc(\n",
    "            recalculate,\n",
    "            combined_ds,\n",
    "            input_core_dims=[['time']],\n",
    "            vectorize=True,\n",
    "            output_core_dims=[['time_recalc']],\n",
    "            output_dtypes=[np.float64],\n",
    "            dask='parallelized',\n",
    "        )\n",
    "    result = result.transpose('time_recalc' ,'latitude', 'longitude')\n",
    "    \n",
    "    time = combined_ds.time[1:]\n",
    "    latitude = result['latitude']\n",
    "    longitude = result['longitude']\n",
    "\n",
    "    final_result = xr.Dataset(coords={'latitude': latitude, 'longitude': longitude, 'time': time})\n",
    "\n",
    "    final_result['tp'] = (('time', 'latitude', 'longitude'), np.nan * np.ones((len(time), len(latitude), len(longitude))))\n",
    "\n",
    "    final_result['tp'][:,:,:] = result['tp'][:,:,:].data\n",
    "    final_result.to_netcdf(f\"../ERA5_land_data_recalculated/Hourly_Total_Precipitation_T_{str(2).zfill(2)}{year}.nc\")\n",
    "    print(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399131fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdd55a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15db9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded99d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee278466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_1178 = read_data(curr_data_path,11,1978)\n",
    "# ds_1278 = read_data(curr_data_path,12,1978)\n",
    "# start_time  = len(ds_1178.tp)-24\n",
    "# end_time = len(ds_1178.tp)\n",
    "# your_existing_dataarray = ds_1178.tp[-1]\n",
    "# new_ds = xr.Dataset({'tp': your_existing_dataarray})\n",
    "# combined_ds = xr.concat([new_ds, ds_1278], dim='time')\n",
    "# combined_ds = combined_ds.transpose('time' , 'latitude', 'longitude')\n",
    "\n",
    "# def concat(prev_month, curr_month, prev_year, curr_year):\n",
    "#     prev_ds = read_data(curr_data_path, prev_month, prev_year)\n",
    "#     curr_ds = read_data(curr_data_path, curr_month, curr_year)\n",
    "#     to_append = prev_ds.tp[-1]\n",
    "#     new_ds = xr.Dataset({'tp': to_append})\n",
    "#     combined_ds = xr.concat([new_ds, curr_ds], dim='time')\n",
    "#     combined_ds = combined_ds.transpose('time' , 'latitude', 'longitude')\n",
    "#     return combined_ds\n",
    "\n",
    "# d78 = concat(11,12,1978,1978)\n",
    "# j79 = read_data(curr_data_path,1,1979)\n",
    "# f79 = read_data(curr_data_path,2,1979)\n",
    "\n",
    "# con = xr.concat([d78,j79,f79], dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfb2f643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf42e5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17d065d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723417bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
